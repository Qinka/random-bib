@Misc{challenge/ISBI/MEMENTO2020,
  title   = {{MRI White Matter Reconstruction | ISBI 2019/2020 MEMENTO Challenge}},
  url     = {https://my.vanderbilt.edu/memento/},
  urldate = {2021-03-11},
}

@Misc{challenge/NEATBrainS,
  title   = {{NEATBrainS - Image Sciences Institute}},
  url     = {https://www.isi.uu.nl/research/challenges/neatbrains/},
  urldate = {2021-03-11},
}

@Misc{challenge/AccelMR2020,
  title   = {{AccelMR 2020 Prediction Challenge – AccelMR 2020 for ISBI 2020}},
  url     = {https://accelmrorg.wordpress.com/},
  urldate = {2021-03-11},
}

@Misc{challenge/ODIR2019,
  title = {{Peking University International Competition on Ocular Disease Intelligent Recognition}},
  url   = {https://odir2019.grand-challenge.org/},
}

@Misc{challenge/LOLA11,
  title = {{Home - LOLA11 - Grand Challenge}},
  url   = {https://lola11.grand-challenge.org/Home/},
}

@Article{challenge/AMOS22,
  author        = {Ji, Yuanfeng and Bai, Haotian and Yang, Jie and Ge, Chongjian and Zhu, Ye and Zhang, Ruimao and Li, Zhen and Zhang, Lingyan and Ma, Wanling and Wan, Xiang and Luo, Ping},
  title         = {{AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation}},
  year          = {2022},
  month         = {jun},
  abstract      = {Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.},
  archiveprefix = {arXiv},
  arxivid       = {2206.08023},
  eprint        = {2206.08023},
  url           = {http://arxiv.org/abs/2206.08023},
}

@Article{challenge/MMWHS,
  author        = {Zhuang, Xiahai and Li, Lei and Payer, Christian and {\v{S}}tern, Darko and Urschler, Martin and Heinrich, Mattias P. and Oster, Julien and Wang, Chunliang and Smedby, {\"{O}}rjan and Bian, Cheng and Yang, Xin and Heng, Pheng Ann and Mortazi, Aliasghar and Bagci, Ulas and Yang, Guanyu and Sun, Chenchen and Galisot, Gaetan and Ramel, Jean Yves and Brouard, Thierry and Tong, Qianqian and Si, Weixin and Liao, Xiangyun and Zeng, Guodong and Shi, Zenglin and Zheng, Guoyan and Wang, Chengjia and MacGillivray, Tom and Newby, David and Rhode, Kawal and Ourselin, Sebastien and Mohiaddin, Raad and Keegan, Jennifer and Firmin, David and Yang, Guang},
  journal       = {Medical Image Analysis},
  title         = {{Evaluation of algorithms for Multi-Modality Whole Heart Segmentation: An open-access grand challenge}},
  year          = {2019},
  issn          = {13618423},
  month         = {dec},
  volume        = {58},
  abstract      = {Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be challenging due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, an initial set of training data is generally needed for constructing priors or for training. Furthermore, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provided 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results showed that the performance of CT WHS was generally better than that of MRI WHS. The segmentation of the substructures for different categories of patients could present different levels of challenge due to the difference in imaging and variations of heart shapes. The deep learning (DL)-based methods demonstrated great potential, though several of them reported poor results in the blinded evaluation. Their performance could vary greatly across different network structures and training strategies. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated good performance, though the accuracy and computational efficiency could be limited. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/).},
  archiveprefix = {arXiv},
  arxivid       = {1902.07880},
  doi           = {10.1016/j.media.2019.101537},
  eprint        = {1902.07880},
  keywords      = {Benchmark,Challenge,Multi-modality,Whole Heart Segmentation},
  pmid          = {31446280},
  publisher     = {Elsevier B.V.},
}

@Article{challenge/autoimplant,
  author        = {Li, Jianning and Pepe, Antonio and Gsaxner, Christina and Campe, Gord von and Egger, Jan},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{A Baseline Approach for AutoImplant: The MICCAI 2020 Cranial Implant Design Challenge}},
  year          = {2020},
  issn          = {16113349},
  month         = {jun},
  pages         = {75--84},
  volume        = {12445 LNCS},
  abstract      = {In this study, we present a baseline approach for AutoImplant (https://autoimplant.grand-challenge.org/) – the cranial implant design challenge, which can be formulated as a volumetric shape learning task. In this task, the defective skull, the complete skull and the cranial implant are represented as binary voxel grids. To accomplish this task, the implant can be either reconstructed directly from the defective skull or obtained by taking the difference between a defective skull and a complete skull. In the latter case, a complete skull has to be reconstructed given a defective skull, which defines a volumetric shape completion problem. Our baseline approach for this task is based on the former formulation, i.e., a deep neural network is trained to predict the implants directly from the defective skulls. The approach generates high-quality implants in two steps: First, an encoder-decoder network learns a coarse representation of the implant from downsampled, defective skulls; The coarse implant is only used to generate the bounding box of the defected region in the original high-resolution skull. Second, another encoder-decoder network is trained to generate a fine implant from the bounded area. On the test set, the proposed approach achieves an average dice similarity score (DSC) of 0.8555 and Hausdorff distance (HD) of 5.1825 mm. The codes are available at https://github.com/Jianningli/autoimplant.},
  archiveprefix = {arXiv},
  arxivid       = {2006.12449},
  doi           = {10.1007/978-3-030-60946-7_8},
  eprint        = {2006.12449},
  isbn          = {9783030609450},
  keywords      = {Cranial implant design,Cranioplasty,Deep learning,Shape learning,Skull reconstruction,Volumetric shape completion},
  url           = {http://arxiv.org/abs/2006.12449},
}

@Article{challenge/miccai/CAMELYON17,
  author   = {B{\'{a}}ndi, P{\'{e}}ter and Geessink, Oscar and Manson, Quirine and {Van Dijk}, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and {Ehteshami Bejnordi}, Babak and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and Li, Quanzheng and Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Fukuta, Keisuke and Komura, Daisuke and Ovtcharov, Vlado and Cheng, Shenghua and Zeng, Shaoqun and Thagaard, Jeppe and Dahl, Anders B. and Lin, Huangjing and Chen, Hao and Jacobsson, Ludwig and Hedlund, Martin and {\c{C}}etin, Melih and Halici, Eren and Jackson, Hunter and Chen, Richard and Both, Fabian and Franke, J{\"{o}}rg and Kusters-Vandevelde, Heidi and Vreuls, Willem and Bult, Peter and {Van Ginneken}, Bram and {Van Der Laak}, Jeroen and Litjens, Geert},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge}},
  year     = {2019},
  issn     = {1558254X},
  month    = {feb},
  number   = {2},
  pages    = {550--560},
  volume   = {38},
  abstract = {Automated detection of cancer metastases in lymph nodes has the potential to improve the assessment of prognosis for patients. To enable fair comparison between the algorithms for this purpose, we set up the CAMELYON17 challenge in conjunction with the IEEE International Symposium on Biomedical Imaging 2017 Conference in Melbourne. Over 300 participants registered on the challenge website, of which 23 teams submitted a total of 37 algorithms before the initial deadline. Participants were provided with 899 whole-slide images (WSIs) for developing their algorithms. The developed algorithms were evaluated based on the test set encompassing 100 patients and 500 WSIs. The evaluation metric used was a quadratic weighted Cohen's kappa. We discuss the algorithmic details of the 10 best pre-conference and two post-conference submissions. All these participants used convolutional neural networks in combination with pre- and postprocessing steps. Algorithms differed mostly in neural network architecture, training strategy, and pre- and postprocessing methodology. Overall, the kappa metric ranged from 0.89 to -0.13 across all submissions. The best results were obtained with pre-trained architectures such as ResNet. Confusion matrix analysis revealed that all participants struggled with reliably identifying isolated tumor cells, the smallest type of metastasis, with detection rates below 40%. Qualitative inspection of the results of the top participants showed categories of false positives, such as nerves or contamination, which could be targeted for further optimization. Last, we show that simple combinations of the top algorithms result in higher kappa metric values than any algorithm individually, with 0.93 for the best combination.},
  doi      = {10.1109/TMI.2018.2867350},
  keywords = {Breast cancer,grand challenge,lymph node metastases,sentinel lymph node,whole-slide images},
  pmid     = {30716025},
  url      = {https://ieeexplore.ieee.org/document/8447230/},
}

@Article{challenge/BACH,
  author        = {Aresta, Guilherme and Ara{\'{u}}jo, Teresa and Kwok, Scotty and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Marami, Bahram and Prastawa, Marcel and Chan, Monica and Donovan, Michael and Fernandez, Gerardo and Zeineh, Jack and Kohl, Matthias and Walz, Christoph and Ludwig, Florian and Braunewell, Stefan and Baust, Maximilian and Vu, Quoc Dang and To, Minh Nguyen Nhat and Kim, Eal and Kwak, Jin Tae and Galal, Sameh and Sanchez-Freire, Veronica and Brancati, Nadia and Frucci, Maria and Riccio, Daniel and Wang, Yaqi and Sun, Lingling and Ma, Kaiqiang and Fang, Jiannan and Kone, Ismael and Boulmane, Lahsen and Campilho, Aur{\'{e}}lio and Eloy, Catarina and Pol{\'{o}}nia, Ant{\'{o}}nio and Aguiar, Paulo},
  journal       = {Medical Image Analysis},
  title         = {{BACH: Grand challenge on breast cancer histology images}},
  year          = {2019},
  issn          = {13618423},
  month         = {aug},
  pages         = {122--139},
  volume        = {56},
  abstract      = {Breast cancer is the most common invasive cancer in women, affecting more than 10% of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). BACH aimed at the classification and localization of clinically relevant histopathological classes in microscopy and whole-slide images from a large annotated dataset, specifically compiled and made publicly available for the challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. The submitted algorithms improved the state-of-the-art in automatic classification of breast cancer with microscopy images to an accuracy of 87%. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publicly available as to promote further improvements to the field of automatic classification in digital pathology.},
  archiveprefix = {arXiv},
  arxivid       = {1808.04277},
  doi           = {10.1016/j.media.2019.05.010},
  eprint        = {1808.04277},
  keywords      = {Breast cancer,Challenge,Comparative study,Deep learning,Digital pathology,Histology},
  pmid          = {31226662},
  url           = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518307941},
}

@Article{challenge/MICCAI/CAMELYON16,
  author   = {Litjens, Geert and Bandi, Peter and Bejnordi, Babak Ehteshami and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob and Manson, Quirine F. and Stathonikos, Nikolas and Baidoshvili, Alexi and van Diest, Paul and Wauters, Carla and van Dijk, Marcory and van der Laak, Jeroen},
  journal  = {GigaScience},
  title    = {{1399 H&E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset}},
  year     = {2018},
  issn     = {2047217X},
  month    = {jun},
  number   = {6},
  volume   = {7},
  abstract = {Background: The presence of lymph node metastases is one of the most important factors in breast cancer prognosis. The most common way to assess regional lymph node status is the sentinel lymph node procedure. The sentinel lymph node is the most likely lymph node to contain metastasized cancer cells and is excised, histopathologically processed, and examined by a pathologist. This tedious examination process is time-consuming and can lead to small metastases being missed. However, recent advances in whole-slide imaging and machine learning have opened an avenue for analysis of digitized lymph node sections with computer algorithms. For example, convolutional neural networks, a type of machine-learning algorithm, can be used to automatically detect cancer metastases in lymph nodes with high accuracy. To train machine-learning models, large, well-curated datasets are needed. Results: We released a dataset of 1,399 annotated whole-slide images (WSIs) of lymph nodes, both with and without metastases, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYON17 Grand Challenges. Slides were collected from five medical centers to cover a broad range of image appearance and staining variations. Each WSI has a slide-level label indicating whether it contains no metastases, macro-metastases, micro-metastases, or isolated tumor cells. Furthermore, for 209 WSIs, detailed hand-drawn contours for all metastases are provided. Last, open-source software tools to visualize and interact with the data have been made available. Conclusions: A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use.},
  doi      = {10.1093/gigascience/giy065},
  keywords = {Breast cancer,Grand challenge,Lymph node metastases,Sentinel node,Whole-slide images},
  pmid     = {29860392},
  url      = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy065/5026175},
}

@Article{challenge/Lyon19,
  author   = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and van Rijthoven, Mart and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
  journal  = {Medical Image Analysis},
  title    = {{Learning to detect lymphocytes in immunohistochemistry with deep learning}},
  year     = {2019},
  issn     = {13618423},
  volume   = {58},
  abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation ($\kappa$=0.72), whereas the average pathologists agreement with reference standard was $\kappa$=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
  doi      = {10.1016/j.media.2019.101547},
  keywords = {Computational pathology,Deep learning,Immune cell detection,Immunohistochemistry},
  pmid     = {31476576},
}

@Article{challenge/isbi2012/us,
  author   = {Rueda, Sylvia and Fathima, Sana and Knight, Caroline L. and Yaqub, Mohammad and Papageorghiou, Aris T. and Rahmatullah, Bahbibi and Foi, Alessandro and Maggioni, Matteo and Pepe, Antonietta and Tohka, Jussi and Stebbing, Richard V. and McManigle, John E. and Ciurte, Anca and Bresson, Xavier and Cuadra, Meritxell Bach and Sun, Changming and Ponomarev, Gennady V. and Gelfand, Mikhail S. and Kazanov, Marat D. and Wang, Ching Wei and Chen, Hsiang Chou and Peng, Chun Wei and Hung, Chu Mei and Noble, J. Alison},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Evaluation and comparison of current fetal ultrasound image segmentation methods for biometric measurements: A grand challenge}},
  year     = {2014},
  issn     = {02780062},
  number   = {4},
  pages    = {797--813},
  volume   = {33},
  abstract = {This paper presents the evaluation results of the methods submitted to Challenge US: Biometric Measurements from Fetal Ultrasound Images, a segmentation challenge held at the IEEE International Symposium on Biomedical Imaging 2012. The challenge was set to compare and evaluate current fetal ultrasound image segmentation methods. It consisted of automatically segmenting fetal anatomical structures to measure standard obstetric biometric parameters, from 2D fetal ultrasound images taken on fetuses at different gestational ages (21 weeks, 28 weeks, and 33 weeks) and with varying image quality to reflect data encountered in real clinical environments. Four independent sub-challenges were proposed, according to the objects of interest measured in clinical practice: abdomen, head, femur, and whole fetus. Five teams participated in the head sub-challenge and two teams in the femur sub-challenge, including one team who tackled both. Nobody attempted the abdomen and whole fetus sub-challenges. The challenge goals were two-fold and the participants were asked to submit the segmentation results as well as the measurements derived from the segmented objects. Extensive quantitative (region-based, distance-based, and Bland-Altman measurements) and qualitative evaluation was performed to compare the results from a representative selection of current methods submitted to the challenge. Several experts (three for the head sub-challenge and two for the femur sub-challenge), with different degrees of expertise, manually delineated the objects of interest to define the ground truth used within the evaluation framework. For the head sub-challenge, several groups produced results that could be potentially used in clinical settings, with comparable performance to manual delineations. The femur sub-challenge had inferior performance to the head sub-challenge due to the fact that it is a harder segmentation problem and that the techniques presented relied more on the femur's appearance. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2013.2276943},
  keywords = {Challenge,Ultrasound (US),evaluation,fetal biometry,image quality,segmentation},
  pmid     = {23934664},
}

@Article{challenge/lungseg2017,
  author   = {Yang, Jinzhong and Veeraraghavan, Harini and Armato, Samuel G. and Farahani, Keyvan and Kirby, Justin S. and Kalpathy-Kramer, Jayashree and van Elmpt, Wouter and Dekker, Andre and Han, Xiao and Feng, Xue and Aljabar, Paul and Oliveira, Bruno and van der Heyden, Brent and Zamdborg, Leonid and Lam, Dao and Gooding, Mark and Sharp, Gregory C.},
  journal  = {Medical Physics},
  title    = {{Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017}},
  year     = {2018},
  issn     = {00942405},
  number   = {10},
  pages    = {4568--4581},
  volume   = {45},
  abstract = {Purpose: This report presents the methods and results of the Thoracic Auto-Segmentation Challenge organized at the 2017 Annual Meeting of American Association of Physicists in Medicine. The purpose of the challenge was to provide a benchmark dataset and platform for evaluating performance of autosegmentation methods of organs at risk (OARs) in thoracic CT images. Methods : Sixty thoracic CT scans provided by three different institutions were separated into 36 training, 12 offline testing, and 12 online testing scans. Eleven participants completed the offline challenge, and seven completed the online challenge. The OARs were left and right lungs, heart, esophagus, and spinal cord. Clinical contours used for treatment planning were quality checked and edited to adhere to the RTOG 1106 contouring guidelines. Algorithms were evaluated using the Dice coefficient, Hausdorff distance, and mean surface distance. A consolidated score was computed by normalizing the metrics against interrater variability and averaging over all patients and structures. Results : The interrater study revealed highest variability in Dice for the esophagus and spinal cord, and in surface distances for lungs and heart. Five out of seven algorithms that participated in the online challenge employed deep-learning methods. Although the top three participants using deep learning produced the best segmentation for all structures, there was no significant difference in the performance among them. The fourth place participant used a multi-atlas-based approach. The highest Dice scores were produced for lungs, with averages ranging from 0.95 to 0.98, while the lowest Dice scores were produced for esophagus, with a range of 0.55–0.72. Conclusion : The results of the challenge showed that the lungs and heart can be segmented fairly accurately by various algorithms, while deep-learning methods performed better on the esophagus. Our dataset together with the manual contours for all training cases continues to be available publicly as an ongoing benchmarking resource.},
  doi      = {10.1002/mp.13141},
  keywords = {automatic segmentation,grand challenge,lung cancer,radiation therapy},
  pmid     = {30144101},
}

@Article{challenge/CHAOS,
  author        = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Aslan, Sinem and Conze, Pierre-Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and Özkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and Nürnberger, Andreas and Maier-Hein, Klaus H. and Bozdağı Akar, Gözde and Ünal, Gözde and Dicle, Oğuz and Selver, M. Alper},
  journal       = {Medical Image Analysis},
  title         = {CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation},
  year          = {2021},
  issn          = {1361-8415},
  month         = apr,
  pages         = {101950},
  volume        = {69},
  abstract      = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.},
  archiveprefix = {arXiv},
  arxivid       = {2001.06535},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2020.101950},
  eprint        = {2001.06535},
  keywords      = {Abdomen,Challenge,Cross-modality,Segmentation},
  pmid          = {33421920},
  publisher     = {Elsevier BV},
  url           = {https://chaos.grand-challenge.org/Combined_Healthy_Abdominal_Organ_Segmentation/},
}

@Article{challenge/sliver07,
  author   = {Heimann, Tobias and {Van Ginneken}, Brain and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, Gy{\"{o}}rgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M.M. and Chi, Ying and C{\'{o}}rdova, Andr{\'{e}}s and Dawant, Benoit M. and Fidrich, M{\'{a}}rta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and Kainm{\"{u}}ller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans Peter and N{\'{e}}meth, G{\'{a}}bor and Raicu, Daniela S. and Rau, Anne Mareike and {Van Rikxoort}, Eva M. and Rousson, Mika{\"{e}}l and Rusk{\'{o}}, L{\'{a}}szlo and Saddi, Kinda A. and Schmidt, G{\"{u}}nter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Comparison and evaluation of methods for liver segmentation from CT datasets}},
  year     = {2009},
  issn     = {02780062},
  number   = {8},
  pages    = {1251--1265},
  volume   = {28},
  abstract = {This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the "MICCAI 2007 Grand Challenge" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques. {\textcopyright} 2009 IEEE.},
  doi      = {10.1109/TMI.2009.2013851},
  keywords = {Evaluation,Liver,Segmentation},
  pmid     = {19211338},
}

@Article{challenge/promise12,
  author   = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
  journal  = {Medical Image Analysis},
  title    = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
  year     = {2014},
  issn     = {13618415},
  abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p < 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
  doi      = {10.1016/j.media.2013.12.002},
  keywords = {Challenge,MRI,Prostate,Segmentation},
}

@Misc{challenge/ROCC,
  author = {Rabbani, Hossein and Rasti, Reza and Kafieh, Rahele},
  title  = {{ROCC - Retinal OCT Classification Challenge (ROCC)}},
  year   = {2017},
  url    = {https://rocc.grand-challenge.org/},
}

@Misc{challenge/RFGC,
  title = {{Retinal Fundus Glaucoma Challenge}},
  url   = {https://refuge.grand-challenge.org/},
}

@Misc{challenge/refuge20,
  author        = {Orlando, Jos{\'{e}} Ignacio and Fu, Huazhu and {Barbossa Breda}, Jo{\~{a}}o and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andr{\'{e}}s and Fang, Ruogu and Heng, Pheng Ann and Kim, Jeyoung and Lee, Joon Ho and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunovi{\'{c}}, Hrvoje},
  title         = {{REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs}},
  year          = {2020},
  abstract      = {Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.},
  archiveprefix = {arXiv},
  arxivid       = {1910.03667},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2019.101570},
  eprint        = {1910.03667},
  issn          = {13618423},
  keywords      = {Deep learning,Fundus photography,Glaucoma,Image classification,Image segmentation},
  pmid          = {31630011},
  volume        = {59},
}

@Article{challenge/IDRiD,
  author   = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, Tian Bo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^{a}}nia and Ara{\'{u}}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c{c}}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'{e}}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'{e}} and M{\'{e}}riaudeau, Fabrice},
  journal  = {Medical Image Analysis},
  title    = {{IDRiD: Diabetic Retinopathy – Segmentation and Grading Challenge}},
  year     = {2020},
  issn     = {13618423},
  month    = {jan},
  pages    = {101561},
  volume   = {59},
  abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
  doi      = {10.1016/j.media.2019.101561},
  keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis},
  pmid     = {31671320},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841519301033},
}

@Misc{challenge/CADDementia/Home,
  author = {Bron, Esther E. and Klein, Stefan and Smits, Marion and van Swieten, John C. and Niessen, Wiro J.},
  title  = {{CADDementia - Home}},
  year   = {2014},
  url    = {https://caddementia.grand-challenge.org/},
}

@Article{challenge/TADPOLE,
  author        = {Marinescu, Razvan V. and Oxtoby, Neil P. and Young, Alexandra L. and Bron, Esther E. and Toga, Arthur W. and Weiner, Michael W. and Barkhof, Frederik and Fox, Nick C. and Eshaghi, Arman and Toni, Tina and Salaterski, Marcin and Lunina, Veronika and Ansart, Manon and Durrleman, Stanley and Lu, Pascal and Iddi, Samuel and Li, Dan and Thompson, Wesley K. and Donohue, Michael C. and Nahon, Aviv and Levy, Yarden and Halbersberg, Dan and Cohen, Mariya and Liao, Huiling and Li, Tengfei and Yu, Kaixian and Zhu, Hongtu and Tamez-Pe{\~{n}}a, Jos{\'{e}} G. and Ismail, Aya and Wood, Timothy and Bravo, Hector Corrada and Nguyen, Minh and Sun, Nanbo and Feng, Jiashi and {Thomas Yeo}, B. T. and Chen, Gang and Qi, Ke and Chen, Shiyang and Qiu, Deqiang and Buciuman, Ionut and Kelner, Alex and Pop, Raluca and Rimocea, Denisa and Ghazi, Mostafa M. and Nielsen, Mads and Ourselin, Sebastien and S{\o}rensen, Lauge and Venkatraghavan, Vikram and Liu, Keli and Rabe, Christina and Manser, Paul and Hill, Steven M. and Howlett, James and Huang, Zhiyue and Kiddle, Steven and Mukherjee, Sach and Rouanet, Ana{\"{i}}s and Taschler, Bernd and Tom, Brian D.M. and White, Simon R. and Faux, Noel and Sedai, Suman and {de Velasco Oriol}, Javier and Clemente, Edgar E.V. and Estrada, Karol and Aksman, Leon and Altmann, Andre and Stonnington, Cynthia M. and Wang, Yalin and Wu, Jianfeng and Devadas, Vivek and Fourrier, Clementine and Raket, Lars Lau and Sotiras, Aristeidis and Erus, Guray and Doshi, Jimit and Davatzikos, Christos and Vogel, Jacob and Doyle, Andrew and Tam, Angela and Diaz-Papkovich, Alex and Jammeh, Emmanuel and Koval, Igor and Moore, Paul and Lyons, Terry J. and Gallacher, John and Tohka, Jussi and Ciszek, Robert and Jedynak, Bruno and Pandya, Kruti and Bilgel, Murat and Engels, William and Cole, Joseph and Golland, Polina and Klein, Stefan and Alexander, Daniel C.},
  journal       = {arXiv},
  title         = {{The Alzheimer's disease prediction of longitudinal evolution (tadpole) challenge: Results after 1 year follow-up}},
  year          = {2020},
  issn          = {23318422},
  month         = {feb},
  abstract      = {Accurate prediction of progression in subjects at risk of Alzheimer's disease is crucial for enrolling the right subjects in clinical trials. However, a prospective comparison of state-of-the-art algorithms for predicting disease onset and progression is currently lacking. We present the findings of The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guessing. On a limited, cross-sectional subset of the data emulating clinical trials, performance of best algorithms at predicting clinical diagnosis decreased only slightly (3% error increase) compared to the full longitudinal dataset. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as patient-specific biomarker trends. The submission system remains open via the website https://tadpole.grand-challenge.org, while code for submissions is being collated by TADPOLE SHARE: https://tadpole-share.github.io/. Our work suggests that current prediction algorithms are accurate for biomarkers related to clinical diagnosis and ventricle volume, opening up the possibility of cohort refinement in clinical trials for Alzheimer's disease.},
  archiveprefix = {arXiv},
  arxivid       = {2002.03419},
  eprint        = {2002.03419},
  url           = {http://arxiv.org/abs/2002.03419},
}

@Misc{challenge/TADPOLE/HOME,
  title   = {{TADPOLE - Home}},
  url     = {https://tadpole.grand-challenge.org/},
  urldate = {2020-05-25},
}

@Misc{challenge/AAPM/RT-MAC,
  author    = {Cardenas, Carlos E and Mohamed, A and Sharp, G. and Gooding, M. and Veeraraghavan, H. and Yang, J.},
  title     = {{Data from AAPM RT-MAC Grand Challenge 2019}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/tcia.2019.bcfjqfqb},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bAP9Ag},
}

@Article{challenge/MICCAI07/3D-seg,
  author   = {van Ginneken, Bram and Heimann, Tobias and Styner, Martin},
  journal  = {International Conference on Medical Image Computing and Computer Assisted Intervention},
  title    = {{3D segmentation in the clinic: A grand challeng}},
  year     = {2007},
  pages    = {7--15},
  volume   = {10},
  abstract = {This paper describes the setup of a segmentation competition for the automatic extraction of Multiple Sclerosis (MS) lesions from brain Magnetic Resonance Imaging (MRI) data. This competition is one of three competitions that make up a comparison workshop at the 2008 Medical Image Computing and Computer Assisted Intervention (MICCAI) conference and was modeled after the successful comparison workshop on liver and caudate segmentation at the 2007 MICCAI conference. In this paper, the rationale for organizing the competition is discussed, the training and test data sets for both segmentation tasks are described and the scoring system used to evaluate the segmentation is presented.},
  url      = {http://grand-challenge2008.bigr.nl/proceedings/pdfs/msls08/Styner.pdf},
}

@Misc{challenge/MRBrainS18,
  author = {Kuijf, Hugo J. and Bennink, H. Edwin and Biessels, Geert Jan and Viergever, Max A. and Weaver, Nick A. and Vincken, Koen L.},
  title  = {{MRBrainS18 - Grand Challenge on MR Brain Segmentation 2018}},
  year   = {2018},
  url    = {https://mrbrains18.isi.uu.nl/},
}

@Article{challenge/MICCAI09/3D-seg,
  author   = {Hameeteman, K. and Zuluaga, M. A. and Freiman, M. and Joskowicz, L. and Cuisenaire, O. and Valencia, L. Fl{\'{o}}rez and G{\"{u}}ls{\"{u}}n, M. A. and Krissian, K. and Mille, J. and Wong, W. C.K. and Orkisz, M. and Tek, H. and Hoyos, M. Hern{\'{a}}ndez and Benmansour, F. and Chung, A. C.S. and Rozie, S. and van Gils, M. and van den Borne, L. and Sosna, J. and Berman, P. and Cohen, N. and Douek, P. C. and S{\'{a}}nchez, I. and Aissat, M. and Schaap, M. and Metz, C. T. and Krestin, G. P. and van der Lugt, A. and Niessen, W. J. and {Van Walsum}, T.},
  journal  = {Medical Image Analysis},
  title    = {{Evaluation framework for carotid bifurcation lumen segmentation and stenosis grading}},
  year     = {2011},
  issn     = {13618415},
  month    = {aug},
  number   = {4},
  pages    = {477--488},
  volume   = {15},
  abstract = {This paper describes an evaluation framework that allows a standardized and objective quantitative comparison of carotid artery lumen segmentation and stenosis grading algorithms. We describe the data repository comprising 56 multi-center, multi-vendor CTA datasets, their acquisition, the creation of the reference standard and the evaluation measures. This framework has been introduced at the MICCAI 2009 workshop 3D Segmentation in the Clinic: A Grand Challenge III, and we compare the results of eight teams that participated. These results show that automated segmentation of the vessel lumen is possible with a precision that is comparable to manual annotation. The framework is open for new submissions through the website http://cls2009.bigr.nl. {\textcopyright} 2011 Elsevier B.V.},
  doi      = {10.1016/j.media.2011.02.004},
  keywords = {CTA,Carotid,Evaluation framework,Lumen segmentation,Stenosis grading},
  pmid     = {21419689},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841511000260},
}

@Article{challenge/CADDementia,
  author   = {Bron, Esther E. and Smits, Marion and van der Flier, Wiesje M. and Vrenken, Hugo and Barkhof, Frederik and Scheltens, Philip and Papma, Janne M. and Steketee, Rebecca M.E. and {M{\'{e}}ndez Orellana}, Carolina and Meijboom, Rozanna and Pinto, Madalena and Meireles, Joana R. and Garrett, Carolina and Bastos-Leite, Ant{\'{o}}nio J. and Abdulkadir, Ahmed and Ronneberger, Olaf and Amoroso, Nicola and Bellotti, Roberto and C{\'{a}}rdenas-Pe{\~{n}}a, David and {\'{A}}lvarez-Meza, Andr{\'{e}}s M. and Dolph, Chester V. and Iftekharuddin, Khan M. and Eskildsen, Simon F. and Coup{\'{e}}, Pierrick and Fonov, Vladimir S. and Franke, Katja and Gaser, Christian and Ledig, Christian and Guerrero, Ricardo and Tong, Tong and Gray, Katherine R. and Moradi, Elaheh and Tohka, Jussi and Routier, Alexandre and Durrleman, Stanley and Sarica, Alessia and {Di Fatta}, Giuseppe and Sensi, Francesco and Chincarini, Andrea and Smith, Garry M. and Stoyanov, Zhivko V. and S{\o}rensen, Lauge and Nielsen, Mads and Tangaro, Sabina and Inglese, Paolo and Wachinger, Christian and Reuter, Martin and van Swieten, John C. and Niessen, Wiro J. and Klein, Stefan},
  journal  = {NeuroImage},
  title    = {{Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge}},
  year     = {2015},
  issn     = {10959572},
  month    = {may},
  pages    = {562--579},
  volume   = {111},
  abstract = {Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment and healthy controls. The diagnosis based on clinical criteria was used as reference standard, as it was the best available reference despite its known limitations. For evaluation, a previously unseen test set was used consisting of 354 T1-weighted MRI scans with the diagnoses blinded. Fifteen research teams participated with a total of 29 algorithms. The algorithms were trained on a small training set (n. =. 30) and optionally on data from other sources (e.g., the Alzheimer's Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of aging). The best performing algorithm yielded an accuracy of 63.0% and an area under the receiver-operating-characteristic curve (AUC) of 78.8%. In general, the best performances were achieved using feature extraction based on voxel-based morphometry or a combination of features that included volume, cortical thickness, shape and intensity. The challenge is open for new submissions via the web-based framework: http://caddementia.grand-challenge.org.},
  doi      = {10.1016/j.neuroimage.2015.01.048},
  keywords = {Alzheimer's disease,Challenge,Classification,Computer-aided diagnosis,Mild cognitive impairment,Structural MRI},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811915000737},
}

@Misc{challenge/MICCAI/6m-Infant-brain-seg,
  author = {Wang, Li and Bui, Toan Duc and Li, Gang and Lin, Weili and Shen, Dinggang},
  title  = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation}},
  year   = {2019},
  url    = {http://iseg2019.web.unc.edu/},
}

@Misc{challenge/MALBCV,
  author = {Landman, Bennett and Xu, Zhoubing and Igelsias, Juan Eugenio and Styner, Martin and Langerak, Thomas Robin and Klein, Arno},
  title  = {{Multi-Atlas Labeling Beyond the Cranial Vault -- workshop and challenge}},
  year   = {2015},
}

@Misc{challenge/BRATS_SICAS,
  title = {{BRATS - SICAS Medical Image Repository}},
  url   = {https://www.virtualskeleton.ch/BRATS/Start2013},
}

@Misc{challenge/MICCAI/BRATS/2018,
  author   = {{Perelman School Of Medicine}},
  title    = {{MICCAI BraTS 2018: Clinical Relevance | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2018},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2018/clinical-relevance.html},
}

@Misc{challenge/MICCAI/BRATS/2012,
  title = {{MICCAI BRATS 2012}},
  url   = {http://www2.imm.dtu.dk/projects/BRATS2012/},
}

@Misc{challenge/MICCAI/BRATS/2017,
  author   = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Reyes, Mauricio and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Shaykh, Hassan Fathallah and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre},
  title    = {{MICCAI BraTS 2017: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2017},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2017.html},
}

@Misc{challenge/MICCAI/BRATS/2020,
  author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Tiwari, Pallavi and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Huang, Raymond and Colen, Rivka R. and Marcus, Daniel and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek},
  title  = {{MICCAI BRATS - The Multimodal Brain Tumor Segmentation Challenge 2020}},
  year   = {2020},
  url    = {http://braintumorsegmentation.org/},
}

@Article{challenge/MICCAI/BRATS/2013,
  author   = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
  year     = {2015},
  issn     = {1558254X},
  month    = {oct},
  number   = {10},
  pages    = {1993--2024},
  volume   = {34},
  abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  doi      = {10.1109/TMI.2014.2377694},
  keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
  pmid     = {25494501},
  url      = {http://ieeexplore.ieee.org/document/6975210/},
}

@Misc{challenge/MICCAI/ENIGMA2017,
  title   = {{ENIGMA Cerebellum | MICCAI 2017 Workshop & Challenge}},
  url     = {https://my.vanderbilt.edu/enigmacerebellum/},
  urldate = {2020-05-12},
}

@Misc{dataset/neuromorphometrics,
  title = {{Products | Neuromorphometrics, Inc.}},
  url   = {http://www.neuromorphometrics.com/?page_id=23},
}

@Article{challenge/DiabeticRetinopathyDetection,
  journal  = {International Journal of Engineering and Advanced Technology},
  title    = {{Diabetic Retinopathy Detection}},
  year     = {2020},
  number   = {4},
  pages    = {1022--1026},
  volume   = {9},
  abstract = {Diabetic retinopathy is becoming a more prevalent disease in diabetic patients nowadays. The surprising fact about the disease is it leaves no symptoms at the beginning stage and the patient can realize the disease only when his vision starts to fall. If the disease is not found at the earliest it leads to a stage where the probability of curing the disease is less. But if we find the disease at that stage, the patient might be in a situation of losing the vision completely. Hence, this paper aims at finding the disease at the earliest possible stage by extracting two features from the retinal image namely Microaneurysms which is found to be the starting symptom showing feature and Hemorrhage which shows symptoms of the other stages. Based on these two features we classify the stage of the disease as normal, beginning, mild and severe using convolutional neural network, a deep learning technique which reduces the burden of manual feature extraction and gives higher accuracy. We also locate the position of these features in the disease affected retinal images to help the doctors offer better medical treatment.},
  doi      = {10.35940/ijeat.d7786.049420},
  url      = {https://www.kaggle.com/c/diabetic-retinopathy-detection},
}

@Article{dataset/ad-2020a,
  journal  = {Alzheimer's & Dementia},
  title    = {{2020 Alzheimer's disease facts and figures}},
  year     = {2020},
  issn     = {1552-5260},
  month    = {mar},
  number   = {3},
  pages    = {391--460},
  volume   = {16},
  abstract = {This article describes the public health impact of Alzheimer's disease (AD), including incidence and prevalence, mortality and morbidity, use and costs of care, and the overall impact on caregivers and society. The Special Report discusses the future challenges of meeting care demands for the growing number of people living with Alzheimer's dementia in the United States with a particular emphasis on primary care. By mid-century, the number of Americans age 65 and older with Alzheimer's dementia may grow to 13.8 million. This represents a steep increase from the estimated 5.8 million Americans age 65 and older who have Alzheimer's dementia today. Official death certificates recorded 122,019 deaths from AD in 2018, the latest year for which data are available, making Alzheimer's the sixth leading cause of death in the United States and the fifth leading cause of death among Americans age 65 and older. Between 2000 and 2018, deaths resulting from stroke, HIV and heart disease decreased, whereas reported deaths from Alzheimer's increased 146.2%. In 2019, more than 16 million family members and other unpaid caregivers provided an estimated 18.6 billion hours of care to people with Alzheimer's or other dementias. This care is valued at nearly $244 billion, but its costs extend to family caregivers' increased risk for emotional distress and negative mental and physical health outcomes. Average per-person Medicare payments for services to beneficiaries age 65 and older with AD or other dementias are more than three times as great as payments for beneficiaries without these conditions, and Medicaid payments are more than 23 times as great. Total payments in 2020 for health care, long-term care and hospice services for people age 65 and older with dementia are estimated to be $305 billion. As the population of Americans living with Alzheimer's dementia increases, the burden of caring for that population also increases. These challenges are exacerbated by a shortage of dementia care specialists, which places an increasing burden on primary care physicians (PCPs) to provide care for people living with dementia. Many PCPs feel underprepared and inadequately trained to handle dementia care responsibilities effectively. This report includes recommendations for maximizing quality care in the face of the shortage of specialists and training challenges in primary care.},
  doi      = {10.1002/alz.12068},
  keywords = {Alzheimer's dementia,Alzheimer's disease,Alzheimer's disease continuum,Biomarkers,Caregivers,Dementia,Dementia care training,Family caregiver,Geriatrician,Health care costs,Health care expenditures,Health care professional,Incidence,Long-term care costs,Medicaid spending,Medicare spending,Morbidity,Mortality,Prevalence,Primary care physician,Risk factors,Spouse caregiver},
  pmid     = {32157811},
  url      = {https://onlinelibrary.wiley.com/doi/10.1002/alz.12068},
}

@Article{challenge/AnDi,
  author        = {Gorka Muñoz-Gil and Giovanni Volpe and Miguel Angel Garcia-March and Ralf Metzler and Maciej Lewenstein and Carlo Manzo},
  title         = {AnDi: The Anomalous Diffusion Challenge},
  year          = {2020},
  issn          = {23318422},
  month         = aug,
  abstract      = {The deviation from pure Brownian motion generally referred to as anomalous diffusion has received large attention in the scientific literature to describe many physical scenarios. Several methods, based on classical statistics and machine learning approaches, have been developed to characterize anomalous diffusion from experimental data, which are usually acquired as particle trajectories. With the aim to assess and compare the available methods to characterize anomalous diffusion, we have organized the Anomalous Diffusion (AnDi) Challenge (\url{http://www.andi-challenge.org/}). Specifically, the AnDi Challenge will address three different aspects of anomalous diffusion characterization, namely: (i) Inference of the anomalous diffusion exponent. (ii) Identification of the underlying diffusion model. (iii) Segmentation of trajectories. Each problem includes sub-tasks for different number of dimensions (1D, 2D and 3D). In order to compare the various methods, we have developed a dedicated open-source framework for the simulation of the anomalous diffusion trajectories that are used for the training and test datasets. The challenge was launched on March 1, 2020, and consists of three phases. Currently, the participation to the first phase is open. Submissions will be automatically evaluated and the performance of the top-scoring methods will be thoroughly analyzed and compared in an upcoming article.},
  archiveprefix = {arXiv},
  arxivid       = {2003.12036},
  booktitle     = {Emerging Topics in Artificial Intelligence 2020},
  date          = {2020-03-26},
  doi           = {10.1117/12.2567914},
  editor        = {Volpe, Giovanni and Pereira, Joana B. and Brunner, Daniel and Ozcan, Aydogan},
  eprint        = {2003.12036},
  file          = {:http\://arxiv.org/pdf/2003.12036v1:PDF},
  keywords      = {cond-mat.stat-mech, physics.comp-ph, physics.data-an, q-bio.QM},
  primaryclass  = {cond-mat.stat-mech},
  publisher     = {SPIE},
  url           = {https://zenodo.org/record/3707702},
}

@Misc{dataset/ISIC2020,
  author    = {{International Skin Imaging Collaboration}},
  title     = {{SIIM-ISIC 2020 Challenge Dataset}},
  year      = {2020},
  doi       = {10.34970/2020-DS01},
  publisher = {International Skin Imaging Collaboration},
  url       = {https://challenge2020.isic-archive.com/},
}

@Misc{dataset/Alkhasli2019,
  author    = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M and Binkofski, Ferdinand},
  title     = {{Resting State - TMS}},
  year      = {2019},
  doi       = {10.18112/OPENNEURO.DS001832.V1.0.1},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001832/versions/1.0.1},
}

@Misc{dataset/Buda2020,
  author    = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and Swiecicki, Albert and Lo, Joseph Y and Yang, Jichen and Mazurowski, Maciej},
  title     = {{Breast Cancer Screening – Digital Breast Tomosynthesis (BCS-DBT)}},
  year      = {2020},
  doi       = {10.7937/E4WT-CD02},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/DAbbAw},
}

@Misc{dataset/Dalca2020,
  author    = {Dalca, Adrian and {Yipeng Hu} and Vercauteren, Tom and Heinrich, Mattias and Hansen, Lasse and Modat, Marc and Vos, Bob De and {Yiming Xiao} and Rivaz, Hassan and Chabanas, Matthieu and Reinertsen, Ingerid and Landman, Bennett and Cardoso, Jorge and Ginneken, Bram Van and {Alessa Hering} and Murphy, Keelin},
  title     = {{Learn2Reg - The Challenge}},
  year      = {2020},
  doi       = {10.5281/ZENODO.3715651},
  keywords  = {Abdomen,Biomedical Challenges,Brain,Deformable,MICCAI,MICCAI Challenges,Multimodal,Realtime,Registration,Thorax},
  publisher = {Zenodo},
  url       = {https://zenodo.org/record/3715651},
}

@Misc{dataset/Mackin2017,
  author    = {Mackin, Dennis and Ray, Xenia and Zhang, Lifei and Fried, David and Yang, Jinzhong and Taylor, Brian and Rodriguez-Rivera, Edgardo and Dodge, Cristina and Jones, Aaron and Court, Laurence},
  title     = {{Data From Credence Cartridge Radiomics Phantom CT Scans}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.ZUZRML5B},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/jIxyAQ},
}

@Misc{dataset/ShafiqulHassan2018,
  author    = {M, Shafiq ul Hassan and G, Zhang and K, Latifi and G, Ullah and R, Gillies and E, Moros},
  title     = {{Credence Cartridge Radiomics Phantom CT Scans with Controlled Scanning Approach (CC-Radiomics-Phantom-2)}},
  year      = {2018},
  doi       = {10.7937/TCIA.2019.4L24TZ5G},
  publisher = {The Cancer Imaging Archive},
  url       = {http://doi.org/10.7937/TCIA.2019.4l24tz5g},
}

@Misc{dataset/Nadkarni2019,
  author    = {Nadkarni, N A and Bougacha, S and Garin, C and Picq, J L and Dhenain, M},
  title     = {{MouseLemurAtlas_MRIraw}},
  year      = {2019},
  doi       = {10.18112/OPENNEURO.DS001945.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001945/versions/1.0.0},
}

@Misc{dataset/Takata2020,
  author    = {Takata, Norio and Sato, Nobuhiko and Komaki, Yuji and Okano, Hideyuki and Tanaka, Kenji F},
  title     = {{Mouse_awake_rest}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002551.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002551/versions/1.0.0},
}

@Misc{dataset/Ger2018,
  author    = {Ger, Rachel and Yang, Jinzhong and Ding, Yao and Jacobsen, Megan and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, R Jason and Zhou, Shouhao and Court, Laurence},
  title     = {{Data from Synthetic and Phantom MR Images for Determining Deformable Image Registration Accuracy (MRI-DIR)}},
  year      = {2018},
  doi       = {10.7937/K9/TCIA.2018.3F08IEJT},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/-gA4Ag},
}

@Misc{dataset/Ger2019,
  author    = {Ger, Rachel and Zhou, Shouhao and Chi, Pai-Chun and Lee, Hannah and Layman, Rick R and Jones, Kyle and Goff, David and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, Jason and Court, Laurence and Mackin, Dennis},
  title     = {{Data from CT Phantom Scans for Head, Chest, and Controlled Protocols on 100 Scanners (CC-Radiomics-Phantom-3)}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.J71I4FAH},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/RADDAg},
}

@Misc{dataset/Muzi2015,
  author    = {Muzi, Peter and Wanner, Michelle and Kinahan, Paul},
  title     = {{Data From RIDER_PHANTOM_PET-CT}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.8WG2KN4W},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bIRXAQ},
}

@Misc{dataset/Mourya2019,
  author    = {Mourya, Simmi and Kant, Sonaal and Kumar, Pulkit and Gupta, Anubha and Gupta, Rita},
  title     = {{ALL Challenge dataset of ISBI 2019}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.DC64I46R},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/zwYlAw},
}

@Misc{dataset/Campanella2019,
  author    = {Campanella, Gabriele and Hanna, Matthew G and Brogi, Edi and Fuchs, Thomas J},
  title     = {{Breast Metastases to Axillary Lymph Nodes}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.3XBN2JCC},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/yxolAw},
}

@Misc{dataset/Gupta2019,
  author    = {Gupta, Ritu and Gupta, Anubha},
  title     = {{MiMM_SBILab Dataset: Microscopic Images of Multiple Myeloma}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.PNN6AYPL},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/-AElAw},
}

@Misc{dataset/Martel2019,
  author    = {Martel, Anne L. and Salama, Sherine and Nofech-Mozes, Sharon and Akbar, Shazia and Peikari, Mohammad},
  title     = {{Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology [Data set]}},
  year      = {2019},
  abstract  = {Breast cancer (BC) is the second most commonly diagnosed cancer in the U.S. with more than 250,000 new cases of invasive breast cancers reported in 2017. The majority of women with locally advanced and a subset of patients with operable breast cancer will undergo systemic therapy prior to their surgery (neoadjuvant therapy/ NAT) to reduce the size of tumor(s) and possibly further undergo breast conserving surgery. The Post-NAT-BRCA dataset is a collection of representative sections from breast resections in patients with residual invasive BC following NAT. Histologic sections were prepared and digitized to produce high resolution, microscopic images of treated BC tumors. Also included, are clinical features and expert pathology annotations of tumor cellularity and cell types. The Residual Cancer Burden Index (RCBi), is a clinically validated tool for assessment of response to NAT associated with prognosis. Tumor cellularity is one of the parameters used for calculating the RCBi. In this dataset, tumor cellularity refers to a measure of residual disease after NAT, in the form of proportion of malignant tumor inside the tumor bed region; also annotated. (See MD Anderson RCB Calculator for a detailed description of tumor cellularity.) Malignant, healthy, lymphocyte and other labels were also provided for individual cells to aid development of cell segmentation algorithms.},
  doi       = {10.7937/TCIA.2019.4YIBTJNO},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758117},
}

@Misc{dataset/Bloch2015,
  author    = {Bloch, B Nicolas and Jain, Ashali and conrade carl Jaffe},
  title     = {{Data From BREAST-DIAGNOSIS}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.SDNRQXXR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/JQAo},
}

@Misc{dataset/Gavrielides2015,
  author    = {Gavrielides, Marios A and Kinnard, Lisa M and Myers, Kyle J and Peregoy, Jenifer and Pritchard, William F and Zeng, Rongping and Esparza, Juan and Karanian, John and Petrick, Nicholas},
  title     = {{Data From Phantom_FDA}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.ORBJKMUX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/CAA9},
}

@Misc{dataset/Kinahan2017,
  author    = {Kinahan, Paul and Muzi, Mark and Bialecki, Brian and Coombs, Laura},
  title     = {{Data from ACRIN-FLT-Breast}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.OL20ZMXG},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/pAHUAQ},
}

@Misc{dataset/Wee2019,
  author    = {Wee, Leonard and Aerts, Hugo J W L and Kalendralis, Petros and Dekker, Andre},
  title     = {{Data from NSCLC-Radiomics-Interobserver1}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.CWVLPD26},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bgAlAw},
}

@Misc{dataset/Zhao2015,
  author    = {Zhao, Binsheng and Schwartz, Lawrence H and Kris, Mark G},
  title     = {{Data From RIDER_Lung CT}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.U1X8A5NR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/XIRXAQ},
}

@Misc{dataset/Madabhushi2018,
  author    = {Madabhushi, Anant and Rusu, Mirabela},
  title     = {{Fused Radiology-Pathology Lung Dataset}},
  year      = {2018},
  doi       = {10.7937/K9/TCIA.2018.SMT36LPN},
  keywords  = {Computed Tomography,Invasive Adenocarcinoma,Pathology,Pulmonary Nodules},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/LoBgAg},
}

@Misc{dataset/Johnson2019,
  author    = {Johnson, Alistair E W and Pollard, Tom and Mark, Roger and Berkowitz, Seth and Horng, Steven},
  title     = {{The MIMIC-CXR Database}},
  year      = {2019},
  doi       = {10.13026/C2JT1Q},
  publisher = {physionet.org},
  url       = {https://physionet.org/content/mimic-cxr/},
}

@Misc{dataset/Yang2017,
  author    = {Yang, Jinzhong and Sharp, Greg and Veeraraghavan, Harini and {Van Elmpt}, Wouter and Dekker, Andre and Lustberg, Tim and Gooding, Mark},
  title     = {{Data from Lung CT Segmentation Challenge}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.3R3FVZ08},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/e41yAQ},
}

@Misc{dataset/Zhao,
  author = {Zhao, Jichao and Xiong, Zhaohan},
  title  = {{2018 Atrial Segmentation Challenge – Atrial Segmentation Challenge}},
  url    = {http://atriaseg2018.cardiacatlas.org/},
}

@Misc{dataset/Rister2019,
  author    = {Rister, Blaine and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
  title     = {{CT-ORG: A Dataset of CT Volumes With Multiple Organ Segmentations}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.TT7F4V7O},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/OgWkAw},
}

@Misc{dataset/Roth2016,
  author    = {Roth, Holger and Farag, Amal and Turkbey, Evrim B and Lu, Le and Liu, Jiamin and Summers, Ronald M},
  title     = {{Data From Pancreas-CT}},
  year      = {2016},
  doi       = {10.7937/K9/TCIA.2016.TNB1KQBU},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/eIlXAQ},
}

@Misc{dataset/Newitt2016,
  author    = {Newitt, David and Hylton, Nola},
  title     = {{Single site breast DCE-MRI data and segmentations from patients undergoing neoadjuvant chemotherapy}},
  year      = {2016},
  doi       = {10.7937/K9/TCIA.2016.QHsyhJKy},
  pages     = {The Cancer Imaging Archive},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/ZIhXAQ},
}

@Misc{dataset/LitjensGeertFutterer2015,
  author    = {{Litjens Geert; Futterer}, Jurgen; Huisman Henkjan;},
  title     = {{Data From Prostate-3T}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.QJTV5IL5},
  publisher = {The Cancer Imaging Archive},
  url       = {http://dx.doi.org/10.7937/K9/TCIA.2015.QJTV5IL5},
}

@Misc{dataset/Wee2019a,
  author    = {Wee, Leonard and Dekker, Andre},
  title     = {{Data from Head-Neck-Radiomics-HN1}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.8KAP372N},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/iBglAw},
}

@Misc{dataset/DGC2015,
  author = {{Drive Gran Challenge}},
  title  = {{Digital Retinal Images for Vessel Extraction}},
  year   = {2015},
  url    = {http://www.isi.uu.nl/Research/Databases/DRIVE/},
}

@Misc{dataset/Fu2020,
  author    = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovi{\'{c}}, Hrvoje and Sun, Xu and Liao, Jingan and Xu, Yanwu and Zhang, Shaochong and Zhang, Xiulan},
  title     = {{ADAM: Automatic Detection challenge on Age-related Macular degeneration}},
  year      = {2020},
  doi       = {10.21227/dt4f-rt59},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/dt4f-rt59},
}

@Misc{dataset/Marcus2019,
  author   = {Marcus, D S and Wang, T H and Parker, J and Csernansky, J G and Morris, J C and Buckner, R L},
  title    = {{Open Access Series of Imaging Studies (OASIS)}},
  year     = {2019},
  keywords = {MRI,data,longitudinal,neuroimaging,open source},
  number   = {31 May 2011},
  url      = {https://www.oasis-brains.org/%0Awww.oasis-brains.org%0Ahttp://www.oasis-brains.org/},
}

@Misc{dataset/Day2019,
  author  = {Day, Trevor K. M. and Madyastha, Tara M. and Boord, Peter and Askren, Mary K. and Montine, Thomas J. and Grabowski, Thomas J.},
  title   = {{ANT: Healthy aging and Parkinson's disease}},
  year    = {2019},
  url     = {https://openneuro.org/datasets/ds001907/versions/2.0.3},
  urldate = {2020-05-27},
}

@Misc{dataset/Tessa2018,
  author    = {Tessa, Carlo},
  title     = {{PD De Novo: Resting State fMRI and Physiological Signals}},
  year      = {2018},
  doi       = {10.18112/OPENNEURO.DS001354.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001354/versions/1.0.0},
}

@Misc{dataset/Boysen2017,
  author   = {Boysen, Jacob},
  title    = {{MRI and Alzheimers}},
  year     = {2017},
  keywords = {health,health sciences,healthcare,image data,medical facilities and services,neurological conditions,neurology,neuroscience,old age},
  url      = {https://www.kaggle.com/jboysen/mri-and-alzheimers},
  urldate  = {2020-05-25},
}

@Misc{dataset/Malekzadeh2019,
  author   = {Malekzadeh, S.},
  title    = {{MRI Hippocampus Segmentation | Kaggle}},
  year     = {2019},
  keywords = {biology,deep learning,health foundations and medical research,medical facilities and services,neurological conditions,object segmentation,old age},
  url      = {https://www.kaggle.com/sabermalek/mrihs},
  urldate  = {2020-05-25},
}

@Misc{dataset/Styner2008,
  author  = {Styner, Martin and Warfield, Simon and Lee, Joohwi},
  title   = {{MS lesion segmentation challenage 2008}},
  year    = {2008},
  url     = {http://www.ia.unc.edu/MSseg/},
  urldate = {2020-05-21},
}

@Misc{dataset/Pham2015,
  author  = {Pham, Dzung and Bazin, Pierre-Louis and Carass, Aaron and Calabresi, Peter and Crainiceanu, Ciprian and Ellingsen, Lotta and He, Qing and Prince, Jerry and Reich, Daniel and Roy, Snehashis},
  title   = {{MSChallenge - IACL}},
  year    = {2015},
  url     = {http://iacl.ece.jhu.edu/index.php/MSChallenge},
  urldate = {2020-05-21},
}

@Misc{dataset/Egger2016,
  author = {Egger, Karl and Maier, Oskar and Reyes, Mauricio and Wiest, Roland},
  title  = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2016}},
  year   = {2016},
  url    = {http://www.isles-challenge.org/ISLES2017/%0Ahttp://www.isles-challenge.org/ISLES2016/},
}

@Misc{dataset/Hakim2017,
  author  = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Winzeck, Stefan},
  title   = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2017}},
  year    = {2017},
  url     = {http://www.isles-challenge.org/ISLES2017/},
  urldate = {2020-05-20},
}

@Misc{dataset/Bakas2019,
  author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Colen, Rivka R. and Marcus, Daniel and Weber, Marc-Andre and Mahajan, Abhishek},
  title  = {{Multimodal Brain Tumor Segmentation Challenge 2019 | CBICA | Perelman School of Medicine at the University of Pennsylvania}},
  year   = {2019},
  url    = {https://www.med.upenn.edu/cbica/brats-2019/},
}

@Misc{dataset/Landman,
  author = {Landman, Bennett A and Anderson, Adam W and Schilling, Kurt and Alexander, Simon and Kerins, Fergal and Westin, C-F and Rathi, Yogesh and Dyrby, Tim B. and Descoteaux, Maxime and Houde, Jean-Christophe and Verma, Ragini and Pierpaoli, Carlo and Irfanoglu, Okan and Thomas, Cibu},
  title  = {{3-D Validation of Tractography with Experimental MRI (3D VoTEM)}},
  url    = {https://my.vanderbilt.edu/votem/},
}

@Misc{dataset/Zhang2019,
  author    = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
  title     = {{AGE: Angle closure Glaucoma Evaluation Challenge}},
  year      = {2019},
  doi       = {10.21227/petb-fy10},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/petb-fy10},
}

@Misc{dataset/Ikutani2020,
  author    = {Ikutani, Yoshiharu and {Takatomi Kubo} and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
  title     = {{Decoding functional category of source code from the brain (fMRI on Java program comprehension)}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002411.V1.1.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002411/versions/1.1.0},
}

@Misc{dataset/Zhang2020,
  author    = {Zhang, S and Yoshida, W and Mano, H and Yanagisawa, T and Shibata, K and Kawato, M and Seymour, B},
  title     = {{Cognitive control of sensory pain encoding in the pregenual anterior cingulate cortex. d1 - decoder construction in day 1, d2 - adaptive control in day 2.}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002596.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002596/versions/1.0.0},
}

@Misc{dataset/GuohuaShen2020,
  author    = {{Guohua Shen} and {Tomoyasu Horikawa} and Majima, Kei and {Yukiyasu Kamitani}},
  title     = {{Deep Image Reconstruction}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS001506.V1.3.1},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001506/versions/1.3.1},
}

@Misc{dataset/Commowick2018,
  author    = {Commowick, Olivier and Istace, Audrey and Kain, Michael and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Camarasu-Pop, Sorina and Girard, Pascal and Ameli, Roxana and Ferr{\'{e}}, Jean-Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and {Mc Kinley}, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\`{o}}, Xavier and Santos, Michel M and Santos, Wellington P and Silva-Filho, Abel G and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K and Cotton, Fran{\c{c}}ois and Barillot, Christian},
  title     = {{MICCAI 2016 MS lesion segmentation challenge: supplementary results}},
  year      = {2018},
  doi       = {10.5281/zenodo.1307653},
  publisher = {Zenodo},
  url       = {https://doi.org/10.5281/zenodo.1307653},
}

@Misc{dataset/Oskar2015,
  author  = {Oskar, Maier and Bj{\"{o}}rn, Menze and Mauricio, Reyes},
  title   = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2015}},
  year    = {2015},
  url     = {http://www.isles-challenge.org/ISLES2015/},
  urldate = {2020-05-20},
}

@Misc{dataset/Liu2018,
  author        = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  title         = {{Darts: Differentiable architecture search}},
  year          = {2018},
  abstract      = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  archiveprefix = {arXiv},
  arxivid       = {1806.09055},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019arXivarXiv},
  eprint        = {1806.09055},
  volume        = {abs/1806.0},
}

@Misc{dataset/GarciaLayana2017,
  author    = {Garcia-Layana, Alfredo and Ciuffo, Gianfranco and {Javier Zarranz-Ventura} and Alvarez-Vidal, Aurora},
  title     = {{Optical Coherence Tomography in Age-related Macular Degeneration}},
  year      = {2017},
  booktitle = {AMDBOOK},
  url       = {https://amdbook.org/content/optical-coherence-tomography-age-related-macular-degeneration},
  urldate   = {2022-05-12},
}

@Misc{dataset/Minati2009,
  author    = {Minati, Ludovico and Edginton, Trudi and {Grazia Bruzzone}, Maria and Giaccone, Giorgio},
  title     = {{Reviews: Current concepts in alzheimer's disease: A multidisciplinary review}},
  year      = {2009},
  abstract  = {This comprehensive, pedagogically-oriented review is aimed at a heterogeneous audience representative of the allied disciplines involved in research and patient care. After a foreword on epidemiology, genetics, and risk factors, the amyloid cascade model is introduced and the main neuropathological hallmarks are discussed. The progression of memory, language, visual processing, executive, attentional, and praxis deficits, and of behavioral symptoms is presented. After a summary on neuropsychological assessment, emerging biomarkers from cerebrospinal fluid assays, magnetic resonance imaging, nuclear medicine, and electrophysiology are discussed. Existing treatments are briefly reviewed, followed by an introduction to emerging disease-modifying therapies such as secretase modulators, inhibitors of Abeta aggregation, immunotherapy, inhibitors of tau protein phosphorylation, and delivery of nerve growth factor. {\textcopyright} 2009 Sage Publications.},
  booktitle = {American Journal of Alzheimer's Disease and other Dementias},
  doi       = {10.1177/1533317508328602},
  issn      = {15333175},
  keywords  = {Alzheimer's disease,Neuroimaging,Neuropathology,Neuropsychological testing,Pharmacotherapy},
  number    = {2},
  pages     = {95--121},
  pmid      = {19116299},
  volume    = {24},
}

@Misc{dataset/ArmatoIIIS.G.HadjiiskiL.TourassiG.D.DrukkerK.GigerM.L.LiF.2015,
  author    = {{Armato III, S. G., Hadjiiski, L., Tourassi, G.D., Drukker, K., Giger, M.L., Li, F.}, Redmond and {G., Farahani, K., Kirby, J.S. and Clarke}, L.P.},
  title     = {{SPIE-AAPM-NCI Lung Nodule Classification Challenge}},
  year      = {2015},
  booktitle = {Cancer Imaging Arch 10},
  doi       = {10.7937/K9/TCIA.2015.UZLSU3FL},
  pages     = {p.K9},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bImiAQ},
}

@Misc{dataset/Schmainda2018,
  author    = {Schmainda, K.M. and Prah, M.},
  title     = {{Brain-Tumor-Progression}},
  year      = {2018},
  abstract  = {This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images). The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent. All of the series are co-registered with the T1+C images. The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.},
  booktitle = {Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2018.15quzvnb},
  url       = {http://doi.org/10.7937/K9/TCIA.2018.15quzvnb},
}

@Misc{dataset/FLIIAM2016,
  author    = {FLI-IAM},
  title     = {{MS segmentation challenge using a data management and processing infrastructure}},
  year      = {2016},
  booktitle = {FLI-IAM France Life Imaging - Information Analysis and Management},
  url       = {https://portal.fli-iam.irisa.fr/msseg-challenge/overview},
  urldate   = {2020-05-21},
}

@Misc{dataset/Zhang2018,
  author        = {shi Zhang, Quan and chun Zhu, Song},
  title         = {{Visual interpretability for deep learning: a survey}},
  year          = {2018},
  abstract      = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  annote        = {_eprint: 1802.00614},
  archiveprefix = {arXiv},
  arxivid       = {1802.00614},
  booktitle     = {Frontiers of Information Technology and Electronic Engineering},
  doi           = {10.1631/FITEE.1700808},
  eprint        = {1802.00614},
  issn          = {20959230},
  keywords      = {Artificial intelligence,Deep learning,Interpretable model},
  number        = {1},
  pages         = {27--39},
  volume        = {19},
}

@Misc{dataset/Krizhevsky2009,
  author    = {Krizhevsky, A and Nair, V and Hinton, G},
  title     = {{CIFAR-10 and CIFAR-100 datasets}},
  year      = {2009},
  abstract  = {The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset},
  booktitle = {https://www.cs.toronto.edu/$\sim$kriz/cifar.html},
}

@Misc{dataset/Porwal2018,
  author    = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  title     = {{Indian Diabetic Retinopathy Image Dataset (IDRiD)}},
  year      = {2018},
  abstract  = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting working age population in the world. Recent research has given a better understanding of requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of population with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, the database for this challenge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. Moreover, it is the only dataset constituting typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
  booktitle = {IEEE Dataport},
  doi       = {10.21227/H25W98},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/H25W98},
}

@Misc{dataset/Bronstein2017,
  author        = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  title         = {{Geometric Deep Learning: Going beyond Euclidean data}},
  year          = {2017},
  abstract      = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
  annote        = {_eprint: 1611.08097},
  archiveprefix = {arXiv},
  arxivid       = {1611.08097},
  booktitle     = {IEEE Signal Processing Magazine},
  doi           = {10.1109/MSP.2017.2693418},
  eprint        = {1611.08097},
  issn          = {10535888},
  number        = {4},
  pages         = {18--42},
  url           = {http://arxiv.org/abs/1611.08097},
  volume        = {34},
}

@Misc{dataset/Gireesha2015,
  author    = {Gireesha, HM and S, Nanda},
  title     = {{Thyroid Nodule Segmentation And Classification In Ultrasound Images}},
  year      = {2015},
  abstract  = {Abstract—This paper proposes, a novel computer based approach for benign or malignant assessment of thyroid nodules in ultrasound images. Ultrasound imaging is one of the frequently used diagnosis tool to detect and classify abnormalities of the thyroid gland. The proposed approach comprises of four important stages: pre-processing, segmentation, feature extraction and classification. Rayleigh trimmed anisotropic diffusion filter is used for pre-processing. A watershed algorithm is used to segment the nodule region. An artificial neural network (ANN) and support vector machine (SVM) classifiers are employed for the classification task, utilizing feature vectors derived from gray level co-occurrence (GLCM) features. The classification results are evaluated with the use of accuracy, sensitivity and specificity. It is derived that SVM classifier provides better result than ANN for discriminating benign and malignant nodules, obtaining accuracy 92.5%, sensitivity 96.66% and specificity 80%.},
  booktitle = {International Journal of Engineering Research and Technology},
  doi       = {10.5281/ZENODO.3715942},
  keywords  = {"ijert",anisotropic diffusion,cad,computer aided diagnosis,thyroid ultrasound,watershed segmentation},
  number    = {5},
  pages     = {2252--2256},
  publisher = {Zenodo},
  url       = {file:///C:/Users/Anshi/Downloads/IJERTV3IS052114.pdf},
  volume    = {3},
}

@Misc{dataset/Levine2016,
  author        = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  title         = {{End-to-end training of deep visuomotor policies}},
  year          = {2016},
  abstract      = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  archiveprefix = {arXiv},
  arxivid       = {1504.00702},
  booktitle     = {Journal of Machine Learning Research},
  eprint        = {1504.00702},
  issn          = {15337928},
  keywords      = {Neural networks,Optimal control,Reinforcement learning,Vision},
  volume        = {17},
}

@Misc{dataset/APTOS2019,
  author    = {APTOS, Asia Pacific Tele-Ophthalmology Society},
  title     = {{APTOS 2019 blindness detection}},
  year      = {2019},
  abstract  = {Detect diabetic retinopathy to stop blindness before it's too late},
  booktitle = {Kaggle Competition},
  keywords  = {4th Symposium,Asia Pacific Tele-Ophthalmology Society(APTOS)},
  url       = {https://www.kaggle.com/c/aptos2019-blindness-detection/data},
}

@Misc{dataset/LinuxFoundation2020,
  author    = {{The Linux Foundation}},
  title     = {{Production-Grade Container Orchestration - Kubernetes}},
  year      = {2020},
  booktitle = {Kubernetes.Io},
  url       = {https://kubernetes.io/},
}

@Misc{dataset/Andrew1999,
  author    = {Andrew, Alex M.},
  title     = {{The Handbook of Brain Theory and Neural Networks}},
  year      = {1999},
  address   = {Cambridge, MA, USA},
  annote    = {Section: Convolutional Networks for Images, Speech, and Time Series},
  booktitle = {Kybernetes},
  doi       = {10.1108/k.1999.28.9.1084.1},
  editor    = {Arbib, Michael A},
  isbn      = {0-262-51102-9},
  issn      = {0368492X},
  keywords  = {Artificial intelligence,Brain,Cybernetics,Neural networks,Publication},
  number    = {9},
  pages     = {1084--1094},
  publisher = {MIT Press},
  url       = {http://dl.acm.org/citation.cfm?id=303568.303704},
  volume    = {28},
}

@Misc{dataset/Fischl2012,
  author    = {Fischl, Bruce},
  title     = {{FreeSurfer}},
  year      = {2012},
  abstract  = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. {\textcopyright} 2012 Elsevier Inc.},
  booktitle = {NeuroImage},
  doi       = {10.1016/j.neuroimage.2012.01.021},
  issn      = {10538119},
  keywords  = {MRI,Morphometry,Registration,Segmentation},
  number    = {2},
  pages     = {774--781},
  pmid      = {22248573},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
  volume    = {62},
}

@Misc{dataset/Iwatsubo2011,
  author    = {Iwatsubo, Takeshi},
  title     = {{[Alzheimer's disease Neuroimaging Initiative (ADNI)].}},
  year      = {2011},
  abstract  = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) unites researchers with study data as they work to define the progression of Alzheimer's disease (AD). ADNI researchers collect, validate and utilize data, including MRI and PET images, genetics, cognitive tests, CSF and blood biomarkers as predictors of the disease. Study resources and data from the North American ADNI study are available through this website, including Alzheimer's disease patients, mild cognitive impairment subjects, and elderly controls.},
  booktitle = {Nihon rinsho. Japanese journal of clinical medicine},
  issn      = {00471852},
  keywords  = {[1] “Alzheimer's Disease Neuroimaging Initiative.”},
  pages     = {570--574},
  pmid      = {22787853},
  url       = {http://adni.loni.usc.edu/},
  urldate   = {2020-05-25},
  volume    = {69 Suppl 8},
}

@Misc{dataset/Fu2019,
  author    = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovic, Hrvoje and Sun, Xu and Liao, Jingan and XU, Yanwu and ZHANG, Shaochong and ZHANG, Xiulan},
  title     = {{PALM: Pathologic myopia challenge}},
  year      = {2019},
  booktitle = {Proc. IEEE Dataport},
  doi       = {10.21227/55pk-8z03},
  pages     = {1},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/55pk-8z03},
  volume    = {[Online]},
}

@Misc{dataset/Shah2017,
  author    = {Shah, Vikas},
  title     = {{Lipid-rich adrenal adenoma}},
  year      = {2017},
  booktitle = {Radiology Case, Radiopaedia.org},
  doi       = {https://doi.org/10.53347/rID-49349},
  url       = {https://radiopaedia.org/cases/lipid-rich-adrenal-adenoma-1},
  urldate   = {2022-05-12},
}

@Misc{dataset/Desai2020,
  author    = {Desai, Shivang and Baghal, Ahmad and Wongsurawat, Thidathip and Jenjaroenpun, Piroon and Powell, Thomas and Al-Shukri, Shaymaa and Gates, Kim and Farmer, Phillip and Rutherford, Michael and Blake, Geri and Nolan, Tracy and Sexton, Kevin and Bennett, William and Smith, Kirk and Syed, Shorabuddin and Prior, Fred},
  title     = {{Chest imaging representing a COVID-19 positive rural U.S. population}},
  year      = {2020},
  abstract  = {As the COVID-19 pandemic unfolds, radiology imaging is playing an increasingly vital role in determining therapeutic options, patient management, and research directions. Publicly available data are essential to drive new research into disease etiology, early detection, and response to therapy. In response to the COVID-19 crisis, the National Cancer Institute (NCI) has extended the Cancer Imaging Archive (TCIA) to include COVID-19 related images. Rural populations are one population at risk for underrepresentation in such public repositories. We have published in TCIA a collection of radiographic and CT imaging studies for patients who tested positive for COVID-19 in the state of Arkansas. A set of clinical data describes each patient including demographics, comorbidities, selected lab data and key radiology findings. These data are cross-linked to SARS-COV-2 cDNA sequence data extracted from clinical isolates from the same population, uploaded to the GenBank repository. We believe this collection will help to address population imbalance in COVID-19 data by providing samples from this normally underrepresented population.},
  booktitle = {Scientific Data},
  doi       = {10.1038/s41597-020-00741-6},
  issn      = {20524463},
  number    = {1},
  pmid      = {33235265},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/C5IvB},
  volume    = {7},
}

@Misc{dataset/An2020,
  author    = {An, P and Xu, S and Harmon, S.A and Turkbey, E.B and Sanford, T.H. and Amalou, A. and Kassin, M. and Varble, N. and Blain, M. and Anderson, V. and Patella, F. and G., Carrafiello and Turkbey, B.T. and Wood, B.J.},
  title     = {{CT Images in Covid-19 [Data set]}},
  year      = {2020},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2020.GQRY-NC81},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/o5QvB},
}

@Misc{dataset/SmithK2015,
  author    = {{Smith K}, Clark K and Smith, K and Clark, K and Bennett, W and Nolan, T and Kirby, J and Wolfsberger, M and Moulton, J and Vendt, B and Freymann, J},
  title     = {{Data From CT_COLONOGRAPHY}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.NWTESAY1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/DQE2},
}

@Misc{dataset/Yorke2019,
  author    = {Yorke, Afua A. and McDonald, Gary C. and {Solis Jr.}, David and Guerrero., Thomas},
  title     = {{Pelvic Reference Data}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.woskq5oo},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/tQGJAw},
}

@Misc{dataset/LeaveyP.2019,
  author    = {{Leavey P.}, Sengupta A Rakheja D Daescu O Arunachalam H B & Mishra R},
  title     = {{Osteosarcoma data from UT Southwestern/UT Dallas for Viable and Necrotic Tumor Assessment}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.BVHJHDAS},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/xwElAw},
}

@Misc{dataset/Matek2019,
  author    = {Matek, Christian and Schwarz, Simone and Marr, Carsten and Spiekermann, Karsten},
  title     = {{A Single-cell Morphological Dataset of Leukocytes from AML Patients and Non-malignant Controls [Data set]}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/tcia.2019.36f5o9ld},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/fgWkAw},
}

@Misc{dataset/NapelSandy&Plevritis2014,
  author    = {{Napel, Sandy, & Plevritis}, Sylvia K.},
  title     = {{NSCLC Radiogenomics: Initial Stanford Study of 26 Cases. The Cancer Imaging Archive.}},
  year      = {2014},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2014.X7ONY6B1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/Gglp},
}

@Misc{dataset/Armato2015,
  author    = {III, Armato and G., Samuel and McLennan and Geoffrey and Bidaut and Luc and McNitt-Gray and Michael, F. and R., Meyer and Charles and Reeves},
  title     = {{Data From LIDC-IDRI}},
  year      = {2015},
  abstract  = {The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases. Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule > or =3 mm," "nodule <3 mm," and "non-nodule > or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.LO9QL9SX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/rgAe},
}

@Misc{dataset/Litjens2017,
  author    = {Litjens, Geert and Debats, Oscar and Barentsz, Jelle and Karssemeijer, Nico and Huisman, Henkjan},
  title     = {{ProstateX challenge data}},
  year      = {2017},
  abstract  = {This collection is a retrospective set of prostate MR studies. All studies included T2-weighted (T2W), proton density-weighted (PD-W), dynamic contrast enhanced (DCE), and diffusion-weighted (DW) imaging. The images were acquired on two different types of Siemens 3T MR scanners, the MAGNETOM Trio and Skyra. T2-weighted images were acquired using a turbo spin echo sequence and had a resolution of around 0.5 mm in plane and a slice thickness of 3.6 mm. The DCE time series was acquired using a 3-D turbo flash gradient echo sequence with a resolution of around 1.5 mm in-plane, a slice thickness of 4 mm and a temporal resolution of 3.5 s. The proton density weighted image was acquired prior to the DCE time series using the same sequence with different echo and repetition times and a different flip angle. Finally, the DWI series were acquired with a single-shot echo planar imaging sequence with a resolution of 2 mm in-plane and 3.6 mm slice thickness and with diffusion-encoding gradients in three directions. Three b-values were acquired (50, 400, and 800), and subsequently, the ADC map was calculated by the scanner software. All images were acquired without an endorectal coil.},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9TCIA.2017.MURS5CL},
  pages     = {K9TCIA},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/iIFpAQ},
  volume    = {10},
}

@Misc{dataset/Bloch2015a,
  author    = {Bloch, N.},
  title     = {{Nc-isbi 2013 challenge: automated segmentation of prostate structures}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.ZF0VLOPV},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/B4NEAQ},
}

@Misc{dataset/Bloch2015b,
  author    = {Bloch, B. Nicolas and Jain, Ashali and Jaffe, C. Carl},
  title     = {{Data From PROSTATE-DIAGNOSIS}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.FOQEUJVT},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/xgEy},
}

@Misc{dataset/Heller2019,
  author    = {Heller, N. and Sathianathen, N. and {Kalapara, A., Walczak}, E. and Moore, K. and Kaluzniak, H. and Rosenberg, J. and Blake, P. and Rengel, Z. and Oestreich, M. and Dean, J. and Tradewell, M. and Shah, A. and Tejpaul, R. and Edgerton, Z. and Peterson, M. and Raza, S. and Regmi, S. and Papanikolopoulos, N. and Weight, C.},
  title     = {{Data from C4KC-KiTS [Data set]}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.IX49E8NX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/UwakAw},
}

@Misc{dataset/Erickson2017,
  author    = {Erickson, Bradley and Akkus, Zeynettin and Sedlar, Jiri and Korfiatis, Panagiotis},
  title     = {{Data From LGG-1p19qDeletion}},
  year      = {2017},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2017.dwehtz9v},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/coKJAQ},
}

@Misc{dataset/Vallieres2017,
  author    = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang-Shu and Sultanem, Khalil},
  title     = {{Data from Head-Neck-PET-CT}},
  year      = {2017},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2017.8oje5q00},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/24pyAQ},
}

@Misc{dataset/Consortium2018,
  author    = {Consortium, National Cancer Institute Clinical Proteomic Tumor Analysis},
  title     = {{Radiology Data from the Clinical Proteomic Tumor Analysis Consortium Glioblastoma Multiforme [CPTAC-GBM] collection}},
  year      = {2018},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/k9/tcia.2018.3rje41q1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/gAHUAQ},
}

@Misc{dataset/LiP.WangS.LiT.LuJ.HuangFuY.&Wang2020,
  author    = {{Li, P., Wang, S., Li, T., Lu, J., HuangFu, Y., & Wang}, D.},
  title     = {{A Large-Scale CT and PET/CT Dataset for Lung Cancer Diagnosis [Data set]}},
  year      = {2020},
  booktitle = {The Cancer Imaging Archive.},
  doi       = {10.7937/TCIA.2020.NNC2-0461},
  publisher = {The Cancer Imaging Archive},
  url       = {https://doi.org/10.7937/TCIA.2020.NNC2-0461},
}

@Misc{dataset/Holger2015,
  author    = {Holger, Roth and Lu, Le and Seff, Ari and Cherry, Kevin M and Hoffman, Joanne and Wang, Shijun and Summers, Ronald M},
  title     = {{A new 2.5 D representation for lymph node detection in CT.}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive.},
  doi       = {10.7937/K9/TCIA.2015.AQIIDCNM},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/0gAtAQ},
}

@Misc{dataset/NatashaHonomichl2019,
  author    = {{Natasha Honomichl}},
  title     = {{SN-AM Dataset: White Blood cancer dataset of B-ALL and MM for stain normalization}},
  year      = {2019},
  booktitle = {Wiki Cancer Image archive},
  doi       = {10.7937/TCIA.2019.OF2W8LXR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/EQIlAw},
}

@Misc{dataset/Stanford2011,
  author    = {Stanford, Stanford Center for Reproducible Neuroscience},
  title     = {{OpenNEURO}},
  year      = {2011},
  abstract  = {A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data},
  booktitle = {World Wide Web},
}

@Misc{dataset/,
  title   = {{Evaluation Criterion | LVQuan18 - Left Ventricle Full Quantification Challenge MICCAI2018}},
  url     = {https://lvquan18.github.io/2018/03/12/criterion.html},
  urldate = {2022-05-20},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: keypatterndefault:dataset/[auth][year];}
