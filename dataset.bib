@Misc{challenge/ISBI/MEMENTO2020,
  title   = {{MRI White Matter Reconstruction | ISBI 2019/2020 MEMENTO Challenge}},
  url     = {https://my.vanderbilt.edu/memento/},
  urldate = {2021-03-11},
}

@Misc{challenge/NEATBrainS,
  title   = {{NEATBrainS - Image Sciences Institute}},
  url     = {https://www.isi.uu.nl/research/challenges/neatbrains/},
  urldate = {2021-03-11},
}

@Misc{challenge/AccelMR2020,
  title   = {{AccelMR 2020 Prediction Challenge – AccelMR 2020 for ISBI 2020}},
  url     = {https://accelmrorg.wordpress.com/},
  urldate = {2021-03-11},
}

@Misc{challenge/ODIR2019,
  title = {{Peking University International Competition on Ocular Disease Intelligent Recognition}},
  url   = {https://odir2019.grand-challenge.org/},
}

@Misc{challenge/LOLA11,
  title = {{Home - LOLA11 - Grand Challenge}},
  url   = {https://lola11.grand-challenge.org/Home/},
}

@Article{challenge/AMOS22,
  author        = {Ji, Yuanfeng and Bai, Haotian and Yang, Jie and Ge, Chongjian and Zhu, Ye and Zhang, Ruimao and Li, Zhen and Zhang, Lingyan and Ma, Wanling and Wan, Xiang and Luo, Ping},
  title         = {{AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation}},
  year          = {2022},
  month         = {jun},
  abstract      = {Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.},
  archiveprefix = {arXiv},
  arxivid       = {2206.08023},
  eprint        = {2206.08023},
  url           = {http://arxiv.org/abs/2206.08023},
}

@Article{challenge/MMWHS,
  author        = {Zhuang, Xiahai and Li, Lei and Payer, Christian and {\v{S}}tern, Darko and Urschler, Martin and Heinrich, Mattias P. and Oster, Julien and Wang, Chunliang and Smedby, {\"{O}}rjan and Bian, Cheng and Yang, Xin and Heng, Pheng Ann and Mortazi, Aliasghar and Bagci, Ulas and Yang, Guanyu and Sun, Chenchen and Galisot, Gaetan and Ramel, Jean Yves and Brouard, Thierry and Tong, Qianqian and Si, Weixin and Liao, Xiangyun and Zeng, Guodong and Shi, Zenglin and Zheng, Guoyan and Wang, Chengjia and MacGillivray, Tom and Newby, David and Rhode, Kawal and Ourselin, Sebastien and Mohiaddin, Raad and Keegan, Jennifer and Firmin, David and Yang, Guang},
  journal       = {Medical Image Analysis},
  title         = {{Evaluation of algorithms for Multi-Modality Whole Heart Segmentation: An open-access grand challenge}},
  year          = {2019},
  issn          = {13618423},
  month         = {dec},
  volume        = {58},
  abstract      = {Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be challenging due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, an initial set of training data is generally needed for constructing priors or for training. Furthermore, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provided 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results showed that the performance of CT WHS was generally better than that of MRI WHS. The segmentation of the substructures for different categories of patients could present different levels of challenge due to the difference in imaging and variations of heart shapes. The deep learning (DL)-based methods demonstrated great potential, though several of them reported poor results in the blinded evaluation. Their performance could vary greatly across different network structures and training strategies. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated good performance, though the accuracy and computational efficiency could be limited. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage (www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/).},
  archiveprefix = {arXiv},
  arxivid       = {1902.07880},
  doi           = {10.1016/j.media.2019.101537},
  eprint        = {1902.07880},
  keywords      = {Benchmark,Challenge,Multi-modality,Whole Heart Segmentation},
  pmid          = {31446280},
  publisher     = {Elsevier B.V.},
}

@Article{challenge/autoimplant,
  author        = {Li, Jianning and Pepe, Antonio and Gsaxner, Christina and Campe, Gord von and Egger, Jan},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{A Baseline Approach for AutoImplant: The MICCAI 2020 Cranial Implant Design Challenge}},
  year          = {2020},
  issn          = {16113349},
  month         = {jun},
  pages         = {75--84},
  volume        = {12445 LNCS},
  abstract      = {In this study, we present a baseline approach for AutoImplant (https://autoimplant.grand-challenge.org/) – the cranial implant design challenge, which can be formulated as a volumetric shape learning task. In this task, the defective skull, the complete skull and the cranial implant are represented as binary voxel grids. To accomplish this task, the implant can be either reconstructed directly from the defective skull or obtained by taking the difference between a defective skull and a complete skull. In the latter case, a complete skull has to be reconstructed given a defective skull, which defines a volumetric shape completion problem. Our baseline approach for this task is based on the former formulation, i.e., a deep neural network is trained to predict the implants directly from the defective skulls. The approach generates high-quality implants in two steps: First, an encoder-decoder network learns a coarse representation of the implant from downsampled, defective skulls; The coarse implant is only used to generate the bounding box of the defected region in the original high-resolution skull. Second, another encoder-decoder network is trained to generate a fine implant from the bounded area. On the test set, the proposed approach achieves an average dice similarity score (DSC) of 0.8555 and Hausdorff distance (HD) of 5.1825 mm. The codes are available at https://github.com/Jianningli/autoimplant.},
  archiveprefix = {arXiv},
  arxivid       = {2006.12449},
  doi           = {10.1007/978-3-030-60946-7_8},
  eprint        = {2006.12449},
  isbn          = {9783030609450},
  keywords      = {Cranial implant design,Cranioplasty,Deep learning,Shape learning,Skull reconstruction,Volumetric shape completion},
  url           = {http://arxiv.org/abs/2006.12449},
}

@Article{challenge/miccai/CAMELYON17,
  author   = {B{\'{a}}ndi, P{\'{e}}ter and Geessink, Oscar and Manson, Quirine and {Van Dijk}, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and {Ehteshami Bejnordi}, Babak and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and Li, Quanzheng and Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Fukuta, Keisuke and Komura, Daisuke and Ovtcharov, Vlado and Cheng, Shenghua and Zeng, Shaoqun and Thagaard, Jeppe and Dahl, Anders B. and Lin, Huangjing and Chen, Hao and Jacobsson, Ludwig and Hedlund, Martin and {\c{C}}etin, Melih and Halici, Eren and Jackson, Hunter and Chen, Richard and Both, Fabian and Franke, J{\"{o}}rg and Kusters-Vandevelde, Heidi and Vreuls, Willem and Bult, Peter and {Van Ginneken}, Bram and {Van Der Laak}, Jeroen and Litjens, Geert},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge}},
  year     = {2019},
  issn     = {1558254X},
  month    = {feb},
  number   = {2},
  pages    = {550--560},
  volume   = {38},
  abstract = {Automated detection of cancer metastases in lymph nodes has the potential to improve the assessment of prognosis for patients. To enable fair comparison between the algorithms for this purpose, we set up the CAMELYON17 challenge in conjunction with the IEEE International Symposium on Biomedical Imaging 2017 Conference in Melbourne. Over 300 participants registered on the challenge website, of which 23 teams submitted a total of 37 algorithms before the initial deadline. Participants were provided with 899 whole-slide images (WSIs) for developing their algorithms. The developed algorithms were evaluated based on the test set encompassing 100 patients and 500 WSIs. The evaluation metric used was a quadratic weighted Cohen's kappa. We discuss the algorithmic details of the 10 best pre-conference and two post-conference submissions. All these participants used convolutional neural networks in combination with pre- and postprocessing steps. Algorithms differed mostly in neural network architecture, training strategy, and pre- and postprocessing methodology. Overall, the kappa metric ranged from 0.89 to -0.13 across all submissions. The best results were obtained with pre-trained architectures such as ResNet. Confusion matrix analysis revealed that all participants struggled with reliably identifying isolated tumor cells, the smallest type of metastasis, with detection rates below 40%. Qualitative inspection of the results of the top participants showed categories of false positives, such as nerves or contamination, which could be targeted for further optimization. Last, we show that simple combinations of the top algorithms result in higher kappa metric values than any algorithm individually, with 0.93 for the best combination.},
  doi      = {10.1109/TMI.2018.2867350},
  keywords = {Breast cancer,grand challenge,lymph node metastases,sentinel lymph node,whole-slide images},
  pmid     = {30716025},
  url      = {https://ieeexplore.ieee.org/document/8447230/},
}

@Article{challenge/BACH,
  author        = {Aresta, Guilherme and Ara{\'{u}}jo, Teresa and Kwok, Scotty and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Marami, Bahram and Prastawa, Marcel and Chan, Monica and Donovan, Michael and Fernandez, Gerardo and Zeineh, Jack and Kohl, Matthias and Walz, Christoph and Ludwig, Florian and Braunewell, Stefan and Baust, Maximilian and Vu, Quoc Dang and To, Minh Nguyen Nhat and Kim, Eal and Kwak, Jin Tae and Galal, Sameh and Sanchez-Freire, Veronica and Brancati, Nadia and Frucci, Maria and Riccio, Daniel and Wang, Yaqi and Sun, Lingling and Ma, Kaiqiang and Fang, Jiannan and Kone, Ismael and Boulmane, Lahsen and Campilho, Aur{\'{e}}lio and Eloy, Catarina and Pol{\'{o}}nia, Ant{\'{o}}nio and Aguiar, Paulo},
  journal       = {Medical Image Analysis},
  title         = {{BACH: Grand challenge on breast cancer histology images}},
  year          = {2019},
  issn          = {13618423},
  month         = {aug},
  pages         = {122--139},
  volume        = {56},
  abstract      = {Breast cancer is the most common invasive cancer in women, affecting more than 10% of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). BACH aimed at the classification and localization of clinically relevant histopathological classes in microscopy and whole-slide images from a large annotated dataset, specifically compiled and made publicly available for the challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. The submitted algorithms improved the state-of-the-art in automatic classification of breast cancer with microscopy images to an accuracy of 87%. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publicly available as to promote further improvements to the field of automatic classification in digital pathology.},
  archiveprefix = {arXiv},
  arxivid       = {1808.04277},
  doi           = {10.1016/j.media.2019.05.010},
  eprint        = {1808.04277},
  keywords      = {Breast cancer,Challenge,Comparative study,Deep learning,Digital pathology,Histology},
  pmid          = {31226662},
  url           = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518307941},
}

@Article{challenge/MICCAI/CAMELYON16,
  author   = {Litjens, Geert and Bandi, Peter and Bejnordi, Babak Ehteshami and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob and Manson, Quirine F. and Stathonikos, Nikolas and Baidoshvili, Alexi and van Diest, Paul and Wauters, Carla and van Dijk, Marcory and van der Laak, Jeroen},
  journal  = {GigaScience},
  title    = {{1399 H&E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset}},
  year     = {2018},
  issn     = {2047217X},
  month    = {jun},
  number   = {6},
  volume   = {7},
  abstract = {Background: The presence of lymph node metastases is one of the most important factors in breast cancer prognosis. The most common way to assess regional lymph node status is the sentinel lymph node procedure. The sentinel lymph node is the most likely lymph node to contain metastasized cancer cells and is excised, histopathologically processed, and examined by a pathologist. This tedious examination process is time-consuming and can lead to small metastases being missed. However, recent advances in whole-slide imaging and machine learning have opened an avenue for analysis of digitized lymph node sections with computer algorithms. For example, convolutional neural networks, a type of machine-learning algorithm, can be used to automatically detect cancer metastases in lymph nodes with high accuracy. To train machine-learning models, large, well-curated datasets are needed. Results: We released a dataset of 1,399 annotated whole-slide images (WSIs) of lymph nodes, both with and without metastases, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYON17 Grand Challenges. Slides were collected from five medical centers to cover a broad range of image appearance and staining variations. Each WSI has a slide-level label indicating whether it contains no metastases, macro-metastases, micro-metastases, or isolated tumor cells. Furthermore, for 209 WSIs, detailed hand-drawn contours for all metastases are provided. Last, open-source software tools to visualize and interact with the data have been made available. Conclusions: A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use.},
  doi      = {10.1093/gigascience/giy065},
  keywords = {Breast cancer,Grand challenge,Lymph node metastases,Sentinel node,Whole-slide images},
  pmid     = {29860392},
  url      = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy065/5026175},
}

@Article{challenge/Lyon19,
  author   = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and van Rijthoven, Mart and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
  journal  = {Medical Image Analysis},
  title    = {{Learning to detect lymphocytes in immunohistochemistry with deep learning}},
  year     = {2019},
  issn     = {13618423},
  volume   = {58},
  abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation ($\kappa$=0.72), whereas the average pathologists agreement with reference standard was $\kappa$=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
  doi      = {10.1016/j.media.2019.101547},
  keywords = {Computational pathology,Deep learning,Immune cell detection,Immunohistochemistry},
  pmid     = {31476576},
}

@Article{challenge/isbi2012/us,
  author   = {Rueda, Sylvia and Fathima, Sana and Knight, Caroline L. and Yaqub, Mohammad and Papageorghiou, Aris T. and Rahmatullah, Bahbibi and Foi, Alessandro and Maggioni, Matteo and Pepe, Antonietta and Tohka, Jussi and Stebbing, Richard V. and McManigle, John E. and Ciurte, Anca and Bresson, Xavier and Cuadra, Meritxell Bach and Sun, Changming and Ponomarev, Gennady V. and Gelfand, Mikhail S. and Kazanov, Marat D. and Wang, Ching Wei and Chen, Hsiang Chou and Peng, Chun Wei and Hung, Chu Mei and Noble, J. Alison},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Evaluation and comparison of current fetal ultrasound image segmentation methods for biometric measurements: A grand challenge}},
  year     = {2014},
  issn     = {02780062},
  number   = {4},
  pages    = {797--813},
  volume   = {33},
  abstract = {This paper presents the evaluation results of the methods submitted to Challenge US: Biometric Measurements from Fetal Ultrasound Images, a segmentation challenge held at the IEEE International Symposium on Biomedical Imaging 2012. The challenge was set to compare and evaluate current fetal ultrasound image segmentation methods. It consisted of automatically segmenting fetal anatomical structures to measure standard obstetric biometric parameters, from 2D fetal ultrasound images taken on fetuses at different gestational ages (21 weeks, 28 weeks, and 33 weeks) and with varying image quality to reflect data encountered in real clinical environments. Four independent sub-challenges were proposed, according to the objects of interest measured in clinical practice: abdomen, head, femur, and whole fetus. Five teams participated in the head sub-challenge and two teams in the femur sub-challenge, including one team who tackled both. Nobody attempted the abdomen and whole fetus sub-challenges. The challenge goals were two-fold and the participants were asked to submit the segmentation results as well as the measurements derived from the segmented objects. Extensive quantitative (region-based, distance-based, and Bland-Altman measurements) and qualitative evaluation was performed to compare the results from a representative selection of current methods submitted to the challenge. Several experts (three for the head sub-challenge and two for the femur sub-challenge), with different degrees of expertise, manually delineated the objects of interest to define the ground truth used within the evaluation framework. For the head sub-challenge, several groups produced results that could be potentially used in clinical settings, with comparable performance to manual delineations. The femur sub-challenge had inferior performance to the head sub-challenge due to the fact that it is a harder segmentation problem and that the techniques presented relied more on the femur's appearance. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2013.2276943},
  keywords = {Challenge,Ultrasound (US),evaluation,fetal biometry,image quality,segmentation},
  pmid     = {23934664},
}

@Article{challenge/lungseg2017,
  author   = {Yang, Jinzhong and Veeraraghavan, Harini and Armato, Samuel G. and Farahani, Keyvan and Kirby, Justin S. and Kalpathy-Kramer, Jayashree and van Elmpt, Wouter and Dekker, Andre and Han, Xiao and Feng, Xue and Aljabar, Paul and Oliveira, Bruno and van der Heyden, Brent and Zamdborg, Leonid and Lam, Dao and Gooding, Mark and Sharp, Gregory C.},
  journal  = {Medical Physics},
  title    = {{Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017}},
  year     = {2018},
  issn     = {00942405},
  number   = {10},
  pages    = {4568--4581},
  volume   = {45},
  abstract = {Purpose: This report presents the methods and results of the Thoracic Auto-Segmentation Challenge organized at the 2017 Annual Meeting of American Association of Physicists in Medicine. The purpose of the challenge was to provide a benchmark dataset and platform for evaluating performance of autosegmentation methods of organs at risk (OARs) in thoracic CT images. Methods : Sixty thoracic CT scans provided by three different institutions were separated into 36 training, 12 offline testing, and 12 online testing scans. Eleven participants completed the offline challenge, and seven completed the online challenge. The OARs were left and right lungs, heart, esophagus, and spinal cord. Clinical contours used for treatment planning were quality checked and edited to adhere to the RTOG 1106 contouring guidelines. Algorithms were evaluated using the Dice coefficient, Hausdorff distance, and mean surface distance. A consolidated score was computed by normalizing the metrics against interrater variability and averaging over all patients and structures. Results : The interrater study revealed highest variability in Dice for the esophagus and spinal cord, and in surface distances for lungs and heart. Five out of seven algorithms that participated in the online challenge employed deep-learning methods. Although the top three participants using deep learning produced the best segmentation for all structures, there was no significant difference in the performance among them. The fourth place participant used a multi-atlas-based approach. The highest Dice scores were produced for lungs, with averages ranging from 0.95 to 0.98, while the lowest Dice scores were produced for esophagus, with a range of 0.55–0.72. Conclusion : The results of the challenge showed that the lungs and heart can be segmented fairly accurately by various algorithms, while deep-learning methods performed better on the esophagus. Our dataset together with the manual contours for all training cases continues to be available publicly as an ongoing benchmarking resource.},
  doi      = {10.1002/mp.13141},
  keywords = {automatic segmentation,grand challenge,lung cancer,radiation therapy},
  pmid     = {30144101},
}

@Misc{challenge/CHAOS,
  author        = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Aslan, Sinem and Conze, Pierre Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and {\"{O}}zkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and N{\"{u}}rnberger, Andreas and Maier-Hein, Klaus H. and {Bozdağı Akar}, G{\"{o}}zde and {\"{U}}nal, G{\"{o}}zde and Dicle, Oğuz and Selver, M. Alper},
  title         = {{CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation}},
  year          = {2021},
  abstract      = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance are hard to interpret. This makes comparative analysis a necessary tool towards interpretable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal semantic segmentation tasks has been rarely discussed. In order to expand the knowledge on these topics, the CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Abdominal organ segmentation from routine acquisitions plays an important role in several clinical applications, such as pre-surgical planning or morphological and volumetric follow-ups for various diseases. These applications require a certain level of performance on a diverse set of metrics such as maximum symmetric surface distance (MSSD) to determine surgical error-margin or overlap errors for tracking size and shape differences. Previous abdomen related challenges are mainly focused on tumor/lesion detection and/or classification with a single modality. Conversely, CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks were designed to analyze the capabilities of participating approaches from multiple perspectives. The results were investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 ± 0.00 / 0.95 ± 0.01), but the best MSSD performance remains limited (21.89 ± 13.94 / 20.85 ± 10.63 mm). The performances of participating models decrease dramatically for cross-modality tasks both for the liver (DICE: 0.88 ± 0.15 MSSD: 36.33 ± 21.97 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs are observed to perform worse compared to organ-specific ones (performance drop around 5%). Nevertheless, some of the successful models show better performance with their multi-organ versions. We conclude that the exploration of those pros and cons in both single vs multi-organ and cross-modality segmentations is poised to have an impact on further research for developing effective algorithms that would support real-world clinical applications. Finally, having more than 1500 participants and receiving more than 550 submissions, another important contribution of this study is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomenon.},
  archiveprefix = {arXiv},
  arxivid       = {2001.06535},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2020.101950},
  eprint        = {2001.06535},
  issn          = {13618423},
  keywords      = {Abdomen,Challenge,Cross-modality,Segmentation},
  pages         = {1--10},
  pmid          = {33421920},
  publisher     = {Zenodo},
  url           = {https://chaos.grand-challenge.org/Combined_Healthy_Abdominal_Organ_Segmentation/},
  volume        = {69},
}

@Article{challenge/sliver07,
  author   = {Heimann, Tobias and {Van Ginneken}, Brain and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, Gy{\"{o}}rgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M.M. and Chi, Ying and C{\'{o}}rdova, Andr{\'{e}}s and Dawant, Benoit M. and Fidrich, M{\'{a}}rta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and Kainm{\"{u}}ller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans Peter and N{\'{e}}meth, G{\'{a}}bor and Raicu, Daniela S. and Rau, Anne Mareike and {Van Rikxoort}, Eva M. and Rousson, Mika{\"{e}}l and Rusk{\'{o}}, L{\'{a}}szlo and Saddi, Kinda A. and Schmidt, G{\"{u}}nter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Comparison and evaluation of methods for liver segmentation from CT datasets}},
  year     = {2009},
  issn     = {02780062},
  number   = {8},
  pages    = {1251--1265},
  volume   = {28},
  abstract = {This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the "MICCAI 2007 Grand Challenge" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques. {\textcopyright} 2009 IEEE.},
  doi      = {10.1109/TMI.2009.2013851},
  keywords = {Evaluation,Liver,Segmentation},
  pmid     = {19211338},
}

@Article{challenge/promise12,
  author   = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
  journal  = {Medical Image Analysis},
  title    = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
  year     = {2014},
  issn     = {13618415},
  abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p < 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
  doi      = {10.1016/j.media.2013.12.002},
  keywords = {Challenge,MRI,Prostate,Segmentation},
}

@Misc{challenge/ROCC,
  author = {Rabbani, Hossein and Rasti, Reza and Kafieh, Rahele},
  title  = {{ROCC - Retinal OCT Classification Challenge (ROCC)}},
  year   = {2017},
  url    = {https://rocc.grand-challenge.org/},
}

@Misc{challenge/RFGC,
  title = {{Retinal Fundus Glaucoma Challenge}},
  url   = {https://refuge.grand-challenge.org/},
}

@Misc{challenge/refuge20,
  author        = {Orlando, Jos{\'{e}} Ignacio and Fu, Huazhu and {Barbossa Breda}, Jo{\~{a}}o and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andr{\'{e}}s and Fang, Ruogu and Heng, Pheng Ann and Kim, Jeyoung and Lee, Joon Ho and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunovi{\'{c}}, Hrvoje},
  title         = {{REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs}},
  year          = {2020},
  abstract      = {Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.},
  archiveprefix = {arXiv},
  arxivid       = {1910.03667},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2019.101570},
  eprint        = {1910.03667},
  issn          = {13618423},
  keywords      = {Deep learning,Fundus photography,Glaucoma,Image classification,Image segmentation},
  pmid          = {31630011},
  volume        = {59},
}

@Article{challenge/IDRiD,
  author   = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, Tian Bo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^{a}}nia and Ara{\'{u}}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c{c}}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'{e}}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'{e}} and M{\'{e}}riaudeau, Fabrice},
  journal  = {Medical Image Analysis},
  title    = {{IDRiD: Diabetic Retinopathy – Segmentation and Grading Challenge}},
  year     = {2020},
  issn     = {13618423},
  month    = {jan},
  pages    = {101561},
  volume   = {59},
  abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
  doi      = {10.1016/j.media.2019.101561},
  keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis},
  pmid     = {31671320},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841519301033},
}

@Misc{challenge/CADDementia/Home,
  author = {Bron, Esther E. and Klein, Stefan and Smits, Marion and van Swieten, John C. and Niessen, Wiro J.},
  title  = {{CADDementia - Home}},
  year   = {2014},
  url    = {https://caddementia.grand-challenge.org/},
}

@Article{challenge/TADPOLE,
  author        = {Marinescu, Razvan V. and Oxtoby, Neil P. and Young, Alexandra L. and Bron, Esther E. and Toga, Arthur W. and Weiner, Michael W. and Barkhof, Frederik and Fox, Nick C. and Eshaghi, Arman and Toni, Tina and Salaterski, Marcin and Lunina, Veronika and Ansart, Manon and Durrleman, Stanley and Lu, Pascal and Iddi, Samuel and Li, Dan and Thompson, Wesley K. and Donohue, Michael C. and Nahon, Aviv and Levy, Yarden and Halbersberg, Dan and Cohen, Mariya and Liao, Huiling and Li, Tengfei and Yu, Kaixian and Zhu, Hongtu and Tamez-Pe{\~{n}}a, Jos{\'{e}} G. and Ismail, Aya and Wood, Timothy and Bravo, Hector Corrada and Nguyen, Minh and Sun, Nanbo and Feng, Jiashi and {Thomas Yeo}, B. T. and Chen, Gang and Qi, Ke and Chen, Shiyang and Qiu, Deqiang and Buciuman, Ionut and Kelner, Alex and Pop, Raluca and Rimocea, Denisa and Ghazi, Mostafa M. and Nielsen, Mads and Ourselin, Sebastien and S{\o}rensen, Lauge and Venkatraghavan, Vikram and Liu, Keli and Rabe, Christina and Manser, Paul and Hill, Steven M. and Howlett, James and Huang, Zhiyue and Kiddle, Steven and Mukherjee, Sach and Rouanet, Ana{\"{i}}s and Taschler, Bernd and Tom, Brian D.M. and White, Simon R. and Faux, Noel and Sedai, Suman and {de Velasco Oriol}, Javier and Clemente, Edgar E.V. and Estrada, Karol and Aksman, Leon and Altmann, Andre and Stonnington, Cynthia M. and Wang, Yalin and Wu, Jianfeng and Devadas, Vivek and Fourrier, Clementine and Raket, Lars Lau and Sotiras, Aristeidis and Erus, Guray and Doshi, Jimit and Davatzikos, Christos and Vogel, Jacob and Doyle, Andrew and Tam, Angela and Diaz-Papkovich, Alex and Jammeh, Emmanuel and Koval, Igor and Moore, Paul and Lyons, Terry J. and Gallacher, John and Tohka, Jussi and Ciszek, Robert and Jedynak, Bruno and Pandya, Kruti and Bilgel, Murat and Engels, William and Cole, Joseph and Golland, Polina and Klein, Stefan and Alexander, Daniel C.},
  journal       = {arXiv},
  title         = {{The Alzheimer's disease prediction of longitudinal evolution (tadpole) challenge: Results after 1 year follow-up}},
  year          = {2020},
  issn          = {23318422},
  month         = {feb},
  abstract      = {Accurate prediction of progression in subjects at risk of Alzheimer's disease is crucial for enrolling the right subjects in clinical trials. However, a prospective comparison of state-of-the-art algorithms for predicting disease onset and progression is currently lacking. We present the findings of The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guessing. On a limited, cross-sectional subset of the data emulating clinical trials, performance of best algorithms at predicting clinical diagnosis decreased only slightly (3% error increase) compared to the full longitudinal dataset. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as patient-specific biomarker trends. The submission system remains open via the website https://tadpole.grand-challenge.org, while code for submissions is being collated by TADPOLE SHARE: https://tadpole-share.github.io/. Our work suggests that current prediction algorithms are accurate for biomarkers related to clinical diagnosis and ventricle volume, opening up the possibility of cohort refinement in clinical trials for Alzheimer's disease.},
  archiveprefix = {arXiv},
  arxivid       = {2002.03419},
  eprint        = {2002.03419},
  url           = {http://arxiv.org/abs/2002.03419},
}

@Misc{challenge/TADPOLE/HOME,
  title   = {{TADPOLE - Home}},
  url     = {https://tadpole.grand-challenge.org/},
  urldate = {2020-05-25},
}

@Misc{challenge/AAPM/RT-MAC,
  author    = {Cardenas, Carlos E and Mohamed, A and Sharp, G. and Gooding, M. and Veeraraghavan, H. and Yang, J.},
  title     = {{Data from AAPM RT-MAC Grand Challenge 2019}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/tcia.2019.bcfjqfqb},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bAP9Ag},
}

@Article{challenge/MICCAI07/3D-seg,
  author   = {van Ginneken, Bram and Heimann, Tobias and Styner, Martin},
  journal  = {International Conference on Medical Image Computing and Computer Assisted Intervention},
  title    = {{3D segmentation in the clinic: A grand challeng}},
  year     = {2007},
  pages    = {7--15},
  volume   = {10},
  abstract = {This paper describes the setup of a segmentation competition for the automatic extraction of Multiple Sclerosis (MS) lesions from brain Magnetic Resonance Imaging (MRI) data. This competition is one of three competitions that make up a comparison workshop at the 2008 Medical Image Computing and Computer Assisted Intervention (MICCAI) conference and was modeled after the successful comparison workshop on liver and caudate segmentation at the 2007 MICCAI conference. In this paper, the rationale for organizing the competition is discussed, the training and test data sets for both segmentation tasks are described and the scoring system used to evaluate the segmentation is presented.},
  url      = {http://grand-challenge2008.bigr.nl/proceedings/pdfs/msls08/Styner.pdf},
}

@Misc{challenge/MRBrainS18,
  author = {Kuijf, Hugo J. and Bennink, H. Edwin and Biessels, Geert Jan and Viergever, Max A. and Weaver, Nick A. and Vincken, Koen L.},
  title  = {{MRBrainS18 - Grand Challenge on MR Brain Segmentation 2018}},
  year   = {2018},
  url    = {https://mrbrains18.isi.uu.nl/},
}

@Article{challenge/MICCAI09/3D-seg,
  author   = {Hameeteman, K. and Zuluaga, M. A. and Freiman, M. and Joskowicz, L. and Cuisenaire, O. and Valencia, L. Fl{\'{o}}rez and G{\"{u}}ls{\"{u}}n, M. A. and Krissian, K. and Mille, J. and Wong, W. C.K. and Orkisz, M. and Tek, H. and Hoyos, M. Hern{\'{a}}ndez and Benmansour, F. and Chung, A. C.S. and Rozie, S. and van Gils, M. and van den Borne, L. and Sosna, J. and Berman, P. and Cohen, N. and Douek, P. C. and S{\'{a}}nchez, I. and Aissat, M. and Schaap, M. and Metz, C. T. and Krestin, G. P. and van der Lugt, A. and Niessen, W. J. and {Van Walsum}, T.},
  journal  = {Medical Image Analysis},
  title    = {{Evaluation framework for carotid bifurcation lumen segmentation and stenosis grading}},
  year     = {2011},
  issn     = {13618415},
  month    = {aug},
  number   = {4},
  pages    = {477--488},
  volume   = {15},
  abstract = {This paper describes an evaluation framework that allows a standardized and objective quantitative comparison of carotid artery lumen segmentation and stenosis grading algorithms. We describe the data repository comprising 56 multi-center, multi-vendor CTA datasets, their acquisition, the creation of the reference standard and the evaluation measures. This framework has been introduced at the MICCAI 2009 workshop 3D Segmentation in the Clinic: A Grand Challenge III, and we compare the results of eight teams that participated. These results show that automated segmentation of the vessel lumen is possible with a precision that is comparable to manual annotation. The framework is open for new submissions through the website http://cls2009.bigr.nl. {\textcopyright} 2011 Elsevier B.V.},
  doi      = {10.1016/j.media.2011.02.004},
  keywords = {CTA,Carotid,Evaluation framework,Lumen segmentation,Stenosis grading},
  pmid     = {21419689},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841511000260},
}

@Article{challenge/CADDementia,
  author   = {Bron, Esther E. and Smits, Marion and van der Flier, Wiesje M. and Vrenken, Hugo and Barkhof, Frederik and Scheltens, Philip and Papma, Janne M. and Steketee, Rebecca M.E. and {M{\'{e}}ndez Orellana}, Carolina and Meijboom, Rozanna and Pinto, Madalena and Meireles, Joana R. and Garrett, Carolina and Bastos-Leite, Ant{\'{o}}nio J. and Abdulkadir, Ahmed and Ronneberger, Olaf and Amoroso, Nicola and Bellotti, Roberto and C{\'{a}}rdenas-Pe{\~{n}}a, David and {\'{A}}lvarez-Meza, Andr{\'{e}}s M. and Dolph, Chester V. and Iftekharuddin, Khan M. and Eskildsen, Simon F. and Coup{\'{e}}, Pierrick and Fonov, Vladimir S. and Franke, Katja and Gaser, Christian and Ledig, Christian and Guerrero, Ricardo and Tong, Tong and Gray, Katherine R. and Moradi, Elaheh and Tohka, Jussi and Routier, Alexandre and Durrleman, Stanley and Sarica, Alessia and {Di Fatta}, Giuseppe and Sensi, Francesco and Chincarini, Andrea and Smith, Garry M. and Stoyanov, Zhivko V. and S{\o}rensen, Lauge and Nielsen, Mads and Tangaro, Sabina and Inglese, Paolo and Wachinger, Christian and Reuter, Martin and van Swieten, John C. and Niessen, Wiro J. and Klein, Stefan},
  journal  = {NeuroImage},
  title    = {{Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge}},
  year     = {2015},
  issn     = {10959572},
  month    = {may},
  pages    = {562--579},
  volume   = {111},
  abstract = {Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment and healthy controls. The diagnosis based on clinical criteria was used as reference standard, as it was the best available reference despite its known limitations. For evaluation, a previously unseen test set was used consisting of 354 T1-weighted MRI scans with the diagnoses blinded. Fifteen research teams participated with a total of 29 algorithms. The algorithms were trained on a small training set (n. =. 30) and optionally on data from other sources (e.g., the Alzheimer's Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of aging). The best performing algorithm yielded an accuracy of 63.0% and an area under the receiver-operating-characteristic curve (AUC) of 78.8%. In general, the best performances were achieved using feature extraction based on voxel-based morphometry or a combination of features that included volume, cortical thickness, shape and intensity. The challenge is open for new submissions via the web-based framework: http://caddementia.grand-challenge.org.},
  doi      = {10.1016/j.neuroimage.2015.01.048},
  keywords = {Alzheimer's disease,Challenge,Classification,Computer-aided diagnosis,Mild cognitive impairment,Structural MRI},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811915000737},
}

@Misc{challenge/MICCAI/6m-Infant-brain-seg,
  author = {Wang, Li and Bui, Toan Duc and Li, Gang and Lin, Weili and Shen, Dinggang},
  title  = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation}},
  year   = {2019},
  url    = {http://iseg2019.web.unc.edu/},
}

@Misc{challenge/MALBCV,
  author = {Landman, Bennett and Xu, Zhoubing and Igelsias, Juan Eugenio and Styner, Martin and Langerak, Thomas Robin and Klein, Arno},
  title  = {{Multi-Atlas Labeling Beyond the Cranial Vault -- workshop and challenge}},
  year   = {2015},
}

@Misc{challenge/BRATS_SICAS,
  title = {{BRATS - SICAS Medical Image Repository}},
  url   = {https://www.virtualskeleton.ch/BRATS/Start2013},
}

@Misc{challenge/MICCAI/BRATS/2018,
  author   = {{Perelman School Of Medicine}},
  title    = {{MICCAI BraTS 2018: Clinical Relevance | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2018},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2018/clinical-relevance.html},
}

@Misc{challenge/MICCAI/BRATS/2012,
  title = {{MICCAI BRATS 2012}},
  url   = {http://www2.imm.dtu.dk/projects/BRATS2012/},
}

@Misc{challenge/MICCAI/BRATS/2017,
  author   = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Reyes, Mauricio and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Shaykh, Hassan Fathallah and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre},
  title    = {{MICCAI BraTS 2017: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2017},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2017.html},
}

@Misc{challenge/MICCAI/BRATS/2020,
  author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Tiwari, Pallavi and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Huang, Raymond and Colen, Rivka R. and Marcus, Daniel and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek},
  title  = {{MICCAI BRATS - The Multimodal Brain Tumor Segmentation Challenge 2020}},
  year   = {2020},
  url    = {http://braintumorsegmentation.org/},
}

@Article{challenge/MICCAI/BRATS/2013,
  author   = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
  year     = {2015},
  issn     = {1558254X},
  month    = {oct},
  number   = {10},
  pages    = {1993--2024},
  volume   = {34},
  abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  doi      = {10.1109/TMI.2014.2377694},
  keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
  pmid     = {25494501},
  url      = {http://ieeexplore.ieee.org/document/6975210/},
}

@Misc{challenge/MICCAI/ENIGMA2017,
  title   = {{ENIGMA Cerebellum | MICCAI 2017 Workshop & Challenge}},
  url     = {https://my.vanderbilt.edu/enigmacerebellum/},
  urldate = {2020-05-12},
}

@Misc{dataset/neuromorphometrics,
  title = {{Products | Neuromorphometrics, Inc.}},
  url   = {http://www.neuromorphometrics.com/?page_id=23},
}

@Comment{jabref-meta: databaseType:bibtex;}
