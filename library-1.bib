% DBID: 2cntddo5p93dhhehfmto4pskmm

@Article{art/GuiJ_201309,
  author    = {Gui, Jie and Hu, Rongxiang and Zhao, Zhongqiu and Jia, Wei},
  journal   = {International Journal of Computer Mathematics},
  title     = {Semi-supervised learning with local and global consistency},
  year      = {2013},
  issn      = {1029-0265},
  month     = sep,
  number    = {11},
  pages     = {2389--2402},
  volume    = {91},
  abstract  = {In a lot of practical machine learning applications, such as web page classification, protein shape classification, unlabelled instances are easy to obtain, but labelled instances are rather too expensive to get. Thus, recently, semi-supervised learning (SSL) methods including graph-based algorithms have attracted many interests from researchers. However, most of these algorithms used the Gaussian function to calculate weights of the edge of the graph. In this paper, we proposed a novel weight for graph-based semi-supervised algorithms. In this new algorithm, the label information is added from problem into SSL algorithm, and the geodesic distance is utilized instead of Euclidean distance to calculate the distance between two instances. Furthermore, class prior knowledge is also added from problem into the target function. In this paper, we focus on learning with local and global consistency. We found that the effect of class prior knowledge maybe different between under low-label rate and high-label rate. Experiments on two University of California Irvine (UCI) data sets and United States Postal Service handwritten digit recognition show that our proposed algorithm is really effective.},
  doi       = {10.1080/00207160.2013.831082},
  isbn      = {0262201526},
  keywords  = {class prior knowledge,geodesic distance,label information,semi-supervised learning},
  publisher = {Informa UK Limited},
}

@Article{art/LiuY_2019,
  author        = {Liu, Yanbin and Lee, Juho and Park, Minseop and Kim, Saehoon and Yang, Eunho and Hwang, Sung Ju and Yang, Yi},
  journal       = {7th International Conference on Learning Representations, ICLR 2019},
  title         = {{Learning to propagate labels: Transductive propagation network for few-shot learning}},
  year          = {2019},
  abstract      = {The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner. We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results.},
  archiveprefix = {arXiv},
  arxivid       = {1805.10002},
  eprint        = {1805.10002},
}

@Article{art/KimS_202111,
  author        = {Kim, Soopil and An, Sion and Chikontwe, Philip and Park, Sang Hyun},
  journal       = {35th AAAI Conference on Artificial Intelligence, AAAI 2021},
  title         = {{Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation}},
  year          = {2021},
  month         = {nov},
  pages         = {1808--1816},
  volume        = {3A},
  abstract      = {Segmentation of organs of interest in 3D medical images is necessary for accurate diagnosis and longitudinal studies. Though recent advances using deep learning have shown success for many segmentation tasks, large datasets are required for high performance and the annotation process is both time consuming and labor intensive. In this paper, we propose a 3D few shot segmentation framework for accurate organ segmentation using limited training samples of the target organ annotation. To achieve this, a U-Net like network is designed to predict segmentation by learning the relationship between 2D slices of support data and a query image, including a bidirectional gated recurrent unit (GRU) that learns consistency of encoded features between adjacent slices. Also, we introduce a transfer learning method to adapt the characteristics of the target image and organ by updating the model before testing with arbitrary support and query data sampled from the support data. We evaluate our proposed model using three 3D CT datasets with annotations of different organs. Our model yielded significantly improved performance over state-of-the-art few shot segmentation models and was comparable to a fully supervised model trained with more target training data.},
  archiveprefix = {arXiv},
  arxivid       = {2011.09608},
  eprint        = {2011.09608},
  isbn          = {9781713835974},
  url           = {http://arxiv.org/abs/2011.09608},
}

@Article{art/TianZ_2022,
  author        = {Tian, Zhuotao and Zhao, Hengshuang and Shu, Michelle and Yang, Zhicheng and Li, Ruiyu and Jia, Jiaya},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Prior Guided Feature Enrichment Network for Few-Shot Segmentation}},
  year          = {2022},
  issn          = {19393539},
  number        = {2},
  pages         = {1050--1065},
  volume        = {44},
  abstract      = {State-of-the-art semantic segmentation methods require sufficient labeled data to achieve good results and hardly work on unseen classes without fine-tuning. Few-shot segmentation is thus proposed to tackle this problem by learning a model that quickly adapts to new classes with a few labeled support samples. Theses frameworks still face the challenge of generalization ability reduction on unseen classes due to inappropriate use of high-level semantic information of training classes and spatial inconsistency between query and support targets. To alleviate these issues, we propose the Prior Guided Feature Enrichment Network (PFENet). It consists of novel designs of (1) a training-free prior mask generation method that not only retains generalization power but also improves model performance and (2) Feature Enrichment Module (FEM) that overcomes spatial inconsistency by adaptively enriching query features with support features and prior masks. Extensive experiments on PASCAL-5$^i$i and COCO prove that the proposed prior generation method and FEM both improve the baseline method significantly. Our PFENet also outperforms state-of-the-art methods by a large margin without efficiency loss. It is surprising that our model even generalizes to cases without labeled support samples.},
  archiveprefix = {arXiv},
  arxivid       = {2008.01449},
  doi           = {10.1109/TPAMI.2020.3013717},
  eprint        = {2008.01449},
  keywords      = {Few-shot segmentation,few-shot learning,scene understanding,semantic segmentation},
  pmid          = {32750843},
}

@InBook{art/JohnanderJ_202210,
  author        = {Johnander, Joakim and Edstedt, Johan and Felsberg, Michael and Khan, Fahad Shahbaz and Danelljan, Martin},
  pages         = {217--234},
  publisher     = {Springer Nature Switzerland},
  title         = {Dense Gaussian Processes for Few-Shot Segmentation},
  year          = {2022},
  isbn          = {9783031198182},
  month         = {oct},
  abstract      = {Few-shot segmentation is a challenging dense prediction task, which entails segmenting a novel query image given only a small annotated support set. The key problem is thus to design a method that aggregates detailed information from the support set, while being robust to large variations in appearance and context. To this end, we propose a few-shot segmentation method based on dense Gaussian process (GP) regression. Given the support set, our dense GP learns the mapping from local deep image features to mask values, capable of capturing complex appearance distributions. Furthermore, it provides a principled means of capturing uncertainty, which serves as another powerful cue for the final segmentation, obtained by a CNN decoder. Instead of a one-dimensional mask output, we further exploit the end-to-end learning capabilities of our approach to learn a high-dimensional output space for the GP. Our approach sets a new state-of-the-art for both 1-shot and 5-shot FSS on the PASCAL-5$^i$ and COCO-20$^i$ benchmarks, achieving an absolute gain of $+14.9$ mIoU in the COCO-20$^i$ 5-shot setting. Furthermore, the segmentation quality of our approach scales gracefully when increasing the support set size, while achieving robust cross-dataset transfer.},
  archiveprefix = {arXiv},
  arxivid       = {2110.03674},
  booktitle     = {Computer Vision – ECCV 2022},
  doi           = {10.1007/978-3-031-19818-2_13},
  eprint        = {2110.03674},
  issn          = {1611-3349},
  url           = {http://arxiv.org/abs/2110.03674},
}

@Article{art/KimS_202110,
  author        = {Kim, Soopil and Chikontwe, Philip and Park, Sang Hyun},
  title         = {{Uncertainty-Aware Semi-Supervised Few Shot Segmentation}},
  year          = {2021},
  month         = {oct},
  abstract      = {Few shot segmentation (FSS) aims to learn pixel-level classification of a target object in a query image using only a few annotated support samples. This is challenging as it requires modeling appearance variations of target objects and the diverse visual cues between query and support images with limited information. To address this problem, we propose a semi-supervised FSS strategy that leverages additional prototypes from unlabeled images with uncertainty guided pseudo label refinement. To obtain reliable prototypes from unlabeled images, we meta-train a neural network to jointly predict segmentation and estimate the uncertainty of predictions. We employ the uncertainty estimates to exclude predictions with high degrees of uncertainty for pseudo label construction to obtain additional prototypes based on the refined pseudo labels. During inference, query segmentation is predicted using prototypes from both support and unlabeled images including low-level features of the query images. Our approach is end-to-end and can easily supplement existing approaches without the requirement of additional training to employ unlabeled samples. Extensive experiments on PASCAL-$5^i$ and COCO-$20^i$ demonstrate that our model can effectively remove unreliable predictions to refine pseudo labels and significantly improve upon state-of-the-art performances.},
  archiveprefix = {arXiv},
  arxivid       = {2110.08954},
  eprint        = {2110.08954},
  url           = {http://arxiv.org/abs/2110.08954},
}

@InProceedings{art/LiG_2021,
  author        = {Li, Gen and Jampani, Varun and Sevilla-Lara, Laura and Sun, Deqing and Kim, Jonghyun and Kim, Joongkyu},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Adaptive Prototype Learning and Allocation for Few-Shot Segmentation}},
  year          = {2021},
  pages         = {8330--8339},
  abstract      = {Prototype learning is extensively used for few-shot segmentation. Typically, a single prototype is obtained from the support feature by averaging the global object information. However, using one prototype to represent all the information may lead to ambiguities. In this paper, we propose two novel modules, named superpixel-guided clustering (SGC) and guided prototype allocation (GPA), for multiple prototype extraction and allocation. Specifically, SGC is a parameter-free and training-free approach, which extracts more representative prototypes by aggregating similar feature vectors, while GPA is able to select matched prototypes to provide more accurate guidance. By integrating the SGC and GPA together, we propose the Adaptive Superpixel-guided Network (ASGNet), which is a lightweight model and adapts to object scale and shape variation. In addition, our network can easily generalize to k-shot segmentation with substantial improvement and no additional computational cost. In particular, our evaluations on COCO demonstrate that ASGNet surpasses the state-of-the-art method by 5% in 5-shot segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {2104.01893},
  doi           = {10.1109/CVPR46437.2021.00823},
  eprint        = {2104.01893},
  isbn          = {9781665445092},
  issn          = {10636919},
}

@InProceedings{art/LiuY_2020,
  author        = {Liu, Yongfei and Zhang, Xiangyi and Zhang, Songyang and He, Xuming},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Part-Aware Prototype Network for Few-Shot Semantic Segmentation}},
  year          = {2020},
  pages         = {142--158},
  volume        = {12354 LNCS},
  abstract      = {Few-shot semantic segmentation aims to learn to segment new object classes with only a few annotated examples, which has a wide range of real-world applications. Most existing methods either focus on the restrictive setting of one-way few-shot segmentation or suffer from incomplete coverage of object regions. In this paper, we propose a novel few-shot semantic segmentation framework based on the prototype representation. Our key idea is to decompose the holistic class representation into a set of part-aware prototypes, capable of capturing diverse and fine-grained object features. In addition, we propose to leverage unlabeled data to enrich our part-aware prototypes, resulting in better modeling of intra-class variations of semantic objects. We develop a novel graph neural network model to generate and enhance the proposed part-aware prototypes based on labeled and unlabeled images. Extensive experimental evaluations on two benchmarks show that our method outperforms the prior art with a sizable margin (Code is available at: https://github.com/Xiangyi1996/PPNet-PyTorch).},
  archiveprefix = {arXiv},
  arxivid       = {2007.06309},
  doi           = {10.1007/978-3-030-58545-7_9},
  eprint        = {2007.06309},
  isbn          = {9783030585440},
  issn          = {16113349},
}

@Article{art/HansenS_2022,
  author        = {Hansen, Stine and Gautam, Srishti and Jenssen, Robert and Kampffmeyer, Michael},
  journal       = {Medical Image Analysis},
  title         = {{Anomaly detection-inspired few-shot medical image segmentation through self-supervision with supervoxels}},
  year          = {2022},
  issn          = {13618423},
  volume        = {78},
  abstract      = {Recent work has shown that label-efficient few-shot learning through self-supervision can achieve promising medical image segmentation results. However, few-shot segmentation models typically rely on prototype representations of the semantic classes, resulting in a loss of local information that can degrade performance. This is particularly problematic for the typically large and highly heterogeneous background class in medical image segmentation problems. Previous works have attempted to address this issue by learning additional prototypes for each class, but since the prototypes are based on a limited number of slices, we argue that this ad-hoc solution is insufficient to capture the background properties. Motivated by this, and the observation that the foreground class (e.g., one organ) is relatively homogeneous, we propose a novel anomaly detection-inspired approach to few-shot medical image segmentation in which we refrain from modeling the background explicitly. Instead, we rely solely on a single foreground prototype to compute anomaly scores for all query pixels. The segmentation is then performed by thresholding these anomaly scores using a learned threshold. Assisted by a novel self-supervision task that exploits the 3D structure of medical images through supervoxels, our proposed anomaly detection-inspired few-shot medical image segmentation model outperforms previous state-of-the-art approaches on two representative MRI datasets for the tasks of abdominal organ segmentation and cardiac segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {2203.02048},
  doi           = {10.1016/j.media.2022.102385},
  eprint        = {2203.02048},
  keywords      = {Anomaly detection,Cardiac segmentation,Few-shot learning,Organ segmentation,Self-supervision,Supervoxels},
  pmid          = {35272250},
}

@InProceedings{art/YuQ_2021,
  author        = {Yu, Qinji and Dang, Kang and Tajbakhsh, Nima and Terzopoulos, Demetri and DIng, Xiaowei},
  booktitle     = {Proceedings - International Symposium on Biomedical Imaging},
  title         = {{A location-sensitive local prototype network for few-shot medical image segmentation}},
  year          = {2021},
  pages         = {262--266},
  volume        = {2021-April},
  abstract      = {Despite the tremendous success of deep neural networks in medical image segmentation, they typically require a large amount of costly, expert-level annotated data. Few-shot segmentation approaches address this issue by learning to transfer knowledge from limited quantities of labeled examples. Incorporating appropriate prior knowledge is critical in designing high-performance few-shot segmentation algorithms. Since strong spatial priors exist in many medical imaging modalities, we propose a prototype-based method-namely, the location-sensitive local prototype network-that leverages spatial priors to perform few-shot medical image segmentation. Our approach divides the difficult problem of segmenting the entire image with global prototypes into easily solvable subproblems of local region segmentation with local prototypes. For organ segmentation experiments on the VISCERAL CT image dataset, our method outperforms the state-of-the-art approaches by 10% in the mean Dice coefficient. Extensive ablation studies demonstrate the substantial benefits of incorporating spatial information and confirm the effectiveness of our approach.},
  archiveprefix = {arXiv},
  arxivid       = {2103.10178},
  doi           = {10.1109/ISBI48211.2021.9434008},
  eprint        = {2103.10178},
  isbn          = {9781665412469},
  issn          = {19458452},
  keywords      = {Few-shot segmentation,Medical image segmentation,Prototype networks,Spatial layout priors},
}

@InProceedings{art/WangK_2019,
  author        = {Wang, Kaixin and Liew, Jun Hao and Zou, Yingtian and Zhou, Daquan and Feng, Jiashi},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{PANet: Few-shot image semantic segmentation with prototype alignment}},
  year          = {2019},
  pages         = {9196--9205},
  volume        = {2019-Octob},
  abstract      = {Despite the great progress made by deep CNNs in image semantic segmentation, they typically require a large number of densely-annotated images for training and are difficult to generalize to unseen object categories. Few-shot segmentation has thus been developed to learn to perform segmentation from only a few annotated examples. In this paper, we tackle the challenging few-shot segmentation problem from a metric learning perspective and present PANet, a novel prototype alignment network to better utilize the information of the support set. Our PANet learns class-specific prototype representations from a few support images within an embedding space and then performs segmentation over the query images through matching each pixel to the learned prototypes. With non-parametric metric learning, PANet offers high-quality prototypes that are representative for each semantic class and meanwhile discriminative for different classes. Moreover, PANet introduces a prototype alignment regularization between support and query. With this, PANet fully exploits knowledge from the support and provides better generalization on few-shot segmentation. Significantly, our model achieves the mIoU score of 48.1% and 55.7% on PASCAL-5i for 1-shot and 5-shot settings respectively, surpassing the state-of-the-art method by 1.8% and 8.6%.},
  archiveprefix = {arXiv},
  arxivid       = {1908.06391},
  doi           = {10.1109/ICCV.2019.00929},
  eprint        = {1908.06391},
  isbn          = {9781728148038},
  issn          = {15505499},
}

@Article{art/SunL_2022,
  author        = {Sun, Liyan and Li, Chenxin and Ding, Xinghao and Huang, Yue and Chen, Zhong and Wang, Guisheng and Yu, Yizhou and Paisley, John},
  journal       = {Computers in Biology and Medicine},
  title         = {{Few-shot medical image segmentation using a global correlation network with discriminative embedding}},
  year          = {2022},
  issn          = {18790534},
  volume        = {140},
  abstract      = {Despite impressive developments in deep convolutional neural networks for medical imaging, the paradigm of supervised learning requires numerous annotations in training to avoid overfitting. In clinical cases, massive semantic annotations are difficult to acquire where biomedical expert knowledge is required. Moreover, it is common when only a few annotated classes are available. In this study, we proposed a new approach to few-shot medical image segmentation, which enables a segmentation model to quickly generalize to an unseen class with few training images. We constructed a few-shot image segmentation mechanism using a deep convolutional network trained episodically. Motivated by the spatial consistency and regularity in medical images, we developed an efficient global correlation module to model the correlation between a support and query image and incorporate it into the deep network. We enhanced the discrimination ability of the deep embedding scheme to encourage clustering of feature domains belonging to the same class while keeping feature domains of different organs far apart. We experimented using anatomical abdomen images from both CT and MRI modalities.},
  archiveprefix = {arXiv},
  arxivid       = {2012.05440},
  doi           = {10.1016/j.compbiomed.2021.105067},
  eprint        = {2012.05440},
  keywords      = {Cross correlation,Deep embedding,Few-shot learning,Medical image segmentation},
}

@Article{art/KangD_2022,
  author   = {Kang, Dahyun and Cho, Minsu},
  journal  = {Cvpr},
  title    = {{Integrative Few-Shot Learning for Classification and Segmentation}},
  year     = {2022},
  pages    = {9979--9990},
  abstract = {We introduce the integrative task of few-shot classification and segmentation (FS-CS) that aims to both classify and segment target objects in a query image when the target classes are given with a few examples. This task combines two conventional few-shot learning problems, few-shot classification and segmentation. FS-CS generalizes them to more realistic episodes with arbitrary image pairs, where each target class may or may not be present in the query. To address the task, we propose the integrative few-shot learning (iFSL) framework for FS-CS, which trains a learner to construct class-wise foreground maps for multi-label classification and pixel-wise segmentation. We also develop an effective iFSL model, attentive squeeze network (ASNet), that leverages deep semantic correlation and global self-attention to produce reliable foreground maps. In experiments, the proposed method shows promising performance on the FS-CS task and also achieves the state of the art on standard few-shot segmentation benchmarks.},
  url      = {http://cvlab.postech.ac.kr/research/iFSL},
}

@InProceedings{art/JiaB_2021,
  author        = {Jia, Bowei and Tian, Yunzhe and Zhao, Di and Wang, Xiaojin and Li, Chenyang and Niu, Wenjia and Tong, Endong and Liu, Jiqiang},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Bidirectional RNN-Based Few-Shot Training for Detecting Multi-stage Attack}},
  year          = {2021},
  pages         = {37--52},
  volume        = {12612 LNCS},
  abstract      = {Feint attack, as a combination of virtual attacks and real attacks of a new type of APT attack, has become the focus of attention. Under the cover of virtual attacks, real attacks can achieve the real purpose and cause losses inadvertently. However, to our knowledge, all previous works use common methods such as Causal-Correlation or Cased-based to detect outdated multi-stage attacks. Few attentions have been paid to detect the feint attack, because of the diversification of the concept of feint attack and the lack of professional datasets. Aiming at the existing challenge, this paper explores a new method to construct such dataset. A fuzzy clustering method based on attribute similarity is used to mine multi-stage attack chains. Then we use a few-shot deep learning algorithm (SMOTE&CNN-SVM) and bidirectional recurrent neural network model (Bi-RNN) to obtain the feint attack chains. Feint attack is simulated by the real attack inserted in the normal causal attack chain, and the addition of the real attack destroys the causal relationship of the original attack chain. So, we used Bi-RNN coding to obtain the hidden feature of feint attack chain. In experiments, we evaluate our approach through using the LLDoS1.0 and LLDoS2.0 of DARPA2000 and CICIDS2017 of Canadian Institute for Cybersecurity.},
  archiveprefix = {arXiv},
  arxivid       = {1905.03454},
  doi           = {10.1007/978-3-030-71852-7_3},
  eprint        = {1905.03454},
  isbn          = {9783030718510},
  issn          = {16113349},
  keywords      = {Bi-RNN model,Feint attack,Few-shot learning,Fuzzy clustering,Multi-stage attack},
}

@InProceedings{art/TangH_2021,
  author        = {Tang, Hao and Liu, Xingwei and Sun, Shanlin and Yan, Xiangyi and Xie, Xiaohui},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Recurrent Mask Refinement for Few-Shot Medical Image Segmentation}},
  year          = {2021},
  pages         = {3898--3908},
  abstract      = {Although having achieved great success in medical image segmentation, deep convolutional neural networks usually require a large dataset with manual annotations for training and are difficult to generalize to unseen classes. Few-shot learning has the potential to address these challenges by learning new classes from only a few labeled examples. In this work, we propose a new framework for few-shot medical image segmentation based on prototypical networks. Our innovation lies in the design of two key modules: 1) a context relation encoder (CRE) that uses correlation to capture local relation features between foreground and background regions; and 2) a recurrent mask refinement module that repeatedly uses the CRE and a prototypical network to recapture the change of context relationship and refine the segmentation mask iteratively. Experiments on two abdomen CT datasets and an abdomen MRI dataset show the proposed method obtains substantial improvement over the state-of-the-art methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively. Code is publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {2108.00622},
  doi           = {10.1109/ICCV48922.2021.00389},
  eprint        = {2108.00622},
  isbn          = {9781665428125},
  issn          = {15505499},
}

@InProceedings{art/XieG_2022,
  author   = {Xie, Guo-Sen and Xiong, Huan and Liu, Jie and Yao, Yazhou and Shao, Ling},
  title    = {{Few-Shot Semantic Segmentation with Cyclic Memory Network}},
  year     = {2022},
  pages    = {7273--7282},
  abstract = {Few-shot semantic segmentation (FSS) is an important task for novel (unseen) object segmentation under the data-scarcity scenario. However, most FSS methods rely on uni-directional feature aggregation, e.g., from support prototypes to get the query prediction, and from high-resolution features to guide the low-resolution ones. This usually fails to fully capture the cross-resolution feature relationships and thus leads to inaccurate estimates of the query objects. To resolve the above dilemma, we propose a cyclic memory network (CMN) to directly learn to read abundant support information from all resolution features in a cyclic manner. Specifically, we first generate N pairs (key and value) of multi-resolution query features guided by the support feature and its mask. Next, we circularly take one pair of these features as the query to be segmented, and the rest N-1 pairs are written into an external memory accordingly, i.e., this leave-one-out process is conducted for N times. In each cycle , the query feature is updated by collaboratively matching its key and value with the memory, which can elegantly cover all the spatial locations from different resolutions. Furthermore, we incorporate the query feature re-adding and the query feature recursive updating mechanisms into the memory reading operation. CMN, equipped with these merits, can thus capture cross-resolution relationships and better handle the object appearance and scale variations in FSS. Experiments on PASCAL-5 i and COCO-20 i well validate the effectiveness of our model for FSS.},
  doi      = {10.1109/iccv48922.2021.00720},
}

@InProceedings{art/BoudiafM_2021,
  author        = {Boudiaf, Malik and Kervadec, Hoel and Masud, Ziko Imtiaz and Piantanida, Pablo and Ayed, Ismail Ben and Dolz, Jose},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Few-shot segmentation without meta-learning: A good transductive inference is all you need?}},
  year          = {2021},
  pages         = {13974--13983},
  abstract      = {We show that the way inference is performed in few-shot segmentation tasks has a substantial effect on performances-an aspect often overlooked in the literature in favor of the meta-learning paradigm. We introduce a transductive inference for a given query image, leveraging the statistics of its unlabeled pixels, by optimizing a new loss containing three complementary terms: i) the cross-entropy on the labeled support pixels; ii) the Shannon entropy of the posteriors on the unlabeled query-image pixels; and iii) a global KL-divergence regularizer based on the proportion of the predicted foreground. As our inference uses a simple linear classifier of the extracted features, its computational load is comparable to inductive inference and can be used on top of any base training. Foregoing episodic training and using only standard cross-entropy training on the base classes, our inference yields competitive performances on standard benchmarks in the 1-shot scenarios. As the number of available shots increases, the gap in performances widens: on PASCAL-5i, our method brings about 5% and 6% improvements over the state-of-the-art, in the 5- and 10-shot scenarios, respectively. Furthermore, we introduce a new setting that includes domain shifts, where the base and novel classes are drawn from different datasets. Our method achieves the best performances in this more realistic setting. Our code is freely available online: https://github.com/mboudiaf/RePRI-for-Few-Shot-Segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {2012.06166},
  doi           = {10.1109/CVPR46437.2021.01376},
  eprint        = {2012.06166},
  isbn          = {9781665445092},
  issn          = {10636919},
}

@InProceedings{art/LuZ_2022,
  author        = {Lu, Zhihe and He, Sen and Zhu, Xiatian and Zhang, Li and Song, Yi-Zhe and Xiang, Tao},
  title         = {{Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer}},
  year          = {2022},
  pages         = {8721--8730},
  abstract      = {A few-shot semantic segmentation model is typically composed of a CNN encoder, a CNN decoder and a simple classifier (separating foreground and background pixels). Most existing methods meta-learn all three model components for fast adaptation to a new class. However, given that as few as a single support set image is available, effective model adaption of all three components to the new class is extremely challenging. In this work we propose to simplify the meta-learning task by focusing solely on the simplest component, the classifier, whilst leaving the encoder and decoder to pre-training. We hypothesize that if we pre-train an off-the-shelf segmentation model over a set of diverse training classes with sufficient annotations, the encoder and decoder can capture rich discriminative features applicable for any unseen classes, rendering the subsequent meta-learning stage unnecessary. For the classifier meta-learning, we introduce a Classifier Weight Transformer (CWT) designed to dynamically adapt the supportset trained classifier's weights to each query image in an inductive way. Extensive experiments on two standard benchmarks show that despite its simplicity, our method outperforms the state-of-the-art alternatives, often by a large margin.Code is available on https://github.com/zhiheLu/CWT-for-FSS.},
  archiveprefix = {arXiv},
  arxivid       = {2108.03032},
  doi           = {10.1109/iccv48922.2021.00862},
  eprint        = {2108.03032},
}

@InProceedings{art/ZhangB_2021,
  author        = {Zhang, Bingfeng and Xiao, Jimin and Qin, Terry},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Self-Guided and Cross-Guided Learning for Few-Shot Segmentation}},
  year          = {2021},
  pages         = {8308--8317},
  abstract      = {Few-shot segmentation has been attracting a lot of attention due to its effectiveness to segment unseen object classes with a few annotated samples. Most existing approaches use masked Global Average Pooling (GAP) to encode an annotated support image to a feature vector to facilitate query image segmentation. However, this pipeline unavoidably loses some discriminative information due to the average operation. In this paper, we propose a simple but effective self-guided learning approach, where the lost critical information is mined. Specifically, through making an initial prediction for the annotated support image, the covered and uncovered foreground regions are encoded to the primary and auxiliary support vectors using masked GAP, respectively. By aggregating both primary and auxiliary support vectors, better segmentation performances are obtained on query images. Enlightened by our self-guided module for 1-shot segmentation, we propose a cross-guided module for multiple shot segmentation, where the final mask is fused using predictions from multiple annotated samples with high-quality support vectors contributing more and vice versa. This module improves the final prediction in the inference stage without re-training. Extensive experiments show that our approach achieves new state-of-the-art performances on both PASCAL-5i and COCO-20i datasets. Source code is available at https://github.com/zbf1991/SCL.},
  archiveprefix = {arXiv},
  arxivid       = {2103.16129},
  doi           = {10.1109/CVPR46437.2021.00821},
  eprint        = {2103.16129},
  isbn          = {9781665445092},
  issn          = {10636919},
}

@Article{art/ZhangX_2021,
  author   = {Zhang, Xiaolin and Wei, Yunchao and Li, Zhao and Yan, Chenggang and Yang, Yi},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {{Rich Embedding Features for One-Shot Semantic Segmentation}},
  year     = {2021},
  issn     = {21622388},
  abstract = {One-shot semantic segmentation poses the challenging task of segmenting object regions from unseen categories with only one annotated example as guidance. Thus, how to effectively construct robust feature representations from the guidance image is crucial to the success of one-shot semantic segmentation. To this end, we propose in this article a simple, yet effective approach named rich embedding features (REFs). Given a reference image accompanied with its annotated mask, our REF constructs rich embedding features of the support object from three perspectives: 1) global embedding to capture the general characteristics; 2) peak embedding to capture the most discriminative information; 3) adaptive embedding to capture the internal long-range dependencies. By combining these informative features, we can easily harvest sufficient and rich guidance even from a single reference image. In addition to REF, we further propose a simple depth-priority context module to obtain useful contextual cues from the query image. This successfully raises the performance of one-shot semantic segmentation to a new level. We conduct experiments on pattern analysis, statical modeling and computational learning (Pascal) visual object classes (VOC) 2012 and common object in context (COCO) to demonstrate the effectiveness of our approach.},
  doi      = {10.1109/TNNLS.2021.3081693},
  keywords = {Deep learning,Feature extraction,Image segmentation,Prototypes,Pulse modulation,Semantics,Siamese network.,Support vector machines,Task analysis,few shot segmentation,object segmentation},
}

@InProceedings{art/ZhangC_2019,
  author        = {Zhang, Chi and Lin, Guosheng and Liu, Fayao and Yao, Rui and Shen, Chunhua},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{CANET: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning}},
  year          = {2019},
  pages         = {5212--5221},
  volume        = {2019-June},
  abstract      = {Recent progress in semantic segmentation is driven by deep Convolutional Neural Networks and large-scale labeled image datasets. However, data labeling for pixel-wise segmentation is tedious and costly. Moreover, a trained model can only make predictions within a set of pre-defined classes. In this paper, we present CANet, a class-agnostic segmentation network that performs few-shot segmentation on new classes with only a few annotated images available. Our network consists of a two-branch dense comparison module which performs multi-level feature comparison between the support image and the query image, and an iterative optimization module which iteratively refines the predicted results. Furthermore, we introduce an attention mechanism to effectively fuse information from multiple support examples under the setting of k-shot learning. Experiments on PASCAL VOC 2012 show that our method achieves a mean Intersection-over-Union score of 55.4% for 1-shot segmentation and 57.1% for 5-shot segmentation, outperforming state-of-the-art methods by a large margin of 14.6% and 13.2%, respectively.},
  archiveprefix = {arXiv},
  arxivid       = {1903.02351},
  doi           = {10.1109/CVPR.2019.00536},
  eprint        = {1903.02351},
  isbn          = {9781728132938},
  issn          = {10636919},
  keywords      = {Categorization,Grouping and Shape,Recognition: Detection,Retrieval,Segmentation},
}

@Article{art/ZhangX_2020,
  author        = {Zhang, Xiaolin and Wei, Yunchao and Yang, Yi and Huang, Thomas S.},
  journal       = {IEEE Transactions on Cybernetics},
  title         = {{SG-One: Similarity Guidance Network for One-Shot Semantic Segmentation}},
  year          = {2020},
  issn          = {21682275},
  number        = {9},
  pages         = {3855--3865},
  volume        = {50},
  abstract      = {One-shot image semantic segmentation poses a challenging task of recognizing the object regions from unseen categories with only one annotated example as supervision. In this article, we propose a simple yet effective similarity guidance network to tackle the one-shot (SG-One) segmentation problem. We aim at predicting the segmentation mask of a query image with the reference to one densely labeled support image of the same category. To obtain the robust representative feature of the support image, we first adopt a masked average pooling strategy for producing the guidance features by only taking the pixels belonging to the support image into account. We then leverage the cosine similarity to build the relationship between the guidance features and features of pixels from the query image. In this way, the possibilities embedded in the produced similarity maps can be adopted to guide the process of segmenting objects. Furthermore, our SG-One is a unified framework that can efficiently process both support and query images within one network and be learned in an end-to-end manner. We conduct extensive experiments on Pascal VOC 2012. In particular, our SG-One achieves the mIoU score of 46.3%, surpassing the baseline methods.},
  archiveprefix = {arXiv},
  arxivid       = {1810.09091},
  doi           = {10.1109/TCYB.2020.2992433},
  eprint        = {1810.09091},
  keywords      = {Few-shot learning,image segmentation,neural networks,siamese network},
  pmid          = {32497014},
}

@Article{art/OuyangC_202207,
  author    = {Ouyang, Cheng and Biffi, Carlo and Chen, Chen and Kart, Turkay and Qiu, Huaqi and Rueckert, Daniel},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {Self-Supervised Learning for Few-Shot Medical Image Segmentation},
  year      = {2022},
  issn      = {1558-254X},
  month     = jul,
  number    = {7},
  pages     = {1837--1848},
  volume    = {41},
  abstract  = {Fully-supervised deep learning segmentation models are inflexible when encountering new unseen semantic classes and their fine-tuning often requires significant amounts of annotated data. Few-shot semantic segmentation (FSS) aims to solve this inflexibility by learning to segment an arbitrary unseen semantically meaningful class by referring to only a few labeled examples, without involving fine-tuning. State-of-the-art FSS methods are typically designed for segmenting natural images and rely on abundant annotated data of training classes to learn image representations that generalize well to unseen testing classes. However, such a training mechanism is impractical in annotation-scarce medical imaging scenarios. To address this challenge, in this work, we propose a novel self-supervised FSS framework for medical images, named SSL-ALPNet, in order to bypass the requirement for annotations during training. The proposed method exploits superpixel-based pseudo-labels to provide supervision signals. In addition, we propose a simple yet effective adaptive local prototype pooling module which is plugged into the prototype networks to further boost segmentation accuracy. We demonstrate the general applicability of the proposed approach using three different tasks: organ segmentation of abdominal CT and MRI images respectively, and cardiac segmentation of MRI images. The proposed method yields higher Dice scores than conventional FSS methods which require manual annotations for training in our experiments.},
  doi       = {10.1109/tmi.2022.3150682},
  keywords  = {Annotations,Biomedical imaging,Few-shot segmentation,Image segmentation,Prototypes,Representation learning,Self-supervised learning,Semantics,Task analysis,Training},
  pmid      = {35139014},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@Article{art/CaoK_202007,
  author        = {Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
  title         = {{Concept Learners for Few-Shot Learning}},
  year          = {2020},
  month         = {jul},
  abstract      = {Developing algorithms that are able to generalize to a novel task given only a few labeled examples represents a fundamental challenge in closing the gap between machine- and human-level performance. The core of human cognition lies in the structured, reusable concepts that help us to rapidly adapt to new tasks and provide reasoning behind our decisions. However, existing meta-learning methods learn complex representations across prior labeled tasks without imposing any structure on the learned representations. Here we propose COMET, a meta-learning method that improves generalization ability by learning to learn along human-interpretable concept dimensions. Instead of learning a joint unstructured metric space, COMET learns mappings of high-level concepts into semi-structured metric spaces, and effectively combines the outputs of independent concept learners. We evaluate our model on few-shot tasks from diverse domains, including fine-grained image classification, document categorization and cell type annotation on a novel dataset from a biological domain developed in our work. COMET significantly outperforms strong meta-learning baselines, achieving 6-15% relative improvement on the most challenging 1-shot learning tasks, while unlike existing methods providing interpretations behind the model's predictions.},
  archiveprefix = {arXiv},
  arxivid       = {2007.07375},
  eprint        = {2007.07375},
  url           = {http://arxiv.org/abs/2007.07375},
}

@Article{art/DengB_201711,
  author        = {Deng, Boyang and Liu, Qing and Qiao, Siyuan and Yuille, Alan},
  title         = {{Few-shot Learning by Exploiting Visual Concepts within CNNs}},
  year          = {2017},
  month         = {nov},
  abstract      = {Convolutional neural networks (CNNs) are one of the driving forces for the advancement of computer vision. Despite their promising performances on many tasks, CNNs still face major obstacles on the road to achieving ideal machine intelligence. One is that CNNs are complex and hard to interpret. Another is that standard CNNs require large amounts of annotated data, which is sometimes hard to obtain, and it is desirable to learn to recognize objects from few examples. In this work, we address these limitations of CNNs by developing novel, flexible, and interpretable models for few-shot learning. Our models are based on the idea of encoding objects in terms of visual concepts (VCs), which are interpretable visual cues represented by the feature vectors within CNNs. We first adapt the learning of VCs to the few-shot setting, and then uncover two key properties of feature encoding using VCs, which we call category sensitivity and spatial pattern. Motivated by these properties, we present two intuitive models for the problem of few-shot learning. Experiments show that our models achieve competitive performances, while being more flexible and interpretable than alternative state-of-the-art few-shot learning methods. We conclude that using VCs helps expose the natural capability of CNNs for few-shot learning.},
  archiveprefix = {arXiv},
  arxivid       = {1711.08277},
  doi           = {https://doi.org/10.48550/arXiv.1711.08277},
  eprint        = {1711.08277},
  url           = {http://arxiv.org/abs/1711.08277},
}

@Article{art/LangC_202203,
  author        = {Lang, Chunbo and Cheng, Gong and Tu, Binfei and Han, Junwei},
  title         = {{Learning What Not to Segment: A New Perspective on Few-Shot Segmentation}},
  year          = {2022},
  month         = {mar},
  abstract      = {Recently few-shot segmentation (FSS) has been extensively developed. Most previous works strive to achieve generalization through the meta-learning framework derived from classification tasks; however, the trained models are biased towards the seen classes instead of being ideally class-agnostic, thus hindering the recognition of new concepts. This paper proposes a fresh and straightforward insight to alleviate the problem. Specifically, we apply an additional branch (base learner) to the conventional FSS model (meta learner) to explicitly identify the targets of base classes, i.e., the regions that do not need to be segmented. Then, the coarse results output by these two learners in parallel are adaptively integrated to yield precise segmentation prediction. Considering the sensitivity of meta learner, we further introduce an adjustment factor to estimate the scene differences between the input image pairs for facilitating the model ensemble forecasting. The substantial performance gains on PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our versatile scheme sets a new state-of-the-art even with two plain learners. Moreover, in light of the unique nature of the proposed approach, we also extend it to a more realistic but challenging setting, i.e., generalized FSS, where the pixels of both base and novel classes are required to be determined. The source code is available at github.com/chunbolang/BAM.},
  archiveprefix = {arXiv},
  arxivid       = {2203.07615},
  eprint        = {2203.07615},
  url           = {http://arxiv.org/abs/2203.07615},
}

@InProceedings{art/YangL_2020,
  author        = {Yang, Ling and Li, Liangliang and Zhang, Zilun and Zhou, Xinyu and Zhou, Erjin and Liu, Yu},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{DPGN: Distribution propagation graph network for few-shot learning}},
  year          = {2020},
  pages         = {13387--13396},
  abstract      = {Most graph-network-based meta-learning approaches model instance-level relation of examples. We extend this idea further to explicitly model the distribution-level relation of one example to all other examples in a 1-vs-N manner. We propose a novel approach named distribution propagation graph network (DPGN) for few-shot learning. It conveys both the distribution-level relations and instance-level relations in each few-shot learning task. To combine the distribution-level relations and instance-level relations for all examples, we construct a dual complete graph network which consists of a point graph and a distribution graph with each node standing for an example. Equipped with dual graph architecture, DPGN propagates label information from labeled examples to unlabeled examples within several update generations. In extensive experiments on few-shot learning benchmarks, DPGN outperforms state-of-the-art results by a large margin in 5% $\sim$ 12% under supervised settings and 7% $\sim$ 13% under semi-supervised settings. Code is available at https://github.com/megviiresearch/DPGN.},
  archiveprefix = {arXiv},
  arxivid       = {2003.14247},
  doi           = {10.1109/CVPR42600.2020.01340},
  eprint        = {2003.14247},
  issn          = {10636919},
}

@InProceedings{art/WuZ_2021,
  author        = {Wu, Zhonghua and Shi, Xiangxi and Lin, Guosheng and Cai, Jianfei},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Learning Meta-class Memory for Few-Shot Semantic Segmentation}},
  year          = {2021},
  pages         = {497--506},
  abstract      = {Currently, the state-of-the-art methods treat few-shot semantic segmentation task as a conditional foreground-background segmentation problem, assuming each class is independent. In this paper, we introduce the concept of meta-class, which is the meta information (e.g. certain middle-level features) shareable among all classes. To explicitly learn meta-class representations in few-shot segmentation task, we propose a novel Meta-class Memory based few-shot segmentation method (MM-Net), where we introduce a set of learnable memory embeddings to memorize the meta-class information during the base class training and transfer to novel classes during the inference stage. Moreover, for the k-shot scenario, we propose a novel image quality measurement module to select images from the set of support images. A high-quality class prototype could be obtained with the weighted sum of support image features based on the quality measure. Experiments on both PASCAL-5i and COCO datasets show that our proposed method is able to achieve state-of-the-art results in both 1-shot and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5% mIoU on the COCO dataset in 1-shot setting, which is 5.1% higher than the previous state-of-the-art.},
  archiveprefix = {arXiv},
  arxivid       = {2108.02958},
  doi           = {10.1109/ICCV48922.2021.00056},
  eprint        = {2108.02958},
  isbn          = {9781665428125},
  issn          = {15505499},
}

@InProceedings{art/YangL_202110,
  author        = {Yang, Lihe and Zhuo, Wei and Qi, Lei and Shi, Yinghuan and Gao, Yang},
  booktitle     = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  title         = {Mining Latent Classes for Few-shot Segmentation},
  year          = {2021},
  month         = oct,
  pages         = {8701--8710},
  publisher     = {IEEE},
  abstract      = {Few-shot segmentation (FSS) aims to segment unseen classes given only a few annotated samples. Existing methods suffer the problem of feature undermining, i.e. potential novel classes are treated as background during training phase. Our method aims to alleviate this problem and enhance the feature embedding on latent novel classes. In our work, we propose a novel joint-training framework. Based on conventional episodic training on support-query pairs, we add an additional mining branch that exploits latent novel classes via transferable sub-clusters, and a new rectification technique on both background and foreground categories to enforce more stable prototypes. Over and above that, our transferable sub-cluster has the ability to leverage extra unlabeled data for further feature enhancement. Extensive experiments on two FSS benchmarks demonstrate that our method outperforms previous state-of-the-art by a large margin of 3.7% mIOU on PASCAL-5i and 7.0% mIOU on COCO-20i at the cost of 74% fewer parameters and 2.5x faster inference speed. The source code is available at https://github.com/LiheYoung/MiningFSS.},
  archiveprefix = {arXiv},
  arxivid       = {2103.15402},
  doi           = {10.1109/iccv48922.2021.00860},
  eprint        = {2103.15402},
}

@Article{art/ZhouX_2017,
  author   = {Zhou, Xiao-Yun and Shen, Mali and Riga, Celia and Yang, Guang-Zhong and Lee, Su-Lin},
  journal  = {arXiv.org},
  title    = {{Focal FCN: Towards Small Object Segmentation with Limited Training Data}},
  year     = {2017},
  volume   = {cs.CV},
  abstract = {Small object segmentation is a common task in medical image analysis. Traditional feature-based methods require human intervention while methods based on deep learning train the neural network automatically. However, it is still error prone when applying deep learning methods for small objects. In this paper, Focal FCN was proposed for small object segmentation with limited training data. Firstly, Fully-weighted FCN was proposed to apply an initialization for Focal FCN by adding weights to the background and foreground loss. Secondly, focal loss was applied to make the training focus on wrongly-classified pixels and hence achieve good performance on small object segmentation. Comparisons between FCN, Weighted FCN, Fully-weighted FCN and Focal FCN were tested on customized stent graft marker segmentation.},
  url      = {http://arxiv.org/abs/1711.01506v1%0Apapers3://publication/uuid/623F8EAC-4E04-4FE4-8859-3D35AE108EF0},
}

@InProceedings{art/HatamizadehA_2022,
  author        = {Hatamizadeh, Ali and Tang, Yucheng and Nath, Vishwesh and Yang, Dong and Myronenko, Andriy and Landman, Bennett and Roth, Holger R. and Xu, Daguang},
  booktitle     = {Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022},
  title         = {{UNETR: Transformers for 3D Medical Image Segmentation}},
  year          = {2022},
  pages         = {1748--1758},
  abstract      = {Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful "U-shaped"network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.},
  archiveprefix = {arXiv},
  arxivid       = {2103.10504},
  doi           = {10.1109/WACV51458.2022.00181},
  eprint        = {2103.10504},
  isbn          = {9781665409155},
  keywords      = {Medical Imaging/Imaging for Bioinformatics/Biologi},
}

@InProceedings{art/ValanarasuJ_2021,
  author        = {Valanarasu, Jeya Maria Jose and Oza, Poojan and Hacihaliloglu, Ilker and Patel, Vishal M.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Medical Transformer: Gated Axial-Attention for Medical Image Segmentation}},
  year          = {2021},
  pages         = {36--46},
  volume        = {12901 LNCS},
  abstract      = {Over the past decade, deep convolutional neural networks have been widely adopted for medical image segmentation and shown to achieve adequate performance. However, due to inherent inductive biases present in convolutional architectures, they lack understanding of long-range dependencies in the image. Recently proposed transformer-based architectures that leverage self-attention mechanism encode long-range dependencies and learn representations that are highly expressive. This motivates us to explore transformer-based solutions and study the feasibility of using transformer-based network architectures for medical image segmentation tasks. Majority of existing transformer-based network architectures proposed for vision applications require large-scale datasets to train properly. However, compared to the datasets for vision applications, in medical imaging the number of data samples is relatively low, making it difficult to efficiently train transformers for medical imaging applications. To this end, we propose a gated axial-attention model which extends the existing architectures by introducing an additional control mechanism in the self-attention module. Furthermore, to train the model effectively on medical images, we propose a Local-Global training strategy (LoGo) which further improves the performance. Specifically, we operate on the whole image and patches to learn global and local features, respectively. The proposed Medical Transformer (MedT) is evaluated on three different medical image segmentation datasets and it is shown that it achieves better performance than the convolutional and other related transformer-based architectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer},
  archiveprefix = {arXiv},
  arxivid       = {2102.10662},
  doi           = {10.1007/978-3-030-87193-2_4},
  eprint        = {2102.10662},
  isbn          = {9783030871925},
  issn          = {16113349},
  keywords      = {Medical image segmentation,Self-attention,Transformers},
}

@Article{art/WangJ_2021,
  author   = {Wang, Jing and Liu, Xiuping},
  journal  = {Computer Methods and Programs in Biomedicine},
  title    = {{Medical image recognition and segmentation of pathological slices of gastric cancer based on Deeplab v3+ neural network}},
  year     = {2021},
  issn     = {18727565},
  volume   = {207},
  abstract = {Objective: In order to improve the efficiency of gastric cancer pathological slice image recognition and segmentation of cancerous regions, this paper proposes an automatic gastric cancer segmentation model based on Deeplab v3+ neural network. Methods: Based on 1240 gastric cancer pathological slice images, this paper proposes a multi-scale input Deeplab v3+ network, _and compares it with SegNet, ICNet in sensitivity, specificity, accuracy, and Dice coefficient. Results: The sensitivity of Deeplab v3+ is 91.45%, the specificity is 92.31%, the accuracy is 95.76%, and the Dice coefficient reaches 91.66%, which is more than 12% higher than the SegNet and Faster-RCNN models, and the parameter scale of the model is also greatly reduced. Conclusion: Our automatic gastric cancer segmentation model based on Deeplab v3+ neural network has achieved better results in improving segmentation accuracy and saving computing resources. Deeplab v3+ is worthy of further promotion in the medical image analysis and diagnosis of gastric cancer.},
  doi      = {10.1016/j.cmpb.2021.106210},
  keywords = {Convolutional neural network,Deeplab v3+,Gastric cancer pathological slice image,Image segmentation,Multi-scale input},
  pmid     = {34130088},
}

@Article{art/HuX_2020,
  author   = {Hu, Xianliang and Liu, Zongyu and Zhou, Haiying and Fang, Jianyong and Lu, Hui},
  journal  = {PLoS ONE},
  title    = {{Deep HT: A deep neural network for diagnose on MR images of tumors of the hand}},
  year     = {2020},
  issn     = {19326203},
  number   = {8 August},
  volume   = {15},
  abstract = {Background There are many types of hand tumors, and it is often difficult for imaging diagnosticians to make a correct diagnosis, which can easily lead to misdiagnosis and delay in treatment. Thus in this paper, we propose a deep neural network for diagnose on MR Images of tumors of the hand in order to better define preoperative diagnosis and standardize surgical treatment. Methods We collected MRI figures of 221 patients with hand tumors from one medical center from 2016 to 2019, invited medical experts to annotate the images to form the annotation data set. Then the original image is preprocessed to get the image data set. The data set is randomly divided into ten parts, nine for training and one for test. Next, the data set is input into the neural network system for testing. Finally, average the results of ten experiments as an estimate of the accuracy of the algorithm. Results This research uses 221 images as dataset and the system shows an average confidence level of 71.6% in segmentation of hand tumors. The segmented tumor regions are validated through ground truth analysis and manual analysis by a radiologist. Conclusions With the recent advances in convolutional neural networks, vast improvements have been made for image segmentation, mainly based on the skip-connection-linked encoder decoder deep architectures. Therefore, in this paper, we propose an automatic segmentation method based on DeepLab v3+ and achieved a good diagnostic accuracy rate. Copyright:},
  doi      = {10.1371/journal.pone.0237606},
  pmid     = {32797089},
}

@Article{art/GuanS_2020,
  author        = {Guan, Steven and Khan, Amir A. and Sikdar, Siddhartha and Chitnis, Parag V.},
  journal       = {IEEE Journal of Biomedical and Health Informatics},
  title         = {{Fully Dense UNet for 2-D Sparse Photoacoustic Tomography Artifact Removal}},
  year          = {2020},
  issn          = {21682208},
  number        = {2},
  pages         = {568--576},
  volume        = {24},
  abstract      = {Photoacoustic imaging is an emerging imaging modality that is based upon the photoacoustic effect. In photoacoustic tomography (PAT), the induced acoustic pressure waves are measured by an array of detectors and used to reconstruct an image of the initial pressure distribution. A common challenge faced in PAT is that the measured acoustic waves can only be sparsely sampled. Reconstructing sparsely sampled data using standard methods results in severe artifacts that obscure information within the image. We propose a modified convolutional neural network (CNN) architecture termed fully dense UNet (FD-UNet) for removing artifacts from two-dimensional PAT images reconstructed from sparse data and compare the proposed CNN with the standard UNet in terms of reconstructed image quality.},
  archiveprefix = {arXiv},
  arxivid       = {1808.10848},
  doi           = {10.1109/JBHI.2019.2912935},
  eprint        = {1808.10848},
  keywords      = {Image reconstruction,biomedical imaging,image restoration,photoacoustic imaging,tomography},
  pmid          = {31021809},
}

@Article{art/IsenseeF_202102,
  author   = {Isensee, Fabian and Jaeger, Paul F. and Kohl, Simon A.A. and Petersen, Jens and Maier-Hein, Klaus H.},
  journal  = {Nature Methods},
  title    = {{nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation}},
  year     = {2021},
  issn     = {15487105},
  month    = {feb},
  number   = {2},
  pages    = {203--211},
  volume   = {18},
  abstract = {Biomedical imaging is a driver of scientific discovery and a core component of medical care and is being stimulated by the field of deep learning. While semantic segmentation algorithms enable image analysis and quantification in many applications, the design of respective specialized solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We developed nnU-Net, a deep learning-based segmentation method that automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task. The key design choices in this process are modeled as a set of fixed parameters, interdependent rules and empirical decisions. Without manual intervention, nnU-Net surpasses most existing approaches, including highly specialized solutions on 23 public datasets used in international biomedical segmentation competitions. We make nnU-Net publicly available as an out-of-the-box tool, rendering state-of-the-art segmentation accessible to a broad audience by requiring neither expert knowledge nor computing resources beyond standard network training.},
  doi      = {10.1038/s41592-020-01008-z},
  pmid     = {33288961},
  url      = {http://www.nature.com/articles/s41592-020-01008-z},
}

@InCollection{art/CicekOE_2016,
  author        = {{\c{C}}i{\c{c}}ek, {\"{O}}zg{\"{u}}n and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation}},
  year          = {2016},
  isbn          = {9783319467221},
  pages         = {424--432},
  volume        = {9901 LNCS},
  abstract      = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup,the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup,we assume that a representative,sparsely annotated training set exists. Trained on this data set,the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch,i.e.,no pre-trained network is required. We test the performance of the proposed method on a complex,highly variable 3D structure,the Xenopus kidney,and achieve good results for both use cases.},
  archiveprefix = {arXiv},
  arxivid       = {1606.06650},
  doi           = {10.1007/978-3-319-46723-8_49},
  eprint        = {1606.06650},
  issn          = {16113349},
  keywords      = {3D,Biomedical volumetric image segmentation,Convolutional neural networks,Fully-automated,Semi-automated,Sparse annotation,Xenopus kidney},
  url           = {https://link.springer.com/10.1007/978-3-319-46723-8_49},
}

@Article{art/LiX_201812,
  author        = {Li, Xiaomeng and Chen, Hao and Qi, Xiaojuan and Dou, Qi and Fu, Chi Wing and Heng, Pheng Ann},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes}},
  year          = {2018},
  issn          = {1558254X},
  month         = {dec},
  number        = {12},
  pages         = {2663--2674},
  volume        = {37},
  abstract      = {Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2-D and 3-D FCNs, serve as the backbone in many volumetric image segmentation. However, 2-D convolutions cannot fully leverage the spatial information along the third dimension while 3-D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2-D DenseUNet for efficiently extracting intra-slice features and a 3-D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of the H-DenseUNet in an end-to-end manner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion layer. We extensively evaluated our method on the data set of the MICCAI 2017 Liver Tumor Segmentation Challenge and 3DIRCADb data set. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.},
  archiveprefix = {arXiv},
  arxivid       = {1709.07330},
  doi           = {10.1109/TMI.2018.2845918},
  eprint        = {1709.07330},
  keywords      = {CT,deep learning,hybrid features,liver tumor segmentation},
  pmid          = {29994201},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/WangP_202201,
  author    = {Wang, Pei and Chung, Albert C.S.},
  journal   = {Medical Image Analysis},
  title     = {{Relax and focus on brain tumor segmentation}},
  year      = {2022},
  issn      = {13618423},
  month     = {jan},
  volume    = {75},
  abstract  = {In this paper, we present a Deep Convolutional Neural Networks (CNNs) for fully automatic brain tumor segmentation for both high- and low-grade gliomas in MRI images. Unlike normal tissues or organs that usually have a fixed location or shape, brain tumors with different grades have shown great variation in terms of the location, size, structure, and morphological appearance. Moreover, the severe data imbalance exists not only between the brain tumor and non-tumor tissues, but also among the different sub-regions inside brain tumor (e.g., enhancing tumor, necrotic, edema, and non-enhancing tumor). Therefore, we introduce a hybrid model to address the challenges in the multi-modality multi-class brain tumor segmentation task. First, we propose the dynamic focal Dice loss function that is able to focus more on the smaller tumor sub-regions with more complex structures during training, and the learning capacity of the model is dynamically distributed to each class independently based on its training performance in different training stages. Besides, to better recognize the overall structure of the brain tumor and the morphological relationship among different tumor sub-regions, we relax the boundary constraints for the inner tumor regions in coarse-to-fine fashion. Additionally, a symmetric attention branch is proposed to highlight the possible location of the brain tumor from the asymmetric features caused by growth and expansion of the abnormal tissues in the brain. Generally, to balance the learning capacity of the model between spatial details and high-level morphological features, the proposed model relaxes the constraints of the inner boundary and complex details and enforces more attention on the tumor shape, location, and the harder classes of the tumor sub-regions. The proposed model is validated on the publicly available brain tumor dataset from real patients, BRATS 2019. The experimental results reveal that our model improves the overall segmentation performance in comparison with the state-of-the-art methods, with major progress on the recognition of the tumor shape, the structural relationship of tumor sub-regions, and the segmentation of more challenging tumor sub-regions, e.g., the tumor core, and enhancing tumor.},
  doi       = {10.1016/j.media.2021.102259},
  keywords  = {Attention network,Brain tumor segmentation,Data imbalance,Dynamic loss},
  pmid      = {34800788},
  publisher = {Elsevier B.V.},
}

@Article{art/SubbannaN_2019,
  author    = {Subbanna, Nagesh K. and Rajashekar, Deepthi and Cheng, Bastian and Thomalla, G{\"{o}}tz and Fiehler, Jens and Arbel, Tal and Forkert, Nils D.},
  journal   = {Frontiers in Neurology},
  title     = {{Stroke lesion segmentation in FLAIR MRI datasets using customized Markov random fields}},
  year      = {2019},
  issn      = {16642295},
  number    = {MAY},
  volume    = {10},
  abstract  = {Robust and reliable stroke lesion segmentation is a crucial step toward employing lesion volume as an independent endpoint for randomized trials. The aim of this work was to develop and evaluate a novel method to segment sub-acute ischemic stroke lesions from fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) datasets. After preprocessing of the datasets, a Bayesian technique based on Gabor textures extracted from the FLAIR signal intensities is utilized to generate a first estimate of the lesion segmentation. Using this initial segmentation, a customized voxel-level Markov random field model based on intensity as well as Gabor texture features is employed to refine the stroke lesion segmentation. The proposed method was developed and evaluated based on 151 multi-center datasets from three different databases using a leave-one-patient-out validation approach. The comparison of the automatically segmented stroke lesions with manual ground truth segmentation revealed an average Dice coefficient of 0.582, which is in the upper range of previously presented lesion segmentation methods using multi-modal MRI datasets. Furthermore, the results obtained by the proposed technique are superior compared to the results obtained by two methods based on convolutional neural networks and three phase level-sets, respectively, which performed best in the ISLES 2015 challenge using multi-modal imaging datasets. The results of the quantitative evaluation suggest that the proposed method leads to robust lesion segmentation results using FLAIR MRI datasets only as a follow-up sequence.},
  doi       = {10.3389/fneur.2019.00541},
  keywords  = {Brain lesion segmentation,Classification,Image segmentation,Ischemic stroke,Magnetic resonance imaging},
  publisher = {Frontiers Media S.A.},
}

@Article{art/XiX_2020,
  author    = {Xi, Xue Feng and Wang, Lei and Sheng, Victor S. and Cui, Zhiming and Fu, Baochuan and Hu, Fuyuan},
  journal   = {IEEE Access},
  title     = {{Cascade U-ResNets for Simultaneous Liver and Lesion Segmentation}},
  year      = {2020},
  issn      = {21693536},
  pages     = {68944--68952},
  volume    = {8},
  abstract  = {In recent years, several deep learning networks are proposed to segment 2D or 3D bio-medical images. However, in liver and lesion segmentation, the proportion of interested tissues and lesions are tiny when contrasting to the image background. That is, the objects to be segmented are highly imbalanced in terms of the frequency of occurrences. This makes existing deep learning networks prone to predict pixels of livers and lesions as background. To address this imbalance issue, several loss functions are proposed. Since no researches are having made a comparison among those proposed loss functions, we are curious about that which loss function is the best among them? At the same time, we also want to investigate whether the combination of several different loss functions is effective for liver and lesion segmentation. Firstly, we propose a novel deep learning network (cascade U-ResNets) to produce liver and lesion segmentation simultaneously. Then, we investigate the performance of 5 selected loss functions, WCE (Weighted Cross Entropy), DL (Dice Loss), WDL (Weighted Dice Loss), TL (Teverskry Loss), WTL (Weighted Teversky Loss), with our cascade U-ResNets. We further assemble all cascade U-ResNets trained with different loss functions together to segment livers and lesions jointly on the liver CT (Computed Tomography) volume. Experimental results on the LiTS dataset1 showed our ensemble model can achieve much better results than every individual model for liver segmentation.1http://competitions.codalab.org/competitions/17094#},
  doi       = {10.1109/ACCESS.2020.2985671},
  keywords  = {Data imbalance,deep learning,ensemble learning,lesion segmentation,liver segmentation,medical image segmentation},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/XueC_202105,
  author        = {Xue, Cheng and Zhu, Lei and Fu, Huazhu and Hu, Xiaowei and Li, Xiaomeng and Zhang, Hai and Heng, Pheng Ann},
  journal       = {Medical Image Analysis},
  title         = {{Global guidance network for breast lesion segmentation in ultrasound images}},
  year          = {2021},
  issn          = {13618423},
  month         = {may},
  volume        = {70},
  abstract      = {Automatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that affect women globally. Segmenting breast regions accurately from ultrasound image is a challenging task due to the inherent speckle artifacts, blurry breast lesion boundaries, and inhomogeneous intensity distributions inside the breast lesion regions. Recently, convolutional neural networks (CNNs) have demonstrated remarkable results in medical image segmentation tasks. However, the convolutional operations in a CNN often focus on local regions, which suffer from limited capabilities in capturing long-range dependencies of the input ultrasound image, resulting in degraded breast lesion segmentation accuracy. In this paper, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary detection (BD) modules for boosting the breast ultrasound lesion segmentation. The GGB utilizes the multi-layer integrated feature map as a guidance information to learn the long-range non-local dependencies from both spatial and channel domains. The BD modules learn additional breast lesion boundary map to enhance the boundary quality of a segmentation result refinement. Experimental results on a public dataset and a collected dataset show that our network outperforms other medical image segmentation methods and the recent semantic segmentation methods on breast ultrasound lesion segmentation. Moreover, we also show the application of our network on the ultrasound prostate segmentation, in which our method better identifies prostate regions than state-of-the-art networks.},
  archiveprefix = {arXiv},
  arxivid       = {2104.01896},
  doi           = {10.1016/j.media.2021.101989},
  eprint        = {2104.01896},
  keywords      = {Breast lesion segmentation,Deep neural network,Non-local features},
  pmid          = {33640719},
  publisher     = {Elsevier B.V.},
}

@Article{art/CerriS_202101,
  author        = {Cerri, Stefano and Puonti, Oula and Meier, Dominik S. and Wuerfel, Jens and M{\"{u}}hlau, Mark and Siebner, Hartwig R. and {Van Leemput}, Koen},
  journal       = {NeuroImage},
  title         = {{A contrast-adaptive method for simultaneous whole-brain and lesion segmentation in multiple sclerosis}},
  year          = {2021},
  issn          = {10959572},
  month         = {jan},
  volume        = {225},
  abstract      = {Here we present a method for the simultaneous segmentation of white matter lesions and normal-appearing neuroanatomical structures from multi-contrast brain MRI scans of multiple sclerosis patients. The method integrates a novel model for white matter lesions into a previously validated generative model for whole-brain segmentation. By using separate models for the shape of anatomical structures and their appearance in MRI, the algorithm can adapt to data acquired with different scanners and imaging protocols without retraining. We validate the method using four disparate datasets, showing robust performance in white matter lesion segmentation while simultaneously segmenting dozens of other brain structures. We further demonstrate that the contrast-adaptive method can also be safely applied to MRI scans of healthy controls, and replicate previously documented atrophy patterns in deep gray matter structures in MS. The algorithm is publicly available as part of the open-source neuroimaging package FreeSurfer.},
  archiveprefix = {arXiv},
  arxivid       = {2005.05135},
  doi           = {10.1016/j.neuroimage.2020.117471},
  eprint        = {2005.05135},
  keywords      = {Generative model,Lesion segmentation,Multiple sclerosis,Whole-brain segmentation},
  pmid          = {33099007},
  publisher     = {Academic Press Inc.},
}

@Article{art/LiuT_2019,
  author    = {Liu, Tao and Tian, Yun and Zhao, Shifeng and Huang, Xiaoying and Wang, Qingjun},
  journal   = {IEEE Access},
  title     = {{Automatic Whole Heart Segmentation Using a Two-Stage U-Net Framework and an Adaptive Threshold Window}},
  year      = {2019},
  issn      = {21693536},
  pages     = {83628--83636},
  volume    = {7},
  abstract  = {Whole heart segmentation is an important medical imaging method used to enable clinical applications. However, automatic segmentation of the heart is still a challenging task due to the complexity and particularity of medical images, especially when the heart is segmented into substructures. In this study, we present a training strategy that relies on a two-stage U-Net framework and an adaptive threshold window to automatically segment a whole heart and its substructures. The two-stage U-Net framework consists of a region of interest (ROI) detection of the whole heart and accurate segmentation of the heart substructures. The adaptive threshold window method is used to remove the noisy parts of the data while preserving the anatomical relationships between local regions. Experiments were performed on a dataset from the MM-WHS Challenge 2017. The proposed approach resulted in a high segmentation accuracy with a 79.3% and 95.5% Dice similarity coefficient for the whole heart and ascending aorta segmentation, respectively, using limited GPU computing resources and small amounts of annotated data. The full implementation and configuration files in this paper are available at https://github.com/liut969/Whole-Heart-Segmentation.},
  doi       = {10.1109/ACCESS.2019.2923318},
  keywords  = {Adaptive threshold window,Segmentation,U-Net,Whole heart segmentation},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/WangY_201907,
  author        = {Wang, Yan and Zhou, Yuyin and Shen, Wei and Park, Seyoun and Fishman, Elliot K. and Yuille, Alan L.},
  journal       = {Medical Image Analysis},
  title         = {{Abdominal multi-organ segmentation with organ-attention networks and statistical fusion}},
  year          = {2019},
  issn          = {13618423},
  month         = {jul},
  pages         = {88--102},
  volume        = {55},
  abstract      = {Accurate and robust segmentation of abdominal organs on CT is essential for many clinical applications such as computer-aided diagnosis and computer-aided surgery. But this task is challenging due to the weak boundaries of organs, the complexity of the background, and the variable sizes of different organs. To address these challenges, we introduce a novel framework for multi-organ segmentation of abdominal regions by using organ-attention networks with reverse connections (OAN-RCs)which are applied to 2D views, of the 3D CT volume, and output estimates which are combined by statistical fusion exploiting structural similarity. More specifically, OAN is a two-stage deep convolutional network, where deep network features from the first stage are combined with the original image, in a second stage, to reduce the complex background and enhance the discriminative information for the target organs. Intuitively, OAN reduces the effect of the complex background by focusing attention so that each organ only needs to be discriminated from its local background. RCs are added to the first stage to give the lower layers more semantic information thereby enabling them to adapt to the sizes of different organs. Our networks are trained on 2D views (slices)enabling us to use holistic information and allowing efficient computation (compared to using 3D patches). To compensate for the limited cross-sectional information of the original 3D volumetric CT, e.g., the connectivity between neighbor slices, multi-sectional images are reconstructed from the three different 2D view directions. Then we combine the segmentation results from the different views using statistical fusion, with a novel term relating the structural similarity of the 2D views to the original 3D structure. To train the network and evaluate results, 13 structures were manually annotated by four human raters and confirmed by a senior expert on 236 normal cases. We tested our algorithm by 4-fold cross-validation and computed Dice–S{\o}rensen similarity coefficients (DSC)and surface distances for evaluating our estimates of the 13 structures. Our experiments show that the proposed approach gives strong results and outperforms 2D- and 3D-patch based state-of-the-art methods in terms of DSC and mean surface distances.},
  archiveprefix = {arXiv},
  arxivid       = {1804.08414},
  doi           = {10.1016/j.media.2019.04.005},
  eprint        = {1804.08414},
  keywords      = {Abdominal CT,Multi-organ segmentation,Organ-attention network,Reverse connection,Statistical label fusion},
  pmid          = {31035060},
  publisher     = {Elsevier B.V.},
}

@Article{art/FangX_202011,
  author        = {Fang, Xi and Yan, Pingkun},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction}},
  year          = {2020},
  issn          = {1558254X},
  month         = {nov},
  number        = {11},
  pages         = {3619--3629},
  volume        = {39},
  abstract      = {Shortage of fully annotated datasets has been a limiting factor in developing deep learning based image segmentation algorithms and the problem becomes more pronounced in multi-organ segmentation. In this paper, we propose a unified training strategy that enables a novel multi-scale deep neural network to be trained on multiple partially labeled datasets for multi-organ segmentation. In addition, a new network architecture for multi-scale feature abstraction is proposed to integrate pyramid input and feature analysis into a U-shape pyramid structure. To bridge the semantic gap caused by directly merging features from different scales, an equal convolutional depth mechanism is introduced. Furthermore, we employ a deep supervision mechanism to refine the outputs in different scales. To fully leverage the segmentation features from all the scales, we design an adaptive weighting layer to fuse the outputs in an automatic fashion. All these mechanisms together are integrated into a Pyramid Input Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposed method was evaluated on four publicly available datasets, including BTCV, LiTS, KiTS and Spleen, where very promising performance has been achieved. The source code of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FAN to facilitate others to reproduce the work and build their own models using the introduced mechanisms.},
  archiveprefix = {arXiv},
  arxivid       = {2001.00208},
  doi           = {10.1109/TMI.2020.3001036},
  eprint        = {2001.00208},
  keywords      = {Convolutional neural networks,Deep learning,Medical image segmentation,Multi-organ segmentation,Multi-scale feature,Multiple datasets},
  pmid          = {32746108},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/ConzeP_202107,
  author        = {Conze, Pierre Henri and Kavur, Ali Emre and {Cornec-Le Gall}, Emilie and Gezer, Naciye Sinem and {Le Meur}, Yannick and Selver, M. Alper and Rousseau, Fran{\c{c}}ois},
  journal       = {Artificial Intelligence in Medicine},
  title         = {{Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks}},
  year          = {2021},
  issn          = {18732860},
  month         = {jul},
  volume        = {117},
  abstract      = {Abdominal anatomy segmentation is crucial for numerous applications from computer-assisted diagnosis to image-guided surgery. In this context, we address fully-automated multi-organ segmentation from abdominal CT and MR images using deep learning. The proposed model extends standard conditional generative adversarial networks. Additionally to the discriminator which enforces the model to create realistic organ delineations, it embeds cascaded partially pre-trained convolutional encoder-decoders as generator. Encoder fine-tuning from a large amount of non-medical images alleviates data scarcity limitations. The network is trained end-to-end to benefit from simultaneous multi-level segmentation refinements using auto-context. Employed for healthy liver, kidneys and spleen segmentation, our pipeline provides promising results by outperforming state-of-the-art encoder-decoder schemes. Followed for the Combined Healthy Abdominal Organ Segmentation (CHAOS) challenge organized in conjunction with the IEEE International Symposium on Biomedical Imaging 2019, it gave us the first rank for three competition categories: liver CT, liver MR and multi-organ MR segmentation. Combining cascaded convolutional and adversarial networks strengthens the ability of deep learning pipelines to automatically delineate multiple abdominal organs, with good generalization capability. The comprehensive evaluation provided suggests that better guidance could be achieved to help clinicians in abdominal image interpretation and clinical decision making.},
  archiveprefix = {arXiv},
  arxivid       = {2001.09521},
  doi           = {10.1016/j.artmed.2021.102109},
  eprint        = {2001.09521},
  keywords      = {Abdominal images,Adversarial learning,Cascaded networks,Convolutional encoder-decoders,Multi-organ segmentation},
  pmid          = {34127239},
  publisher     = {Elsevier B.V.},
}

@Article{art/TobonGomezC_201308,
  author    = {Tobon-Gomez, C. and {De Craene}, M. and McLeod, K. and Tautz, L. and Shi, W. and Hennemuth, A. and Prakosa, A. and Wang, H. and Carr-White, G. and Kapetanakis, S. and Lutz, A. and Rasche, V. and Schaeffter, T. and Butakoff, C. and Friman, O. and Mansi, T. and Sermesant, M. and Zhuang, X. and Ourselin, S. and Peitgen, H. O. and Pennec, X. and Razavi, R. and Rueckert, D. and Frangi, A. F. and Rhode, K. S.},
  journal   = {Medical Image Analysis},
  title     = {{Benchmarking framework for myocardial tracking and deformation algorithms: An open access database}},
  year      = {2013},
  issn      = {13618415},
  month     = {aug},
  number    = {6},
  pages     = {632--648},
  volume    = {17},
  abstract  = {In this paper we present a benchmarking framework for the validation of cardiac motion analysis algorithms. The reported methods are the response to an open challenge that was issued to the medical imaging community through a MICCAI workshop. The database included magnetic resonance (MR) and 3D ultrasound (3DUS) datasets from a dynamic phantom and 15 healthy volunteers. Participants processed 3D tagged MR datasets (3DTAG), cine steady state free precession MR datasets (SSFP) and 3DUS datasets, amounting to 1158 image volumes. Ground-truth for motion tracking was based on 12 landmarks (4 walls at 3 ventricular levels). They were manually tracked by two observers in the 3DTAG data over the whole cardiac cycle, using an in-house application with 4D visualization capabilities. The median of the inter-observer variability was computed for the phantom dataset (0.77. mm) and for the volunteer datasets (0.84. mm). The ground-truth was registered to 3DUS coordinates using a point based similarity transform. Four institutions responded to the challenge by providing motion estimates for the data: Fraunhofer MEVIS (MEVIS), Bremen, Germany; Imperial College London - University College London (IUCL), UK; Universitat Pompeu Fabra (UPF), Barcelona, Spain; Inria-Asclepios project (INRIA), France. Details on the implementation and evaluation of the four methodologies are presented in this manuscript. The manually tracked landmarks were used to evaluate tracking accuracy of all methodologies. For 3DTAG, median values were computed over all time frames for the phantom dataset (MEVIS = 1.20. mm, IUCL = 0.73. mm, UPF = 1.10. mm, INRIA = 1.09. mm) and for the volunteer datasets (MEVIS = 1.33. mm, IUCL = 1.52. mm, UPF = 1.09. mm, INRIA = 1.32. mm). For 3DUS, median values were computed at end diastole and end systole for the phantom dataset (MEVIS = 4.40. mm, UPF = 3.48. mm, INRIA = 4.78. mm) and for the volunteer datasets (MEVIS = 3.51. mm, UPF = 3.71. mm, INRIA = 4.07. mm). For SSFP, median values were computed at end diastole and end systole for the phantom dataset(UPF = 6.18. mm, INRIA = 3.93. mm) and for the volunteer datasets (UPF = 3.09. mm, INRIA = 4.78. mm). Finally, strain curves were generated and qualitatively compared. Good agreement was found between the different modalities and methodologies, except for radial strain that showed a high variability in cases of lower image quality. {\textcopyright} 2013 Elsevier B.V.},
  doi       = {10.1016/j.media.2013.03.008},
  keywords  = {3D tagged MR,3D ultrasound,Cardiac motion tracking,Multimodal,Spatiotemporal registration},
  pmid      = {23708255},
  publisher = {Elsevier},
}

@Misc{art/_,
  title   = {{Evaluation Criterion | LVQuan18 - Left Ventricle Full Quantification Challenge MICCAI2018}},
  url     = {https://lvquan18.github.io/2018/03/12/criterion.html},
  urldate = {2022-05-20},
}

@Misc{art/BajajA_2021,
  author    = {Bajaj, Aayush},
  title     = {{Performance Metrics in Machine Learning [Complete Guide] - neptune.ai}},
  year      = {2021},
  booktitle = {Neptune Blog},
  pages     = {1--15},
  url       = {https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide},
  urldate   = {2022-05-20},
}

@Article{art/BernardO_201604,
  author   = {Bernard, Olivier and Bosch, Johan G. and Heyde, Brecht and Alessandrini, Martino and Barbosa, Daniel and Camarasu-Pop, Sorina and Cervenansky, Frederic and Valette, S{\'{e}}bastien and Mirea, Oana and Bernier, Michel and Jodoin, Pierre Marc and Domingos, Jaime Santo and Stebbing, Richard V. and Keraudren, Kevin and Oktay, Ozan and Caballero, Jose and Shi, Wei and Rueckert, Daniel and Milletari, Fausto and Ahmadi, Seyed Ahmad and Smistad, Erik and Lindseth, Frank and {Van Stralen}, Maartje and Wang, Chen and Smedby, {\"{O}}rjan and Donal, Erwan and Monaghan, Mark and Papachristidis, Alex and Geleijnse, Marcel L. and Galli, Elena and D'Hooge, Jan},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Standardized Evaluation System for Left Ventricular Segmentation Algorithms in 3D Echocardiography}},
  year     = {2016},
  issn     = {1558254X},
  month    = {apr},
  number   = {4},
  pages    = {967--977},
  volume   = {35},
  abstract = {Real-time 3D Echocardiography (RT3DE) has been proven to be an accurate tool for left ventricular (LV) volume assessment. However, identification of the LV endocardium remains a challenging task, mainly because of the low tissue/blood contrast of the images combined with typical artifacts. Several semi and fully automatic algorithms have been proposed for segmenting the endocardium in RT3DE data in order to extract relevant clinical indices, but a systematic and fair comparison between such methods has so far been impossible due to the lack of a publicly available common database. Here, we introduce a standardized evaluation framework to reliably evaluate and compare the performance of the algorithms developed to segment the LV border in RT3DE. A database consisting of 45 multivendor cardiac ultrasound recordings acquired at different centers with corresponding reference measurements from three experts are made available. The algorithms from nine research groups were quantitatively evaluated and compared using the proposed online platform. The results showed that the best methods produce promising results with respect to the experts' measurements for the extraction of clinical indices, and that they offer good segmentation precision in terms of mean distance error in the context of the experts' variability range. The platform remains open for new submissions.},
  doi      = {10.1109/TMI.2015.2503890},
  keywords = {Endocardium,left ventricle segmentation,realtime 3D echocardiography,standardized evaluation system},
  pmid     = {26625409},
  url      = {http://ieeexplore.ieee.org/document/7337434/},
}

@Article{art/BejnordiB_201712,
  author   = {Bejnordi, Babak Ehteshami and Veta, Mitko and {Van Diest}, Paul Johannes and {Van Ginneken}, Bram and Karssemeijer, Nico and Litjens, Geert and {Van Der Laak}, Jeroen A.W.M. and Hermsen, Meyke and Manson, Quirine F. and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and {Van Dijk}, Marcory C.R.F. and Bult, Peter and Beca, Francisco and Beck, Andrew H. and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang Jing and Heng, Pheng Ann and Ha{\ss}, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and {\"{O}}ner, Mustafa {\"{U}}mit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Phoulady, Hady Ahmady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Ven{\^{a}}ncio, Rui},
  journal  = {JAMA - Journal of the American Medical Association},
  title    = {{Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer}},
  year     = {2017},
  issn     = {15383598},
  month    = {dec},
  number   = {22},
  pages    = {2199--2210},
  volume   = {318},
  abstract = {IMPORTANCE: Application of deep learning algorithms to whole-slide pathology imagescan potentially improve diagnostic accuracy and efficiency. OBJECTIVE: Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin-stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists' diagnoses in a diagnostic setting. DESIGN, SETTING, AND PARTICIPANTS: Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). EXPOSURES: Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. MAIN OUTCOMES AND MEASURES: The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. RESULTS: The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4% [95% CI, 64.3%-80.4%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P <.001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95% CI, 0.927-0.998] for the pathologist WOTC). CONCLUSIONS AND RELEVANCE: In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
  doi      = {10.1001/jama.2017.14585},
  pmid     = {29234806},
  url      = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.14585},
}

@Book{art/FaroS_2010,
  author    = {Faro, Scott H. and Mohamed, Feroze B.},
  editor    = {Faro, Scott H. and Mohamed, Feroze B.},
  publisher = {Springer New York},
  title     = {{BOLD fMRI: A guide to functional imaging for neuroscientists}},
  year      = {2010},
  address   = {New York, NY},
  isbn      = {9781441913289},
  abstract  = {BOLD fMRI: A Guide for Neuroscientists is a compilation of key chapters selected by the editors to present neuroscientists with a comprehensive introduction to the basic principles of BOLD fMRI and its applications in cognitive neuroscience. This comprehensive book introduces important topics in BOLD fMRI such as scanning methodologies, experimental design and data analysis, and its challenges and limitations. Clinical applications of functional imaging are included in chapters on brain mapping in cognitive neuroscience, memory in aging and dementia, language systems, and the Wada test. Also included is a full-color neuroanatomical atlas of the basic motor, sensory, and cognitive activation sites that are discussed throughout the book. With contributions from internationally recognized neuroradiologists, neurologists, psychiatrists, cognitive neuroscientists, and physicists, this volume will appeal to all neuroscientists seeking a concise introduction to the fundamentals of BOLD fMRI. {\textcopyright} Springer Science + Business Media, LLC 2010. All rights reserved.},
  booktitle = {BOLD fMRI: A Guide to Functional Imaging for Neuroscientists},
  doi       = {10.1007/978-1-4419-1329-6},
  pages     = {1--291},
  url       = {http://link.springer.com/10.1007/978-1-4419-1329-6},
}

@Article{art/AgravatR_202108,
  author        = {Agravat, Rupal R. and Raval, Mehul S.},
  journal       = {Archives of Computational Methods in Engineering},
  title         = {{A Survey and Analysis on Automated Glioma Brain Tumor Segmentation and Overall Patient Survival Prediction}},
  year          = {2021},
  issn          = {18861784},
  month         = {aug},
  number        = {5},
  pages         = {4117--4152},
  volume        = {28},
  abstract      = {Glioma is the deadliest brain tumor with high mortality. Treatment planning by human experts depends on the proper diagnosis of physical symptoms along with Magnetic Resonance (MR) image analysis. Highly variability of a brain tumor in terms of size, shape, location, and a high volume of MR images make the analysis time-consuming. Automatic segmentation methods achieve a reduction in time with excellent reproducible results. The article aims to survey the advancement of automated methods for Glioma brain tumor segmentation. It is also essential to make an objective evaluation of various models based on the benchmark. Therefore, the 2012–2019 BraTS challenges evaluate the state-of-the-art methods. The complexity of the tasks facing this challenge has grown from segmentation (Task 1) to overall survival prediction (Task 2) to uncertainty prediction for classification (Task 3). The paper covers the complete gamut of brain tumor segmentation using handcrafted features to deep neural network models for Task 1. The aim is to showcase a complete change of trends in automated brain tumor models. The paper also covers end to end joint models involving brain tumor segmentation and overall survival prediction. All the methods are probed, and parameters that affect performance are tabulated and analyzed.},
  archiveprefix = {arXiv},
  arxivid       = {2101.10599},
  doi           = {10.1007/s11831-021-09559-w},
  eprint        = {2101.10599},
  url           = {https://link.springer.com/10.1007/s11831-021-09559-w},
}

@Article{art/CouinaudC_1999,
  author   = {Couinaud, C.},
  journal  = {Digestive Surgery},
  title    = {{Liver anatomy: Portal (and suprahepatic) or biliary segmentation}},
  year     = {1999},
  issn     = {02534886},
  number   = {6},
  pages    = {459--467},
  volume   = {16},
  abstract = {Background/Aims: In liver anatomy and surgery, is portal and hepatic vein segmentation (French segmentation) to be preferred over arteriobiliary segmentation (Healey and Schroy, North American segmentation)? Methods: Several embryological arguments and an analysis of anatomical data from a personal collection of 110 vasculobiliary casts were made. Results: Embryological arguments: Portal vein branching appears first, arteriobiliary branching secondly follows the portal vein distribution. Segment II (the left lateral sector) is the development of the right lateral embryological lobe. The umbilical vein enters the left portion of the middle embryological lobe, forming segment IV on the right and segment III on the left: this is the left paramedian sector. So the left portal fissure (between left and middle lobes) transversally crosses the classical left lobe, which is not a portal unit. Segment VI is a late secondary prominence of segment VII, reaching the anterior margin of the liver only in man. Anatomical arguments: hepatic vein segmentation must be added to portal segmentation; the academic left lobe is the left hepatic vein sector, and the left hepatic fissure separates the classical right and left lobes. Portal vein segmentation must be preferred: portal vein duplication of branches of first order occurs only in 23.5 of the cases, while arteriobiliary duplication of first-order branches is noted in 50 of the livers, portal segmentation being much simpler. Conclusions: Portal and hepatic vein segmentation seems to be much more accurate. Copyright (C) 1999 S. Karger AG, Basel.},
  doi      = {10.1159/000018770},
  keywords = {Liver anatomy,Liver resections,Liver segmentation},
  pmid     = {10805544},
  url      = {https://www.karger.com/Article/FullText/18770},
}

@Article{art/BoulangerM_202109,
  author   = {Boulanger, M. and Nunes, Jean Claude and Chourak, H. and Largent, A. and Tahri, S. and Acosta, O. and {De Crevoisier}, R. and Lafond, C. and Barateau, A.},
  journal  = {Physica Medica},
  title    = {{Deep learning methods to generate synthetic CT from MRI in radiotherapy: A literature review}},
  year     = {2021},
  issn     = {1724191X},
  month    = {sep},
  pages    = {265--281},
  volume   = {89},
  abstract = {Purpose: In radiotherapy, MRI is used for target volume and organs-at-risk delineation for its superior soft-tissue contrast as compared to CT imaging. However, MRI does not provide the electron density of tissue necessary for dose calculation. Several methods of synthetic-CT (sCT) generation from MRI data have been developed for radiotherapy dose calculation. This work reviewed deep learning (DL) sCT generation methods and their associated image and dose evaluation, in the context of MRI-based dose calculation. Methods: We searched the PubMed and ScienceDirect electronic databases from January 2010 to March 2021. For each paper, several items were screened and compiled in figures and tables. Results: This review included 57 studies. The DL methods were either generator-only based (45% of the reviewed studies), or generative adversarial network (GAN) architecture and its variants (55% of the reviewed studies). The brain and pelvis were the most commonly investigated anatomical localizations (39% and 28% of the reviewed studies, respectively), and more rarely, the head-and-neck (H&N) (15%), abdomen (10%), liver (5%) or breast (3%). All the studies performed an image evaluation of sCTs with a diversity of metrics, with only 36 studies performing dosimetric evaluations of sCT. Conclusions: The median mean absolute errors were around 76 HU for the brain and H&N sCTs and 40 HU for the pelvis sCTs. For the brain, the mean dose difference between the sCT and the reference CT was <2%. For the H&N and pelvis, the mean dose difference was below 1% in most of the studies. Recent GAN architectures have advantages compared to generator-only, but no superiority was found in term of image or dose sCT uncertainties. Key challenges of DL-based sCT generation methods from MRI in radiotherapy is the management of movement for abdominal and thoracic localizations, the standardization of sCT evaluation, and the investigation of multicenter impacts.},
  doi      = {10.1016/j.ejmp.2021.07.027},
  keywords = {Deep learning,Dose calculation,MRI,Radiation therapy,Synthetic-CT},
  pmid     = {34474325},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1120179721002714},
}

@Article{art/HowardA_201704,
  author        = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  title         = {{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}},
  year          = {2017},
  month         = {apr},
  abstract      = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arXiv},
  arxivid       = {1704.04861},
  eprint        = {1704.04861},
  url           = {http://arxiv.org/abs/1704.04861},
}

@Article{art/EveringhamM_201006,
  author   = {Everingham, Mark and {Van Gool}, Luc and Williams, Christopher K.I. and Winn, John and Zisserman, Andrew},
  journal  = {International Journal of Computer Vision},
  title    = {{The pascal visual object classes (VOC) challenge}},
  year     = {2010},
  issn     = {09205691},
  month    = {jun},
  number   = {2},
  pages    = {303--338},
  volume   = {88},
  abstract = {The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection. This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different, what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge, and proposes directions for future improvement and extension. {\textcopyright} 2009 Springer Science+Business Media, LLC.},
  doi      = {10.1007/s11263-009-0275-4},
  keywords = {Benchmark,Database,Object detection,Object recognition},
  url      = {http://link.springer.com/10.1007/s11263-009-0275-4},
}

@Article{art/DingY_201902,
  author   = {Ding, Yiming and Sohn, Jae Ho and Kawczynski, Michael G. and Trivedi, Hari and Harnish, Roy and Jenkins, Nathaniel W. and Lituiev, Dmytro and Copeland, Timothy P. and Aboian, Mariam S. and Aparici, Carina Mari and Behr, Spencer C. and Flavell, Robert R. and Huang, Shih Ying and Zalocusky, Kelly A. and Nardo, Lorenzo and Seo, Youngho and Hawkins, Randall A. and Pampaloni, Miguel Hernandez and Hadley, Dexter and Franc, Benjamin L.},
  journal  = {Radiology},
  title    = {{A deep learning model to predict a diagnosis of Alzheimer disease by using 18 F-FDG PET of the brain}},
  year     = {2019},
  issn     = {15271315},
  month    = {feb},
  number   = {3},
  pages    = {456--464},
  volume   = {290},
  abstract = {Purpose: To develop and validate a deep learning algorithm that predicts the final diagnosis of Alzheimer disease (AD), mild cognitive impairment, or neither at fluorine 18 ( 18 F) fluorodeoxyglucose (FDG) PET of the brain and compare its performance to that of radiologic readers. Materials and Methods: Prospective 18 F-FDG PET brain images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) (2109 imaging studies from 2005 to 2017, 1002 patients) and retrospective independent test set (40 imaging studies from 2006 to 2016, 40 patients) were collected. Final clinical diagnosis at follow-up was recorded. Convolutional neural network of InceptionV3 architecture was trained on 90% of ADNI data set and tested on the remaining 10%, as well as the independent test set, with performance compared to radiologic readers. Model was analyzed with sensitivity, specificity, receiver operating characteristic (ROC), saliency map, and t-distributed stochastic neighbor embedding. Results: The algorithm achieved area under the ROC curve of 0.98 (95% confidence interval: 0.94, 1.00) when evaluated on predicting the final clinical diagnosis of AD in the independent test set (82% specificity at 100% sensitivity), an average of 75.8 months prior to the final diagnosis, which in ROC space outperformed reader performance (57% [four of seven] sensitivity, 91% [30 of 33] specificity; P , .05). Saliency map demonstrated attention to known areas of interest but with focus on the entire brain. Conclusion: By using fluorine 18 fluorodeoxyglucose PET of the brain, a deep learning algorithm developed for early prediction of Alzheimer disease achieved 82% specificity at 100% sensitivity, an average of 75.8 months prior to the final diagnosis.},
  doi      = {10.1148/radiol.2018180958},
  pmid     = {30398430},
  url      = {http://pubs.rsna.org/doi/10.1148/radiol.2018180958},
}

@Article{art/HelalyH_2021,
  author   = {Helaly, Hadeer A. and Badawy, Mahmoud and Haikal, Amira Y.},
  journal  = {Cognitive Computation},
  title    = {{Deep Learning Approach for Early Detection of Alzheimer's Disease}},
  year     = {2021},
  issn     = {18669964},
  abstract = {Alzheimer's disease (AD) is a chronic, irreversible brain disorder, no effective cure for it till now. However, available medicines can delay its progress. Therefore, the early detection of AD plays a crucial role in preventing and controlling its progression. The main objective is to design an end-to-end framework for early detection of Alzheimer's disease and medical image classification for various AD stages. A deep learning approach, specifically convolutional neural networks (CNN), is used in this work. Four stages of the AD spectrum are multi-classified. Furthermore, separate binary medical image classifications are implemented between each two-pair class of AD stages. Two methods are used to classify the medical images and detect AD. The first method uses simple CNN architectures that deal with 2D and 3D structural brain scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset based on 2D and 3D convolution. The second method applies the transfer learning principle to take advantage of the pre-trained models for medical image classifications, such as the VGG19 model. Due to the COVID-19 pandemic, it is difficult for people to go to hospitals periodically to avoid gatherings and infections. As a result, Alzheimer's checking web application is proposed using the final qualified proposed architectures. It helps doctors and patients to check AD remotely. It also determines the AD stage of the patient based on the AD spectrum and advises the patient according to its AD stage. Nine performance metrics are used in the evaluation and the comparison between the two methods. The experimental results prove that the CNN architectures for the first method have the following characteristics: suitable simple structures that reduce computational complexity, memory requirements, overfitting, and provide manageable time. Besides, they achieve very promising accuracies, 93.61% and 95.17% for 2D and 3D multi-class AD stage classifications. The VGG19 pre-trained model is fine-tuned and achieved an accuracy of 97% for multi-class AD stage classifications.},
  doi      = {10.1007/s12559-021-09946-2},
  keywords = {Alzheimer's disease,Brain MRI,Convolutional neural network (CNN),Deep learning,Medical image classification},
  url      = {https://doi.org/10.1007/s12559-021-09946-2},
}

@Article{art/HeJ_201805,
  author    = {He, Jun Yan and Wu, Xiao and Jiang, Yu Gang and Peng, Qiang and Jain, Ramesh},
  journal   = {IEEE Transactions on Image Processing},
  title     = {{Hookworm Detection in Wireless Capsule Endoscopy Images with Deep Learning}},
  year      = {2018},
  issn      = {10577149},
  month     = {may},
  number    = {5},
  pages     = {2379--2392},
  volume    = {27},
  abstract  = {As one of the most common human helminths, hookworm is a leading cause of maternal and child morbidity, which seriously threatens human health. Recently, wireless capsule endoscopy (WCE) has been applied to automatic hookworm detection. Unfortunately, it remains a challenging task. In recent years, deep convolutional neural network (CNN) has demonstrated impressive performance in various image and video analysis tasks. In this paper, a novel deep hookworm detection framework is proposed for WCE images, which simultaneously models visual appearances and tubular patterns of hookworms. This is the first deep learning framework specifically designed for hookworm detection in WCE images. Two CNN networks, namely edge extraction network and hookworm classification network, are seamlessly integrated in the proposed framework, which avoid the edge feature caching and speed up the classification. Two edge pooling layers are introduced to integrate the tubular regions induced from edge extraction network and the feature maps from hookworm classification network, leading to enhanced feature maps emphasizing the tubular regions. Experiments have been conducted on one of the largest WCE datasets with 440K WCE images, which demonstrate the effectiveness of the proposed hookworm detection framework. It significantly outperforms the state-of-The-Art approaches. The high sensitivity and accuracy of the proposed method in detecting hookworms shows its potential for clinical application.},
  doi       = {10.1109/TIP.2018.2801119},
  keywords  = {Hookworm detection,computer-Aided detection,convolutional neural network,deep learning,wireless capsule endoscopy},
  pmid      = {29470172},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Misc{art/ShahV_2017,
  author    = {Shah, Vikas},
  title     = {{Lipid-rich adrenal adenoma}},
  year      = {2017},
  booktitle = {Radiology Case, Radiopaedia.org},
  doi       = {https://doi.org/10.53347/rID-49349},
  url       = {https://radiopaedia.org/cases/lipid-rich-adrenal-adenoma-1},
  urldate   = {2022-05-12},
}

@Misc{art/GarciaLayanaA_2017,
  author    = {Garcia-Layana, Alfredo and Ciuffo, Gianfranco and {Javier Zarranz-Ventura} and Alvarez-Vidal, Aurora},
  title     = {{Optical Coherence Tomography in Age-related Macular Degeneration}},
  year      = {2017},
  booktitle = {AMDBOOK},
  url       = {https://amdbook.org/content/optical-coherence-tomography-age-related-macular-degeneration},
  urldate   = {2022-05-12},
}

@Book{art/CoscasG_2009,
  author    = {Coscas, Gabriel},
  publisher = {Springer Science & Business Media},
  title     = {{Optical coherence tomography in age-related macular degeneration}},
  year      = {2009},
}

@Article{art/VincentR_201608,
  author    = {Vincent, Robert D. and Neelin, Peter and Khalili-Mahani, Najmeh and Janke, Andrew L. and Fonov, Vladimir S. and Robbins, Steven M. and Baghdadi, Leila and Lerch, Jason and Sled, John G. and Adalat, Reza and Macdonald, David and Zijdenbos, Alex P. and Collins, D. Louis and Evans, Alan C.},
  journal   = {Frontiers in neuroinformatics},
  title     = {{MINC 2.0: A Flexible Format for Multi-Modal Images}},
  year      = {2016},
  issn      = {1662-5196},
  month     = {aug},
  number    = {AUG},
  volume    = {10},
  abstract  = {It is often useful that an imaging data format can afford rich metadata, be flexible, scale to very large file sizes, support multi-modal data, and have strong inbuilt mechanisms for data provenance. Beginning in 1992, MINC was developed as a system for flexible, self-documenting representation of neuroscientific imaging data with arbitrary orientation and dimensionality. The MINC system incorporates three broad components: a file format specification, a programming library, and a growing set of tools. In the early 2000's the MINC developers created MINC 2.0, which added support for 64-bit file sizes, internal compression, and a number of other modern features. Because of its extensible design, it has been easy to incorporate details of provenance in the header metadata, including an explicit processing history, unique identifiers, and vendor-specific scanner settings. This makes MINC ideal for use in large scale imaging studies and databases. It also makes it easy to adapt to new scanning sequences and modalities.},
  doi       = {10.3389/FNINF.2016.00035},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vincent et al. - 2016 - MINC 2.0 A Flexible Format for Multi-Modal Images.pdf:pdf},
  keywords  = {Alan C Evans,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC4980430,Peter Neelin,PubMed Abstract,Robert D Vincent,doi:10.3389/fninf.2016.00035,pmid:27563289},
  pmid      = {27563289},
  publisher = {Front Neuroinform},
  url       = {https://pubmed.ncbi.nlm.nih.gov/27563289/},
}

@Article{art/MccormickM_2014,
  author   = {Mccormick, Matthew and Liu, Xiaoxiao and Jomier, Julien and Marion, Charles and Ibanez, Luis},
  journal  = {Frontiers in Neuroinformatics},
  title    = {{Itk: Enabling reproducible research and open science}},
  year     = {2014},
  issn     = {16625196},
  number   = {FEB},
  volume   = {8},
  abstract = {Reproducibility verification is essential to the practice of the scientific method. Researchers report their findings, which are strengthened as other independent groups in the scientific community share similar outcomes. In the many scientific fields where software has become a fundamental tool for capturing and analyzing data, this requirement of reproducibility implies that reliable and comprehensive software platforms and tools should be made available to the scientific community. The tools will empower them and the public to verify, through practice, the reproducibility of observations that are reported in the scientific literature. Medical image analysis is one of the fields in which the use of computational resources, both software and hardware, are an essential platform for performing experimental work. In this arena, the introduction of the Insight Toolkit (ITK) in 1999 has transformed the field and facilitates its progress by accelerating the rate at which algorithmic implementations are developed, tested, disseminated and improved. By building on the efficiency and quality of open source methodologies, ITK has provided the medical image community with an effective platform on which to build a daily workflow that incorporates the true scientific practices of reproducibility verification. This article describes the multiple tools, methodologies, and practices that the ITK community has adopted, refined, and followed during the past decade, in order to become one of the research communities with the most modern reproducibility verification infrastructure. For example, 207 contributors have created over 2400 unit tests that provide over 84% code line test coverage. The Insight Journal, an open publication journal associated with the toolkit, has seen over 360,000 publication downloads. The median normalized closeness centrality, a measure of knowledge flow, resulting from the distributed peer code review system was high, 0.46. {\textcopyright} 2014 McCormick, Liu, Jomier, Marion and Ibanez.},
  doi      = {10.3389/fninf.2014.00013},
  keywords = {Code review,Insight journal,Insight toolkit,Itk,Open science,Reproducibility},
  url      = {http://journal.frontiersin.org/article/10.3389/fninf.2014.00013/abstract},
}

@InProceedings{art/DongN_2019,
  author    = {Dong, Nanqing and Xing, Eric P.},
  booktitle = {British Machine Vision Conference 2018, BMVC 2018},
  title     = {{Few-shot semantic segmentation with prototype learning}},
  year      = {2019},
  abstract  = {Semantic segmentation assigns a class label to each image pixel. This dense prediction problem requires large amounts of manually annotated data, which is often unavailable. Few-shot learning aims to learn the pattern of a new category with only a few annotated examples. In this paper, we formulate the few-shot semantic segmentation problem from 1-way (class) to N-way (classes). Inspired by few-shot classification, we propose a generalized framework for few-shot semantic segmentation with an alternative training scheme. The framework is based on prototype learning and metric learning. Our approach outperforms the baselines by a large margin and shows comparable performance for 1-way few-shot semantic segmentation on PASCAL VOC 2012 dataset.},
}

@Article{art/AndrychowiczM_2016,
  author        = {Andrychowicz, Marcin and Denil, Misha and Colmenarejo, Sergio G{\'{o}}mez and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {De Freitas}, Nando},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {{Learning to learn by gradient descent by gradient descent}},
  year          = {2016},
  issn          = {10495258},
  pages         = {3988--3996},
  abstract      = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  arxivid       = {1606.04474},
  eprint        = {1606.04474},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient descent.pdf:pdf},
}

@Article{art/HuJ_202009,
  author        = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Squeeze-and-Excitation Networks}},
  year          = {2020},
  issn          = {19393539},
  month         = {sep},
  number        = {8},
  pages         = {2011--2023},
  volume        = {42},
  abstract      = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of ${\sim }$∼25 percent. Models and code are available at https://github.com/hujie-frank/SENet.},
  archiveprefix = {arXiv},
  arxivid       = {1709.01507},
  doi           = {10.1109/TPAMI.2019.2913372},
  eprint        = {1709.01507},
  keywords      = {Squeeze-and-excitation,attention,convolutional neural networks,image representations},
  pmid          = {31034408},
  url           = {http://arxiv.org/abs/1709.01507},
}

@InProceedings{art/ChenX_2019,
  author    = {Chen, Xu and Williams, Bryan M. and Vallabhaneni, Srinivasa R. and Czanner, Gabriela and Williams, Rachel and Zheng, Yalin},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title     = {{Learning active contour models for medical image segmentation}},
  year      = {2019},
  pages     = {11624--11632},
  volume    = {2019-June},
  abstract  = {Image segmentation is an important step in medical image processing and has been widely studied and developed for refinement of clinical analysis and applications. New models based on deep learning have improved results but are restricted to pixel-wise fitting of the segmentation map. Our aim was to tackle this limitation by developing a new model based on deep learning which takes into account the area inside as well as outside the region of interest as well as the size of boundaries during learning. Specically, we propose a new loss function which incorporates area and size information and integrates this into a dense deep learning model. We evaluated our approach on a dataset of more than 2,000 cardiac MRI scans. Our results show that the proposed loss function outperforms other mainstream loss function Cross-entropy on two common segmentation networks. Our loss function is robust while using different hyperparameter lambda.},
  doi       = {10.1109/CVPR.2019.01190},
  isbn      = {9781728132938},
  issn      = {10636919},
  keywords  = {Deep Learning,Grouping and Shape,Segmentation},
}

@InProceedings{art/SmailagicA_2019,
  author    = {Smailagic, Asim and Costa, Pedro and {Young Noh}, Hae and Walawalkar, Devesh and Khandelwal, Kartik and Galdran, Adrian and Mirshekari, Mostafa and Fagert, Jonathon and Xu, Susu and Zhang, Pei and Campilho, Aurelio},
  booktitle = {Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018},
  title     = {{MedAL: Accurate and Robust Deep Active Learning for Medical Image Analysis}},
  year      = {2019},
  pages     = {481--488},
  abstract  = {Deep learning models have been successfully used in medical image analysis problems but they require a large amount of labeled images to obtain good performance. However, such large labeled datasets are costly to acquire. Active learning techniques can be used to minimize the number of required training labels while maximizing the model's performance. In this work, we propose a novel sampling method that queries the unlabeled examples that maximize the average distance to all training set examples in a learned feature space. We then extend our sampling method to define a better initial training set, without the need for a trained model, by using Oriented FAST and Rotated BRIEF (ORB) feature descriptors. We validate MedAL on 3 medical image datasets and show that our method is robust to different dataset properties. MedAL is also efficient, achieving 80% accuracy on the task of Diabetic Retinopathy detection using only 425 labeled images, corresponding to a 32% reduction in the number of required labeled examples compared to the standard uncertainty sampling technique, and a 40% reduction compared to random sampling.},
  doi       = {10.1109/ICMLA.2018.00078},
  isbn      = {9781538668047},
  keywords  = {Active Learning,Deep Learning,Medical Imaging},
}

@Misc{art/BuddS_2021,
  author        = {Budd, Samuel and Robinson, Emma C. and Kainz, Bernhard},
  title         = {{A survey on active learning and human-in-the-loop deep learning for medical image analysis}},
  year          = {2021},
  abstract      = {Fully automatic deep learning has become the state-of-the-art technique for many tasks including image acquisition, analysis and interpretation, and for the extraction of clinically useful information for computer-aided detection, diagnosis, treatment planning, intervention and therapy. However, the unique challenges posed by medical image analysis suggest that retaining a human end-user in any deep learning enabled system will be beneficial. In this review we investigate the role that humans might play in the development and deployment of deep learning enabled diagnostic applications and focus on techniques that will retain a significant input from a human end user. Human-in-the-Loop computing is an area that we see as increasingly important in future research due to the safety-critical nature of working in the medical domain. We evaluate four key areas that we consider vital for deep learning in the clinical practice: (1) Active Learning to choose the best data to annotate for optimal model performance; (2) Interaction with model outputs - using iterative feedback to steer models to optima for a given prediction and offering meaningful ways to interpret and respond to predictions; (3) Practical considerations - developing full scale applications and the key considerations that need to be made before deployment; (4) Future Prospective and Unanswered Questions - knowledge gaps and related research fields that will benefit human-in-the-loop computing as they evolve. We offer our opinions on the most promising directions of research and how various aspects of each area might be unified towards common goals.},
  archiveprefix = {arXiv},
  arxivid       = {1910.02923},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2021.102062},
  eprint        = {1910.02923},
  issn          = {13618423},
  keywords      = {Active learning,Deep Learning,Human-in-the-Loop,Medical image analysis},
  pmid          = {33901992},
  volume        = {71},
}

@Article{art/NathV_2020,
  author   = {Nath, Vishwesh and Yang, Dong and Landman, Bennett A. and Xu, Daguang and Roth, Holger R.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Diminishing Uncertainty within the Training Pool: Active Learning for Medical Image Segmentation}},
  year     = {2020},
  issn     = {1558254X},
  abstract = {Active learning is a unique abstraction of machine learning techniques where the model/algorithm could guide users for annotation of a set of data points that would be beneficial to the model, unlike passive machine learning. The primary advantage being that active learning frameworks select data points that can accelerate the learning process of amodel and can reduce the amount of data needed to achieve full accuracy as compared to a model trained on a randomly acquired data set. Multiple frameworks for active learning combined with deep learning have been proposed, and the majority of them are dedicated to classification tasks. Herein, we explore active learning for the task of segmentation of medical imaging data sets. We investigate our proposed framework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans of pancreas and tumors. This work presents a query-by-committee approach for active learning where a joint optimizer is used for the committee. At the same time, we propose three new strategies for active learning: 1.) increasing frequency of uncertain data to bias the training data set; 2.) Using mutual information among the input images as a regularizer for acquisition to ensure diversity in the training dataset; 3.) adaptation of Dice log-likelihood for Stein variational gradient descent (SVGD). The results indicate an improvement in terms of data reduction by achieving full accuracy while only using 22.69 % and 48.85 % of the available data for each dataset, respectively.},
  doi      = {10.1109/TMI.2020.3048055},
  keywords = {Active Learning,Biomedical imaging,Deep Learning,Deep learning,Hippocampus,Image segmentation,Medical Imaging,Mutual information,SVGD,Segmentation,Task analysis,Training},
}

@Article{art/LeeC_2020,
  author   = {Lee, Changhwan and Jang, Jongseong and Lee, Seunghun and Kim, Young Soo and Jo, Hang Joon and Kim, Yeesuk},
  journal  = {Scientific Reports},
  title    = {{Classification of femur fracture in pelvic X-ray images using meta-learned deep neural network}},
  year     = {2020},
  issn     = {20452322},
  number   = {1},
  volume   = {10},
  abstract = {In the medical field, various studies using artificial intelligence (AI) techniques have been attempted. Numerous attempts have been made to diagnose and classify diseases using image data. However, different forms of fracture exist, and inaccurate results have been confirmed depending on condition at the time of imaging, which is problematic. To overcome this limitation, we present an encoder-decoder structured neural network that utilizes radiology reports as ancillary information at training. This is a type of meta-learning method used to generate sufficiently adequate features for classification. The proposed model learns representation for classification from X-ray images and radiology reports simultaneously. When using a dataset of only 459 cases for algorithm training, the model achieved a favorable performance in a test dataset containing 227 cases (classification accuracy of 86.78% and classification F1 score of 0.867 for fracture or normal classification). This finding demonstrates the potential for deep learning to improve performance and accelerate application of AI in clinical practice.},
  doi      = {10.1038/s41598-020-70660-4},
  pmid     = {32792627},
}

@InProceedings{art/TanC_2020,
  author        = {Tan, Chuan and Zhu, Jin and Lio', Pietro},
  booktitle     = {IFIP Advances in Information and Communication Technology},
  title         = {{Arbitrary scale super-resolution for brain MRI images}},
  year          = {2020},
  pages         = {165--176},
  volume        = {583 IFIP},
  abstract      = {Recent attempts at Super-Resolution for medical images used deep learning techniques such as Generative Adversarial Networks (GANs) to achieve perceptually realistic single image Super-Resolution. Yet, they are constrained by their inability to generalise to different scale factors. This involves high storage and energy costs as every integer scale factor involves a separate neural network. A recent paper has proposed a novel meta-learning technique that uses a Weight Prediction Network to enable Super-Resolution on arbitrary scale factors using only a single neural network. In this paper, we propose a new network that combines that technique with SRGAN, a state-of-the-art GAN-based architecture, to achieve arbitrary scale, high fidelity Super-Resolution for medical images. By using this network to perform arbitrary scale magnifications on images from the Multimodal Brain Tumor Segmentation Challenge (BraTS) dataset, we demonstrate that it is able to outperform traditional interpolation methods by up to 20% on SSIM scores whilst retaining generalisability on brain MRI images. We show that performance across scales is not compromised, and that it is able to achieve competitive results with other state-of-the-art methods such as EDSR whilst being fifty times smaller than them. Combining efficiency, performance, and generalisability, this can hopefully become a new foundation for tackling Super-Resolution on medical images.},
  archiveprefix = {arXiv},
  arxivid       = {2004.02086},
  doi           = {10.1007/978-3-030-49161-1_15},
  eprint        = {2004.02086},
  isbn          = {9783030491604},
  issn          = {1868422X},
  keywords      = {Image processing,Medical image analysis,Meta-Learning,Super-Resolution},
}

@Article{art/SinghR_2021,
  author   = {Singh, Rishav and Bharti, Vandana and Purohit, Vishal and Kumar, Abhinav and Singh, Amit Kumar and Singh, Sanjay Kumar},
  journal  = {Pattern Recognition},
  title    = {{MetaMed: Few-shot medical image classification using gradient-based meta-learning}},
  year     = {2021},
  issn     = {00313203},
  pages    = {108111},
  abstract = {The occurrence of long-tailed distributions and unavailability of high-quality annotated images is a common phenomenon in medical datasets. The use of conventional Deep Learning techniques to obtain an unbiased model with high generalization accuracy for such datasets is a challenging task. Thus, we formulated a few-shot learning problem and presented a meta-learning-based “MetaMed” approach. The model presented here can adapt to rare disease classes with the availability of few images, and less compute. MetaMed is validated on three publicly accessible medical datasets – Pap smear, BreakHis, and ISIC 2018. We used advanced image augmentation techniques like CutOut, MixUp, and CutMix to overcome the problem of over-fitting. Our approach has shown promising results on all the three datasets with an accuracy of more than 70%. Inclusion of advanced augmentation techniques regularizes the model and increases the generalization capability by  2-5%. Comparative analysis of MetaMed against transfer learning demonstrated that MetaMed classifies images with a higher confidence score and on average outperforms transfer learning for 3, 5, and 10-shot tasks for both 2-way and 3-way classification.},
  doi      = {10.1016/j.patcog.2021.108111},
}

@Article{art/ZhangP_202102,
  author    = {Zhang, Penghao and Li, Jiayue and Wang, Yining and Pan, Judong},
  journal   = {Journal of Imaging},
  title     = {{Domain adaptation for medical image segmentation: A meta-learning method}},
  year      = {2021},
  issn      = {2313433X},
  month     = {feb},
  number    = {2},
  volume    = {7},
  abstract  = {Convolutional neural networks (CNNs) have demonstrated great achievement in increasing the accuracy and stability of medical image segmentation. However, existing CNNs are limited by the problem of dependency on the availability of training data owing to high manual annotation costs and privacy issues. To counter this limitation, domain adaptation (DA) and few-shot learning have been extensively studied. Inspired by these two categories of approaches, we propose an optimizationbased meta-learning method for segmentation tasks. Even though existing meta-learning methods use prior knowledge to choose parameters that generalize well from few examples, these methods limit the diversity of the task distribution that they can learn from in medical image segmentation. In this paper, we propose a meta-learning algorithm to augment the existing algorithms with the capability to learn from diverse segmentation tasks across the entire task distribution. Specifically, our algorithm aims to learn from the diversity of image features which characterize a specific tissue type while showing diverse signal intensities. To demonstrate the effectiveness of the proposed algorithm, we conducted experiments using a diverse set of segmentation tasks from the Medical Segmentation Decathlon and two meta-learning benchmarks: model-agnostic meta-learning (MAML) and Reptile. U-Net and Dice similarity coefficient (DSC) were selected as the baseline model and the main performance metric, respectively. The experimental results show that our algorithm maximally surpasses MAML and Reptile by 2% and 2.4% respectively, in terms of the DSC. By showing a consistent improvement in subjective measures, we can also infer that our algorithm can produce a better generalization of a target task that has few examples.},
  doi       = {10.3390/jimaging7020031},
  keywords  = {Domain adaptation,Medical image segmentation,Meta-learning,U-Net},
  publisher = {MDPI AG},
}

@Article{art/KrizhevskyA_2009,
  author   = {Krizhevsky, Alex and Hinton, G},
  journal  = {{\ldots} Science Department, University of Toronto, Tech. {\ldots}},
  title    = {{Learning multiple layers of features from tiny images.(2009)}},
  year     = {2009},
  issn     = {1098-6596},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels.},
  doi      = {10.1.1.222.9220},
  url      = {https://www.cs.toronto.edu/$\sim$kriz/cifar.html},
}

@Article{art/FengH_2007,
  author   = {Feng, Hanhua and Misra, Vishal and Rubenstein, Dan},
  journal  = {Electrical Engineering},
  title    = {{The CIFAR-10 dataset}},
  year     = {2007},
  issn     = {01635999},
  number   = {1},
  volume   = {35},
  abstract = {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.},
}

@Misc{art/KrizhevskyA_2009a,
  author    = {Krizhevsky, A and Nair, V and Hinton, G},
  title     = {{CIFAR-10 and CIFAR-100 datasets}},
  year      = {2009},
  abstract  = {The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset},
  booktitle = {https://www.cs.toronto.edu/$\sim$kriz/cifar.html},
}

@InProceedings{art/MishraN_2018,
  author        = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  booktitle     = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
  title         = {{A simple neural attentive meta-learner}},
  year          = {2018},
  abstract      = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
  archiveprefix = {arXiv},
  arxivid       = {1707.03141},
  eprint        = {1707.03141},
}

@InProceedings{art/SantoroA_2016,
  author    = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle = {33rd International Conference on Machine Learning, ICML 2016},
  title     = {{Meta-Learning with Memory-Augmented Neural Networks}},
  year      = {2016},
  volume    = {4},
  abstract  = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently releam their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  doi       = {10.5555/3045390.3045585},
}

@InProceedings{art/SungF_2018,
  author        = {Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip H.S. and Hospedales, Timothy M.},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Learning to Compare: Relation Network for Few-Shot Learning}},
  year          = {2018},
  pages         = {1199--1208},
  abstract      = {We present a conceptually simple, flexible, and general framework for few-shot learning, where a classifier must learn to recognise new classes given only few examples from each. Our method, called the Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Once trained, a RN is able to classify images of new classes by computing relation scores between query images and the few examples of each new class without further updating the network. Besides providing improved performance on few-shot learning, our framework is easily extended to zero-shot learning. Extensive experiments on five benchmarks demonstrate that our simple approach provides a unified and effective approach for both of these two tasks.},
  archiveprefix = {arXiv},
  arxivid       = {1711.06025},
  doi           = {10.1109/CVPR.2018.00131},
  eprint        = {1711.06025},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@InProceedings{art/RusuA_2019,
  author        = {Rusu, Andrei A. and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019},
  title         = {{Meta-learning with latent embedding optimization}},
  year          = {2019},
  abstract      = {Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.},
  archiveprefix = {arXiv},
  arxivid       = {1807.05960},
  eprint        = {1807.05960},
}

@InProceedings{art/WangX_2019,
  author        = {Wang, Xin and Yu, Fisher and Wang, Ruth and Darrell, Trevor and Gonzalez, Joseph E.},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{TAFE-Net: Task-aware feature embeddings for low shot learning}},
  year          = {2019},
  pages         = {1831--1840},
  volume        = {2019-June},
  abstract      = {Learning good feature embeddings for images often requires substantial training data. As a consequence, in settings where training data is limited (e.g., few-shot and zero-shot learning), we are typically forced to use a general feature embedding across prediction tasks. Ideally, we would like to construct feature embeddings that are tuned for the given task and even input image. In this work, we propose Task Aware Feature Embedding Networks (TAFE-Nets) to learn how to adapt the image representation to a new task in a meta learning fashion. Our network is composed of a meta learner and a prediction network, where the meta learner generates parameters for the feature layers in the prediction network based on a task input so that the feature embedding can be accurately adjusted for that task. We show that TAFE-Net is highly effective in generalizing to new tasks or concepts and evaluate the TAFE-Net on a range of benchmarks in zero-shot and few-shot learning. Our model matches or exceeds the state-of-the-art on all tasks. In particular, our approach improves the prediction accuracy of unseen attribute-object pairs by 4 to 15 points on the challenging visual attribute-object composition task.},
  archiveprefix = {arXiv},
  arxivid       = {1904.05967},
  doi           = {10.1109/CVPR.2019.00193},
  eprint        = {1904.05967},
  isbn          = {9781728132938},
  issn          = {10636919},
  keywords      = {Categorization,Recognition: Detection,Representation Learning,Retrieval},
}

@InProceedings{art/QiH_2018,
  author        = {Qi, Hang and Brown, Matthew and Lowe, David G.},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Low-Shot Learning with Imprinted Weights}},
  year          = {2018},
  pages         = {5822--5830},
  abstract      = {Human vision is able to immediately recognize novel visual categories after seeing just one or a few training examples. We describe how to add a similar capability to ConvNet classifiers by directly setting the final layer weights from novel training examples during low-shot learning. We call this process weight imprinting as it directly sets weights for a new category based on an appropriately scaled copy of the embedding layer activations for that training example. The imprinting process provides a valuable complement to training with stochastic gradient descent, as it provides immediate good classification performance and an initialization for any further fine-tuning in the future. We show how this imprinting process is related to proxy-based embeddings. However, it differs in that only a single imprinted weight vector is learned for each novel category, rather than relying on a nearest-neighbor distance to training instances as typically used with embedding methods. Our experiments show that using averaging of imprinted weights provides better generalization than using nearest-neighbor instance embeddings.},
  archiveprefix = {arXiv},
  arxivid       = {1712.07136},
  doi           = {10.1109/CVPR.2018.00610},
  eprint        = {1712.07136},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@InProceedings{art/GidarisS_2018,
  author        = {Gidaris, Spyros and Komodakis, Nikos},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Dynamic Few-Shot Visual Learning Without Forgetting}},
  year          = {2018},
  pages         = {4367--4375},
  abstract      = {The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on 'unseen' categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results.},
  archiveprefix = {arXiv},
  arxivid       = {1804.09458},
  doi           = {10.1109/CVPR.2018.00459},
  eprint        = {1804.09458},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@InProceedings{art/JamalM_2019,
  author        = {Jamal, Muhammad Abdullah and Qi, Guo Jun},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Task agnostic meta-learning for few-shot learning}},
  year          = {2019},
  pages         = {11711--11719},
  volume        = {2019-June},
  abstract      = {Meta-learning approaches have been proposed to tackle the few-shot learning problem. Typically, a meta-learner is trained on a variety of tasks in the hopes of being generalizable to new tasks. However, the generalizability on new tasks of a meta-learner could be fragile when it is over-trained on existing tasks during meta-training phase. In other words, the initial model of a meta-learner could be too biased towards existing tasks to adapt to new tasks, especially when only very few examples are available to update the model. To avoid a biased meta-learner and improve its generalizability, we propose a novel paradigm of Task-Agnostic Meta-Learning (TAML) algorithms. Specifically, we present an entropy-based approach that meta-learns an unbiased initial model with the largest uncertainty over the output labels by preventing it from over-performing in classification tasks. Alternatively, a more general inequality-minimization TAML is presented for more ubiquitous scenarios by directly minimizing the inequality of initial losses beyond the classification tasks wherever a suitable loss can be defined. Experiments on benchmarked datasets demonstrate that the proposed approaches outperform compared meta-learning algorithms in both few-shot classification and reinforcement learning tasks.},
  archiveprefix = {arXiv},
  arxivid       = {1805.07722},
  doi           = {10.1109/CVPR.2019.01199},
  eprint        = {1805.07722},
  isbn          = {9781728132938},
  issn          = {10636919},
  keywords      = {Deep Learning,Statistical Learning},
}

@Article{art/ZhangL_2020,
  author   = {Zhang, Lamei and Zhang, Siyu and Zou, Bin and Dong, Hongwei},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  title    = {{Unsupervised Deep Representation Learning and Few-Shot Classification of PolSAR Images}},
  year     = {2020},
  issn     = {15580644},
  abstract = {Deep learning and convolutional neural networks (CNNs) have made progress in polarimetric synthetic aperture radar (PolSAR) image classification over the past few years. However, a crucial issue has not been addressed, i.e., the requirement of CNNs for abundant labeled samples versus the insufficient human annotations of PolSAR images. It is well known that following the supervised learning paradigm may lead to the overfitting of training data, and the lack of supervision information of PolSAR images undoubtedly aggravates this problem, which greatly affects the generalization performance of CNN-based classifiers in large-scale applications. To handle this problem, in this article, learning transferrable representations from unlabeled PolSAR data through convolutional architectures is explored for the first time. Specifically, a PolSAR-tailored contrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR representation learning and few-shot classification. Different from the utilization of optical processing methods, a diversity stimulation mechanism is constructed to narrow the application gap between optics and PolSAR. Beyond the conventional supervised methods, PCLNet develops an unsupervised pretraining phase based on the proxy objective of instance discrimination to learn useful representations from unlabeled PolSAR data. The acquired representations are transferred to the downstream task, i.e., few-shot PolSAR classification. Experiments on two widely used PolSAR benchmark data sets confirm the validity of PCLNet. Besides, this work may enlighten how to efficiently utilize the massive unlabeled PolSAR data to alleviate the greedy demands of CNN-based methods for human annotations.},
  doi      = {10.1109/TGRS.2020.3043191},
  keywords = {Annotations,Contrastive learning (CL),Neural networks,Optical imaging,Optical sensors,Task analysis,Training,Training data,few-shot learning,polarimetric synthetic aperture radar (PolSAR) ima,unsupervised representation learning.},
}

@Article{art/HuY_2020,
  author        = {Hu, Yuqing and Gripon, Vincent and Pateux, St{\'{e}}phane},
  journal       = {arXiv},
  title         = {{Exploiting Unsupervised Inputs for Accurate Few-Shot Classification}},
  year          = {2020},
  issn          = {23318422},
  abstract      = {In few-shot classification, the aim is to learn models able to
discriminate classes with only a small number of labelled examples. Most of the literature considers the problem of labelling a single unknown input at a time. Instead, it can be beneficial to consider a setting where a batch of unlabelled inputs are treated conjointly and non-independently. In this vein, we propose a method able to exploit three levels of information: a) feature extractors pretrained on generic datasets, b) few labelled examples of classes to discriminate and c) other available unlabelled inputs. If for a), we use state-of-the-art approaches, we introduce the use of simplified graph convolutions to perform b) and c) together. Our proposed model reaches state-of-the-art accuracy with a 6 − 11% increase compared to available alternatives on standard few-shot vision classification datasets.},
  archiveprefix = {arXiv},
  arxivid       = {2001.09849},
  eprint        = {2001.09849},
  keywords      = {Few-Shot Learning,Graph Convolutions,Semi-supervised Learning,Transfer},
}

@Article{art/XuH_2021,
  author   = {Xu, Hui and Wang, Jiaxing and Li, Hao and Ouyang, Deqiang and Shao, Jie},
  journal  = {Pattern Recognition},
  title    = {{Unsupervised meta-learning for few-shot learning}},
  year     = {2021},
  issn     = {00313203},
  volume   = {116},
  abstract = {Meta-learning is an effective tool to address the few-shot learning problem, which requires new data to be classified considering only a few training examples. However, when used for classification, it requires large labeled datasets, which are not always available in practice. In this paper, we propose an unsupervised meta-learning algorithm that learns from an unlabeled dataset and adapts to downstream human-specific tasks with few labeled data. The proposed algorithm constructs tasks using clustering embedding methods and data augmentation functions to satisfy two critical class distinction requirements. To alleviate the biases and the weak diversity problem introduced by data augmentation functions, the proposed algorithm uses two methods, which are shifting the feeding data between the inner-outer loops and a novel data augmentation function. We further provide theoretical analysis of the effect of augmentation data in the inner/outer loop. Experiments on the MiniImagenet and Omniglot datasets demonstrate that the proposed unsupervised meta-learning approach outperforms other tested unsupervised representation learning approaches and two recent unsupervised meta-learning baselines. Compared with supervised meta-learning approaches, certain results produced by our method are quite close to those produced by such methods trained on the human-designed labeled tasks.},
  doi      = {10.1016/j.patcog.2021.107951},
  keywords = {Few-shot learning,Meta-learning,Unsupervised learning},
}

@InProceedings{art/LiuM_2019,
  author        = {Liu, Ming Yu and Huang, Xun and Mallya, Arun and Karras, Tero and Aila, Timo and Lehtinen, Jaakko and Kautz, Jan},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Few-shot unsupervised image-to-image translation}},
  year          = {2019},
  pages         = {10550--10559},
  volume        = {2019-Octob},
  abstract      = {Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT.},
  archiveprefix = {arXiv},
  arxivid       = {1905.01723},
  doi           = {10.1109/ICCV.2019.01065},
  eprint        = {1905.01723},
  isbn          = {9781728148038},
  issn          = {15505499},
}

@Article{art/ChenZ_2018,
  author        = {Chen, Zitian and Fu, Yanwei and Zhang, Yinda and Jiang, Yu-Gang and Xue, Xiangyang and Sigal, Leonid},
  journal       = {Eccv},
  title         = {{Semantic Feature Augmentation in Few-shot Learning}},
  year          = {2018},
  pages         = {1--21},
  volume        = {abs/1804.0},
  abstract      = {A fundamental problem with few-shot learning is the scarcity of data in training. A natural solution to alleviate this scarcity is to augment the existing images for each training class. However, directly augmenting samples in image space may not necessarily, nor sufficiently, explore the intra-class variation. To this end, we propose to directly synthesize instance features by leveraging the semantics of each class. Essentially, a novel auto-encoder network dual TriNet, is proposed for feature augmentation. The encoder TriNet projects multi-layer visual features of deep CNNs into the semantic space. In this space, data augmentation is induced, and the augmented instance representation is projected back into the image feature spaces by the decoder TriNet. Two data argumentation strategies in the semantic space are explored; notably these seemingly simple augmentations in semantic space result in complex augmented feature distributions in the image feature space, resulting in substantially better performance. The code and models of our paper will be published on: https://github.com/tankche1/Semantic-Feature-Augmentation-in-Few-shot-Learning.},
  archiveprefix = {arXiv},
  arxivid       = {1804.05298},
  eprint        = {1804.05298},
  url           = {https://arxiv.org/pdf/1804.05298.pdf},
}

@Article{art/LeeT_2021,
  author   = {Lee, Taemin and Yoo, Sungjoo},
  journal  = {IEEE Access},
  title    = {{Augmenting Few-Shot Learning with Supervised Contrastive Learning}},
  year     = {2021},
  issn     = {21693536},
  pages    = {61466--61474},
  volume   = {9},
  abstract = {Few-shot learning deals with a small amount of data which incurs insufficient performance with conventional cross-entropy loss. We propose a pretraining approach for few-shot learning scenarios. That is, considering that the feature extractor quality is a critical factor in few-shot learning, we augment the feature extractor using a contrastive learning technique. It is reported that supervised contrastive learning applied to base class training in transductive few-shot training pipeline leads to improved results, outperforming the state-of-the-art methods on Mini-ImageNet and CUB. Furthermore, our experiment shows that a much larger dataset is needed to retain few-shot classification accuracy when domain-shift degradation exists, and if our method is applied, the need for a large dataset is eliminated. The accuracy gain can be translated to a runtime reduction of 3.87× in a resource-constrained environment.},
  doi      = {10.1109/ACCESS.2021.3074525},
  keywords = {Few-shot learning,contrastive learning,information maximization},
}

@Article{art/TsengH_2020,
  author   = {Tseng, Hung-Yu},
  journal  = {Iclr},
  title    = {{Cross-Domain Few-Shot Classification}},
  year     = {2020},
  abstract = {Few-shot classification aims to recognize novel categories with only few labeled images in each class. Existing metric-based few-shot classification algorithms pre- dict categories by comparing the feature embeddings of query images with those from a few labeled images (support examples) using a learned metric function. While promising performance has been demonstrated, these methods often fail to generalize to unseen domains due to large discrepancy of the feature distribution across domains. In this work, we address the problem of few-shot classification under domain shifts for metric-based methods. Our core idea is to use feature-wise transformation layers for augmenting the image features using affine transforms to simulate various feature distributions under different domains in the training stage. To capture variations of the feature distributions under different domains, we further apply a learning-to-learn approach to search for the hyper-parameters of the feature-wise transformation layers. We conduct extensive experiments and ablation studies under the domain generalization setting using five few-shot clas- sification datasets: mini-ImageNet, CUB, Cars, Places, and Plantae. Experimental results demonstrate that the proposed feature-wise transformation layer is appli- cable to various metric-based models, and provides consistent improvements on the few-shot classification performance under domain shift. 1},
}

@InProceedings{art/KangB_2019,
  author        = {Kang, Bingyi and Liu, Zhuang and Wang, Xin and Yu, Fisher and Feng, Jiashi and Darrell, Trevor},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Few-shot object detection via feature reweighting}},
  year          = {2019},
  pages         = {8419--8428},
  volume        = {2019-Octob},
  abstract      = {Conventional training of a deep CNN based object detector demands a large number of bounding box annotations, which may be unavailable for rare categories. In this work we develop a few-shot object detector that can learn to detect novel objects from only a few annotated examples. Our proposed model leverages fully labeled base classes and quickly adapts to novel classes, using a meta feature learner and a reweighting module within a one-stage detection architecture. The feature learner extracts meta features that are generalizable to detect novel object classes, using training data from base classes with sufficient samples. The reweighting module transforms a few support examples from the novel classes to a global vector that indicates the importance or relevance of meta features for detecting the corresponding objects. These two modules, together with a detection prediction module, are trained end-to-end based on an episodic few-shot learning scheme and a carefully designed loss function. Through extensive experiments we demonstrate that our model outperforms well-established baselines by a large margin for few-shot object detection, on multiple datasets and settings. We also present analysis on various aspects of our proposed model, aiming to provide some inspiration for future few-shot detection works.},
  archiveprefix = {arXiv},
  arxivid       = {1812.01866},
  doi           = {10.1109/ICCV.2019.00851},
  eprint        = {1812.01866},
  isbn          = {9781728148038},
  issn          = {15505499},
}

@Article{art/HuismanM_2021,
  author        = {Huisman, Mike and van Rijn, Jan N. and Plaat, Aske},
  journal       = {Artificial Intelligence Review},
  title         = {{A survey of deep meta-learning}},
  year          = {2021},
  issn          = {15737462},
  abstract      = {Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, in-depth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into (i) metric-, (ii) model-, and (iii) optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.},
  archiveprefix = {arXiv},
  arxivid       = {2010.03522},
  doi           = {10.1007/s10462-021-10004-4},
  eprint        = {2010.03522},
  keywords      = {Deep learning,Few-shot learning,Learning to learn,Meta-learning,Transfer learning},
}

@Article{art/HospedalesT_2021,
  author        = {Hospedales, Timothy M. and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos J.},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Meta-Learning in Neural Networks: A Survey}},
  year          = {2021},
  issn          = {19393539},
  abstract      = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  arxivid       = {2004.05439},
  doi           = {10.1109/TPAMI.2021.3079209},
  eprint        = {2004.05439},
  keywords      = {Deep learning,Few-Shot Learning,Learning-to-Learn,Machine learning algorithms,Meta-Learning,Neural Architecture Search,Neural networks,Optimization,Predictive models,Task analysis,Training,Transfer Learning},
}

@Article{art/KhojastehP_2019,
  author   = {Khojasteh, Parham and {Passos J{\'{u}}nior}, Leandro Aparecido and Carvalho, Tiago and Rezende, Edmar and Aliahmad, Behzad and Papa, Jo{\~{a}}o Paulo and Kumar, Dinesh Kant},
  journal  = {Computers in Biology and Medicine},
  title    = {{Exudate detection in fundus images using deeply-learnable features}},
  year     = {2019},
  issn     = {18790534},
  pages    = {62--69},
  volume   = {104},
  abstract = {Presence of exudates on a retina is an early sign of diabetic retinopathy, and automatic detection of these can improve the diagnosis of the disease. Convolutional Neural Networks (CNNs) have been used for automatic exudate detection, but with poor performance. This study has investigated different deep learning techniques to maximize the sensitivity and specificity. We have compared multiple deep learning methods, and both supervised and unsupervised classifiers for improving the performance of automatic exudate detection, i.e., CNNs, pre-trained Residual Networks (ResNet-50) and Discriminative Restricted Boltzmann Machines. The experiments were conducted on two publicly available databases: (i) DIARETDB1 and (ii) e-Ophtha. The results show that ResNet-50 with Support Vector Machines outperformed other networks with an accuracy and sensitivity of 98% and 0.99, respectively. This shows that ResNet-50 can be used for the analysis of the fundus images to detect exudates.},
  doi      = {10.1016/j.compbiomed.2018.10.031},
  keywords = {Convolutional neural networks,Deep learning,Deep residual networks,Diabetic retinopathy,Discriminative restricted Boltzmann machines,Exudate detection},
  pmid     = {30439600},
}

@Article{art/AbramoffM_2016,
  author   = {Abr{\`{a}}moff, Michael David and Lou, Yiyue and Erginay, Ali and Clarida, Warren and Amelon, Ryan and Folk, James C. and Niemeijer, Meindert},
  journal  = {Investigative Ophthalmology and Visual Science},
  title    = {{Improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning}},
  year     = {2016},
  issn     = {15525783},
  number   = {13},
  pages    = {5200--5206},
  volume   = {57},
  abstract = {Purpose: To compare performance of a deep-learning enhanced algorithm for automated detection of diabetic retinopathy (DR), to the previously published performance of that algorithm, the Iowa Detection Program (IDP)-without deep learning components-on the same publicly available set of fundus images and previously reported consensus reference standard set, by three US Board certified retinal specialists. Methods: We used the previously reported consensus reference standard of referable DR (rDR), defined as International Clinical Classification of Diabetic Retinopathy moderate, severe nonproliferative (NPDR), proliferative DR, and/or macular edema (ME). Neither Messidor-2 images, nor the three retinal specialists setting the Messidor-2 reference standard were used for training IDx-DR version X2.1. Sensitivity, specificity, negative predictive value, area under the curve (AUC), and their confidence intervals (CIs) were calculated. Results: Sensitivity was 96.8% (95% CI: 93.3%-98.8%), specificity was 87.0% (95% CI: 84.2%-89.4%), with 6/874 false negatives, resulting in a negative predictive value of 99.0% (95% CI: 97.8%-99.6%). No cases of severe NPDR, PDR, or ME were missed. The AUC was 0.980 (95% CI: 0.968-0.992). Sensitivity was not statistically different from published IDP sensitivity, which had a CI of 94.4% to 99.3%, but specificity was significantly better than the published IDP specificity CI of 55.7% to 63.0%. Conclusions: A deep-learning enhanced algorithm for the automated detection of DR, achieves significantly better performance than a previously reported, otherwise essentially identical, algorithm that does not employ deep learning. Deep learning enhanced algorithms have the potential to improve the efficiency of DR screening, and thereby to prevent visual loss and blindness from this devastating disease.},
  doi      = {10.1167/iovs.16-19964},
  keywords = {Algorithm,Deep learning,Detection,Diabetes,Diabetic retinopathy},
  pmid     = {27701631},
}

@Article{art/LiF_2019,
  author   = {Li, Feng and Chen, Hua and Liu, Zheng and Zhang, Xuedian and Wu, Zhizheng},
  journal  = {Graefe's Archive for Clinical and Experimental Ophthalmology},
  title    = {{Fully automated detection of retinal disorders by image-based deep learning}},
  year     = {2019},
  issn     = {1435702X},
  number   = {3},
  pages    = {495--505},
  volume   = {257},
  abstract = {Purpose: With the aging population and the global diabetes epidemic, the prevalence of age-related macular degeneration (AMD) and diabetic macular edema (DME) diseases which are the leading causes of blindness is further increasing. Intravitreal injections with anti-vascular endothelial growth factor (anti-VEGF) medications are the standard of care for their indications. Optical coherence tomography (OCT), as a noninvasive imaging modality, plays a major part in guiding the administration of anti-VEGF therapy by providing detailed cross-sectional scans of the retina pathology. Fully automating OCT image detection can significantly decrease the tedious clinician labor and obtain a faithful pre-diagnosis from the analysis of the structural elements of the retina. Thereby, we explore the use of deep transfer learning method based on the visual geometry group 16 (VGG-16) network for classifying AMD and DME in OCT images accurately and automatically. Method: A total of 207,130 retinal OCT images between 2013 and 2017 were selected from retrospective cohorts of 5319 adult patients from the Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Center Ophthalmology Associates, the Shanghai First People's Hospital, and the Beijing Tongren Eye Center, with 109,312 images (37,456 with choroidal neovascularization, 11,599 with diabetic macular edema, 8867 with drusen, and 51,390 normal) for the experiment. After images preprocessing, 1000 images (250 images from each category) from 633 patients were selected as validation dataset while the rest images from another 4686 patients were used as training dataset. We used deep transfer learning method to fine-tune the VGG-16 network pre-trained on the ImageNet dataset, and evaluated its performance on the validation dataset. Then, prediction accuracy, sensitivity, specificity, and receiver-operating characteristic (ROC) were calculated. Results: Experimental results proved that the proposed approach had manifested superior performance in retinal OCT images detection, which achieved a prediction accuracy of 98.6%, with a sensitivity of 97.8%, a specificity of 99.4%, and introduced an area under the ROC curve of 100%. Conclusion: Deep transfer learning method based on the VGG-16 network shows significant effectiveness on classification of retinal OCT images with a relatively small dataset, which can provide assistant support for medical decision-making. Moreover, the performance of the proposed approach is comparable to that of human experts with significant clinical experience. Thereby, it will find promising applications in an automatic diagnosis and classification of common retinal diseases.},
  doi      = {10.1007/s00417-018-04224-8},
  keywords = {Age-related macular degeneration,Deep transfer learning,Diabetic macular edema,Optical coherence tomography,Visual geometry group 16 network},
  pmid     = {30610422},
}

@Article{art/DalmisM_201801,
  author   = {Dalmış, Mehmet Ufuk and Vreemann, Suzan and Kooi, Thijs and Mann, Ritse M. and Karssemeijer, Nico and Gubern-M{\'{e}}rida, Albert},
  journal  = {Journal of Medical Imaging},
  title    = {{Fully automated detection of breast cancer in screening MRI using convolutional neural networks}},
  year     = {2018},
  issn     = {2329-4302},
  month    = {jan},
  number   = {01},
  pages    = {1},
  volume   = {5},
  abstract = {{\textcopyright} 2018 Society of Photo-Optical Instrumentation Engineers (SPIE). Current computer-aided detection (CADe) systems for contrast-enhanced breast MRI rely on both spatial information obtained from the early-phase and temporal information obtained from the late-phase of the contrast enhancement. However, late-phase information might not be available in a screening setting, such as in abbreviated MRI protocols, where acquisition is limited to early-phase scans. We used deep learning to develop a CADe system that exploits the spatial information obtained from the early-phase scans. This system uses three-dimensional (3-D) morphological information in the candidate locations and the symmetry information arising from the enhancement differences of the two breasts. We compared the proposed system to a previously developed system, which uses the full dynamic breast MRI protocol. For training and testing, we used 385 MRI scans, containing 161 malignant lesions. Performance was measured by averaging the sensitivity values between 1/8 - eight false positives. In our experiments, the proposed system obtained a significantly (p=0.008) higher average sensitivity (0.6429±0.0537) compared with that of the previous CADe system (0.5325±0.0547). In conclusion, we developed a CADe system that is able to exploit the spatial information obtained from the early-phase scans and can be used in screening programs where abbreviated MRI protocols are used.},
  doi      = {10.1117/1.jmi.5.1.014502},
  url      = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-01/014502/Fully-automated-detection-of-breast-cancer-in-screening-MRI-using/10.1117/1.JMI.5.1.014502.full},
}

@Article{art/ChiangT_201901,
  author   = {Chiang, Tsung Chen and Huang, Yao Sian and Chen, Rong Tai and Huang, Chiun Sheng and Chang, Ruey Feng},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Tumor detection in automated breast ultrasound using 3-D CNN and prioritized candidate aggregation}},
  year     = {2019},
  issn     = {1558254X},
  month    = {jan},
  number   = {1},
  pages    = {240--249},
  volume   = {38},
  abstract = {Automated whole breast ultrasound (ABUS) has been widely used as a screening modality for examination of breast abnormalities. Reviewing hundreds of slices produced by ABUS, however, is time consuming. Therefore, in this paper, a fast and effective computer-aided detection system based on 3-D convolutional neural networks (CNNs) and prioritized candidate aggregation is proposed to accelerate this reviewing. First, an efficient sliding window method is used to extract volumes of interest (VOIs). Then, each VOI is estimated the tumor probability with a 3-D CNN, and VOIs with higher estimated probability are selected as tumor candidates. Since the candidates may overlap each other, a novel scheme is designed to aggregate the overlapped candidates. During the aggregation, candidates are prioritized based on estimated tumor probability to alleviate over-aggregation issue. The relationship between the sizes of VOI and target tumor is optimally exploited to effectively perform each stage of our detection algorithm. On evaluation with a test set of 171 tumors, our method achieved sensitivities of 95% (162/171), 90% (154/171), 85% (145/171), and 80% (137/171) with 14.03, 6.92, 4.91, and 3.62 false positives per patient (with six passes), respectively. In summary, our method is more general and much faster than preliminary works and demonstrates promising results.},
  doi      = {10.1109/TMI.2018.2860257},
  keywords = {Automated whole breast ultrasound,breast cancer,computer-aided detection,convolutional neural networks},
  pmid     = {30059297},
  url      = {https://ieeexplore.ieee.org/document/8421260/},
}

@InProceedings{art/SahaA_2018,
  author   = {Saha, Ashirbani and Zhang, Jun and Cain, Elizabeth Hope and Zhu, Zhe and Mazurowski, Maciej A.},
  title    = {{Breast mass detection in mammography and tomosynthesis via fully convolutional network-based heatmap regression}},
  year     = {2018},
  pages    = {76},
  abstract = {{\textcopyright} 2018 SPIE. Breast mass detection in mammography and digital breast tomosynthesis (DBT) is an essential step in computerized breast cancer analysis. Deep learning-based methods incorporate feature extraction and model learning into a unified framework and have achieved impressive performance in various medical applications (e.g., disease diagnosis, tumor detection, and landmark detection). However, these methods require large-scale accurately annotated data. Unfortunately, it is challenging to get precise annotations of breast masses. To address this issue, we propose a fully convolutional network (FCN) based heatmap regression method for breast mass detection, using only weakly annotated mass regions in mammography images. Specifically, we first generate heat maps of masses based on human-annotated rough regions for breast masses. We then develop an FCN model for end-to-end heatmap regression with an F-score loss function, where the mammography images are regarded as the input and heatmaps for breast masses are used as the output. Finally, the probability map of mass locations can be estimated with the trained model. Experimental results on a mammography dataset with 439 subjects demonstrate the effectiveness of our method. Furthermore, we evaluate whether we can use mammography data to improve detection models for DBT, since mammography shares similar structure with tomosynthesis. We propose a transfer learning strategy by fine-tuning the learned FCN model from mammography images. We test this approach on a small tomosynthesis dataset with only 40 subjects, and we show an improvement in the detection performance as compared to training the model from scratch.},
  doi      = {10.1117/12.2295443},
  isbn     = {9781510616394},
  issn     = {16057422},
}

@Article{art/LaukampK_2019,
  author   = {Laukamp, Kai Roman and Thiele, Frank and Shakirin, Georgy and Zopfs, David and Faymonville, Andrea and Timmer, Marco and Maintz, David and Perkuhn, Michael and Borggrefe, Jan},
  journal  = {European Radiology},
  title    = {{Fully automated detection and segmentation of meningiomas using deep learning on routine multiparametric MRI}},
  year     = {2019},
  issn     = {14321084},
  number   = {1},
  pages    = {124--132},
  volume   = {29},
  abstract = {Objectives: Magnetic resonance imaging (MRI) is the method of choice for imaging meningiomas. Volumetric assessment of meningiomas is highly relevant for therapy planning and monitoring. We used a multiparametric deep-learning model (DLM) on routine MRI data including images from diverse referring institutions to investigate DLM performance in automated detection and segmentation of meningiomas in comparison to manual segmentations. Methods: We included 56 of 136 consecutive preoperative MRI datasets [T1/T2-weighted, T1-weighted contrast-enhanced (T1CE), FLAIR] of meningiomas that were treated surgically at the University Hospital Cologne and graded histologically as tumour grade I (n = 38) or grade II (n = 18). The DLM was trained on an independent dataset of 249 glioma cases and segmented different tumour classes as defined in the brain tumour image segmentation benchmark (BRATS benchmark). The DLM was based on the DeepMedic architecture. Results were compared to manual segmentations by two radiologists in a consensus reading in FLAIR and T1CE. Results: The DLM detected meningiomas in 55 of 56 cases. Further, automated segmentations correlated strongly with manual segmentations: average Dice coefficients were 0.81 ± 0.10 (range, 0.46-0.93) for the total tumour volume (union of tumour volume in FLAIR and T1CE) and 0.78 ± 0.19 (range, 0.27-0.95) for contrast-enhancing tumour volume in T1CE. Conclusions: The DLM yielded accurate automated detection and segmentation of meningioma tissue despite diverse scanner data and thereby may improve and facilitate therapy planning as well as monitoring of this highly frequent tumour entity. Key Points: • Deep learning allows for accurate meningioma detection and segmentation • Deep learning helps clinicians to assess patients with meningiomas • Meningioma monitoring and treatment planning can be improved.},
  doi      = {10.1007/s00330-018-5595-8},
  keywords = {Artificial intelligence,Brain neoplasms,Machine learning,Magnetic resonance imaging,Meningioma},
  pmid     = {29943184},
}

@InProceedings{art/PandaA_2019,
  author    = {Panda, Abhilash and Mishra, Tusar Kanti and Phaniharam, Vishnu Ganesh},
  booktitle = {Advances in Intelligent Systems and Computing},
  title     = {{Automated Brain Tumor Detection Using Discriminative Clustering Based MRI Segmentation}},
  year      = {2019},
  pages     = {117--126},
  volume    = {851},
  abstract  = {This paper presents a framework for detecting a tumor from a brain MR image automatically using discriminative clustering based brain MRI segmentation. The main objective of this paper is to perform an automatic brain tumor detection which uses superpixel zoning for its initial segmentation, which reduces computational overhead and uses discriminative clustering which accounts for tissue heterogeneity in brain MR images. In the past few years, automated brain tumor detection has become an effective topic of research in medical diagnostics and clinical expedition. Superpixel zoning of brain tissues from a brain MR image is used in this paper and superpixel zones are constructed by analyzing the intensity values of the neighborhood pixels. In clustering of these brain tissues, it still faces challenges such as tissue heterogeneity and redundancy of MRI features. To encounter these challenges, we have used a discriminative clustering method to segregate the vital regions of brain such as cerebro spinal fluid (CSF), white matter (WM) and gray matter (GM). This method uses a haar wavelet transform, which generates candidate area matrix vectors. These vectors are transformed into feature vectors which in turn used for feature selection in the dimensionality reduction. This method also uses a classification algorithm namely, AdaBoost with random forests (ADBRF) algorithm which builds a classifier that categorize the input image into tumor affected or unaffected. Experimental results of the proposed algorithm are compared to the existing methods on brain MRI segmentation and brain tumor detection shows our method outclasses the other advanced methods.},
  doi       = {10.1007/978-981-13-2414-7_12},
  isbn      = {9789811324130},
  issn      = {21945357},
  keywords  = {AdaBoost,Automated brain tumor detection,Brain MRI segmentation,Haar wavelet transform (DWT)},
}

@Article{art/ChenX_201806,
  author        = {Chen, Xiaoran and Konukoglu, Ender},
  title         = {{Unsupervised Detection of Lesions in Brain MRI using constrained adversarial auto-encoders}},
  year          = {2018},
  month         = {jun},
  abstract      = {Lesion detection in brain Magnetic Resonance Images (MRI) remains a challenging task. State-of-the-art approaches are mostly based on supervised learning making use of large annotated datasets. Human beings, on the other hand, even non-experts, can detect most abnormal lesions after seeing a handful of healthy brain images. Replicating this capability of using prior information on the appearance of healthy brain structure to detect lesions can help computers achieve human level abnormality detection, specifically reducing the need for numerous labeled examples and bettering generalization of previously unseen lesions. To this end, we study detection of lesion regions in an unsupervised manner by learning data distribution of brain MRI of healthy subjects using auto-encoder based methods. We hypothesize that one of the main limitations of the current models is the lack of consistency in latent representation. We propose a simple yet effective constraint that helps mapping of an image bearing lesion close to its corresponding healthy image in the latent space. We use the Human Connectome Project dataset to learn distribution of healthy-appearing brain MRI and report improved detection, in terms of AUC, of the lesions in the BRATS challenge dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1806.04972},
  doi           = {10.3929/ethz-b-000321650},
  eprint        = {1806.04972},
  url           = {http://arxiv.org/abs/1806.04972},
}

@Article{art/MakhzaniA_201511,
  author        = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  title         = {{Adversarial Autoencoders}},
  year          = {2015},
  month         = {nov},
  abstract      = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  archiveprefix = {arXiv},
  arxivid       = {1511.05644},
  eprint        = {1511.05644},
  url           = {http://arxiv.org/abs/1511.05644},
}

@Article{art/AlaverdyanZ_2020,
  author   = {Alaverdyan, Zaruhi and Jung, Julien and Bouet, Romain and Lartizien, Carole},
  journal  = {Medical Image Analysis},
  title    = {{Regularized siamese neural network for unsupervised outlier detection on brain multiparametric magnetic resonance imaging: Application to epilepsy lesion screening}},
  year     = {2020},
  issn     = {13618423},
  volume   = {60},
  abstract = {In this study, we propose a novel anomaly detection model targeting subtle brain lesions in multiparametric MRI. To compensate for the lack of annotated data adequately sampling the heterogeneity of such pathologies, we cast this problem as an outlier detection problem and introduce a novel configuration of unsupervised deep siamese networks to learn normal brain representations using a series of non-pathological brain scans. The proposed siamese network, composed of stacked convolutional autoencoders as subnetworks is designed to map patches extracted from healthy control scans only and centered at the same spatial localization to ‘close' representations with respect to the chosen metric in a latent space. It is based on a novel loss function combining a similarity term and a regularization term compensating for the lack of dissimilar pairs. These latent representations are then fed into oc-SVM models at voxel-level to produce anomaly score maps. We evaluate the performance of our brain anomaly detection model to detect subtle epilepsy lesions in multiparametric (T1-weighted, FLAIR) MRI exams considered as normal (MRI-negative). Our detection model trained on 75 healthy subjects and validated on 21 epilepsy patients (with 18 MRI-negatives) achieves a maximum sensitivity of 61% on the MRI-negative lesions, identified among the 5 most suspicious detections on average. It is shown to outperform detection models based on the same architecture but with stacked convolutional or Wasserstein autoencoders as unsupervised feature extraction mechanisms.},
  doi      = {10.1016/j.media.2019.101618},
  keywords = {Anomaly detection,Brain lesions,Deep learning,Regularized siamese network,Unsupervised representation learning,Wasserstein autoencoder},
  pmid     = {31841950},
}

@Article{art/RedmonJ_201804,
  author        = {Redmon, Joseph and Farhadi, Ali},
  title         = {{YOLOv3: An Incremental Improvement}},
  year          = {2018},
  month         = {apr},
  abstract      = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  archiveprefix = {arXiv},
  arxivid       = {1804.02767},
  eprint        = {1804.02767},
  url           = {http://arxiv.org/abs/1804.02767},
}

@Article{art/CholletF_201710,
  author        = {Chollet, Fran{\c{c}}ois},
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{Xception: Deep learning with depthwise separable convolutions}},
  year          = {2017},
  month         = {oct},
  pages         = {1800--1807},
  volume        = {2017-Janua},
  abstract      = {We present an interpretation of Inception modules in con-volutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17, 000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  archiveprefix = {arXiv},
  arxivid       = {1610.02357},
  doi           = {10.1109/CVPR.2017.195},
  eprint        = {1610.02357},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1610.02357},
}

@Article{art/ChenL_201812,
  author        = {Chen, Liang Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
  year          = {2018},
  issn          = {01628828},
  month         = {dec},
  number        = {4},
  pages         = {834--848},
  volume        = {40},
  abstract      = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed 'DeepLab' system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  archiveprefix = {arXiv},
  arxivid       = {1606.00915},
  doi           = {10.1109/TPAMI.2017.2699184},
  eprint        = {1606.00915},
  keywords      = {Convolutional neural networks,atrous convolution,conditional random fields,semantic segmentation},
  pmid          = {28463186},
  url           = {http://arxiv.org/abs/1412.7062},
}

@InProceedings{art/LongJ_201506,
  author    = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title     = {{Fully convolutional networks for semantic segmentation}},
  year      = {2015},
  month     = {jun},
  pages     = {431--440},
  publisher = {IEEE},
  volume    = {07-12-June},
  abstract  = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build 'fully convolutional' networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  doi       = {10.1109/CVPR.2015.7298965},
  isbn      = {9781467369640},
  issn      = {10636919},
  url       = {http://ieeexplore.ieee.org/document/7298965/},
}

@InProceedings{art/GirshickR_201512,
  author        = {Girshick, Ross},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Fast R-CNN}},
  year          = {2015},
  month         = {dec},
  pages         = {1440--1448},
  publisher     = {IEEE},
  volume        = {2015 Inter},
  abstract      = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
  archiveprefix = {arXiv},
  arxivid       = {1504.08083},
  doi           = {10.1109/ICCV.2015.169},
  eprint        = {1504.08083},
  isbn          = {9781467383912},
  issn          = {15505499},
  url           = {http://ieeexplore.ieee.org/document/7410526/},
}

@Article{art/ChenL_201706,
  author        = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  title         = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
  year          = {2017},
  month         = {jun},
  abstract      = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1706.05587},
  eprint        = {1706.05587},
  url           = {http://arxiv.org/abs/1706.05587},
}

@Article{art/ZhaoH_201712,
  author        = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  journal       = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{Pyramid scene parsing network}},
  year          = {2017},
  month         = {dec},
  pages         = {6230--6239},
  volume        = {2017-Janua},
  abstract      = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.},
  archiveprefix = {arXiv},
  arxivid       = {1612.01105},
  doi           = {10.1109/CVPR.2017.660},
  eprint        = {1612.01105},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1612.01105},
}

@Article{art/ChenL_201802,
  author        = {Chen, Liang Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Encoder-decoder with atrous separable convolution for semantic image segmentation}},
  year          = {2018},
  issn          = {16113349},
  month         = {feb},
  pages         = {833--851},
  volume        = {11211 LNCS},
  abstract      = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab.},
  archiveprefix = {arXiv},
  arxivid       = {1802.02611},
  doi           = {10.1007/978-3-030-01234-2_49},
  eprint        = {1802.02611},
  isbn          = {9783030012335},
  keywords      = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
  url           = {http://arxiv.org/abs/1802.02611},
}

@InProceedings{art/IslamJ_2018,
  author    = {Islam, Jyoti and Zhang, Yanqing},
  booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  title     = {{Early diagnosis of alzheimer's disease: A neuroimaging study with deep learning architectures}},
  year      = {2018},
  pages     = {1962--1964},
  volume    = {2018-June},
  abstract  = {Alzheimer's Disease is an incurable, progressive neurological brain disorder. Early diagnosis of Alzheimer's Disease can help with proper treatment and prevent brain tissue damage. Several statistical and machine learning models have been exploited by researchers for Alzheimer's Disease diagnosis. Detection of Alzheimer's Disease is exacting due to the similarity in Alzheimer's Disease Magnetic Resonance Imaging (MRI) data and standard healthy MRI data of older people. Recently, advanced deep learning techniques have successfully demonstrated human-level performance in numerous fields including medical image analysis. We propose a deep convolutional neural network for Alzheimer's Disease diagnosis using brain MRI data analysis. We have conducted ample experiments to demonstrate that our proposed model outperforms comparative baselines on the Open Access Series of Imaging Studies (OASIS) dataset.},
  doi       = {10.1109/CVPRW.2018.00247},
  isbn      = {9781538661000},
  issn      = {21607516},
}

@InProceedings{art/LedleyR_1966,
  author    = {Ledley, Robert S. and Rotolo, Louis S. and Belson, Marilyn and Jacobsen, John and Wilson, James B. and Golab, Thomas},
  booktitle = {AFIPS Conference Proceedings - 1966 Spring Joint Computer Conference, AFIPS 1966},
  title     = {{Pattern recognition studies in the biomedical sciences}},
  year      = {1966},
  address   = {New York, New York, USA},
  pages     = {411--430},
  publisher = {ACM Press},
  abstract  = {The biomedical sciences characteristically deal with huge masses of data, which must be organized, reduced, analyzed, and generally processed in many different ways.1 Much of this data is in the form of pictures: photomicrographs, electron micrographs, X-ray films, Schlieren photographs, X-ray diffraction patterns, autoradiographs, time-lapse films, cineradiographs, or the like. Individual pictures hold a great wealth of precise numerical information, such as the morphological and structural characteristics of lengths, areas, volumes, and densities. From sequences of pictures, quantitative results can be derived, such as the kinematic and dynamic characteristics of trajectories. Such pictures relate to almost every field of biomedical research: chromosome karyograms in cytogenetics, angiogram cineradiographs in cardiology, Schlieren photographs in ultracentrifugal molecular-weight determinations, autoradiographs of polymorphonuclear leukocytes in the study of leukemia, Golgi-stained neuron photomicrographs in the study of the ontogeny and phylogeny of the brain, X rays of bones in studies of calcium density distribution in orthopedic diseases, X rays of epiphysial plates of the hand in investigations of accurate physiological age, X-ray crystallographic plates in protein structure determination, electron micrographs in the investigation of the fine structure of virus particles, motion pictures of marine crustaceans in the detection of their sensitivity to polarized light, tissue-culture time-lapse films in the investigation of cancer-cell motility, and many others. In this paper we shall describe selected illustrations of work already accomplished by the authors in the pattern-recognition analysis of biomedical pictures. The technique involves two main steps: first, a scanning instrument, called FIDAC (Film Input to Digital Automatic Computer), scans the picture on-line into the high-speed memory of a digital computer; second, two computer programming systems (called FIDACSYS and BUGSYS) are used to recognize the objects to be measured and to process the quantitative data, according to the requirements of the particular biological or medical problem under consideration. This FIDAC system was designed specifically for the processing of biomedical pictures.},
  doi       = {10.1145/1464182.1464232},
  url       = {http://portal.acm.org/citation.cfm?doid=1464182.1464232},
}

@Article{art/CaoM_2019,
  author   = {Cao, Maomao and Chen, Wanqing},
  journal  = {Chinese Journal of Clinical Oncology},
  title    = {中国恶性肿瘤流行情况及防控现状},
  year     = {2019},
  issn     = {10008179},
  number   = {3},
  volume   = {46},
  abstract = {Cancer is one of the main diseases that greatly endanger human health. The incidence and mortality rates of cancer in China account to 23.7% and 30%, respectively, across the globe. This figure will further rise because of aging, intensification of industrialization and urbanization, lifestylemodifications, etc. Thus, the burden of cancer cannot be ignored. Cancer is caused by hereditary and environmental factors. Besides aging, other factors, such as infections and unhealthy lifestyles are also related to cancer development. Currently, the status of cancer prevention and control is facing a tremendous challenge. There are still some burning issues to be addressed in regards to implementing prophylactic measures systematically to reduce the incidence and mortality rates of cancer effectively.},
}

@Article{art/WeiW_202007,
  author   = {Wei, Wenqiang and Zeng, Hongmei and Zheng, Rongshou and Zhang, Siwei and An, Lan and Chen, Ru and Wang, Shaoming and Sun, Kexin and Matsuda, Tomohiro and Bray, Freddie and He, Jie},
  journal  = {The Lancet Oncology},
  title    = {{Cancer registration in China and its role in cancer prevention and control}},
  year     = {2020},
  issn     = {14745488},
  month    = {jul},
  number   = {7},
  pages    = {e342--e349},
  volume   = {21},
  abstract = {Cancer has become a leading cause of death in China, with an increasing burden of cancer incidence and mortality observed over the past half century. Population-based cancer registries have been operating in China for about 60 years, and, in 2018, their role has expanded to include the formulation and evaluation of national cancer control programmes and the care of patients with cancer. The purpose of this Review is to provide an overview of the key milestones in the development of cancer registration in China, the current status of registry coverage and quality, and a description of the changing cancer profile in China from 1973 to 2015. This Review is a comprehensive and updated review on the development of population-based cancer registries in China over a 60-year time span. We highlight some aspects of cancer control plans that illustrate how cancer registration data have become central to the identification of health priorities for China and provide a means to track progress in cancer control for the country.},
  doi      = {10.1016/S1470-2045(20)30073-5},
  pmid     = {32615118},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1470204520300735},
}

@TechReport{art/郝捷_2017,
  author  = {郝捷},
  title   = {2017最新中国肿瘤现状和趋势},
  year    = {2017},
  address = {Hao2017},
}

@Article{art/CainE_201901,
  author   = {Cain, Elizabeth Hope and Saha, Ashirbani and Harowicz, Michael R. and Marks, Jeffrey R. and Marcom, P. Kelly and Mazurowski, Maciej A.},
  journal  = {Breast Cancer Research and Treatment},
  title    = {{Multivariate machine learning models for prediction of pathologic response to neoadjuvant therapy in breast cancer using MRI features: a study using an independent validation set}},
  year     = {2019},
  issn     = {15737217},
  month    = {jan},
  number   = {2},
  pages    = {455--463},
  volume   = {173},
  abstract = {Purpose: To determine whether a multivariate machine learning-based model using computer-extracted features of pre-treatment dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) can predict pathologic complete response (pCR) to neoadjuvant therapy (NAT) in breast cancer patients. Methods: Institutional review board approval was obtained for this retrospective study of 288 breast cancer patients at our institution who received NAT and had a pre-treatment breast MRI. A comprehensive set of 529 radiomic features was extracted from each patient's pre-treatment MRI. The patients were divided into equal groups to form a training set and an independent test set. Two multivariate machine learning models (logistic regression and a support vector machine) based on imaging features were trained to predict pCR in (a) all patients with NAT, (b) patients with neoadjuvant chemotherapy (NACT), and (c) triple-negative or human epidermal growth factor receptor 2-positive (TN/HER2+) patients who had NAT. The multivariate models were tested using the independent test set, and the area under the receiver operating characteristics (ROC) curve (AUC) was calculated. Results: Out of the 288 patients, 64 achieved pCR. The AUC values for predicting pCR in TN/HER+ patients who received NAT were significant (0.707, 95% CI 0.582–0.833, p < 0.002). Conclusions: The multivariate models based on pre-treatment MRI features were able to predict pCR in TN/HER2+ patients.},
  doi      = {10.1007/s10549-018-4990-9},
  keywords = {Breast cancer,Breast cancer MRI,Logistic regression,MRI radiomics,Machine learning,Neoadjuvant therapy,Pathologic complete response,Support vector machines},
  pmid     = {30328048},
  url      = {http://link.springer.com/10.1007/s10549-018-4990-9},
}

@Article{art/LiJ_202104,
  author   = {Li, Jiaxin and Zhou, Zijun and Dong, Jianyu and Fu, Ying and Li, Yuan and Luan, Ze and Peng, Xin},
  journal  = {PLoS ONE},
  title    = {{Predicting breast cancer 5-year survival using machine learning: A systematic review}},
  year     = {2021},
  issn     = {19326203},
  month    = {apr},
  number   = {4 April},
  pages    = {e0250370},
  volume   = {16},
  abstract = {Background Accurately predicting the survival rate of breast cancer patients is a major issue for cancer researchers. Machine learning (ML) has attracted much attention with the hope that it could provide accurate results, but its modeling methods and prediction performance remain controversial. The aim of this systematic review is to identify and critically appraise current studies regarding the application of ML in predicting the 5-year survival rate of breast cancer. Methods In accordance with the PRISMA guidelines, two researchers independently searched the PubMed (including MEDLINE), Embase, and Web of Science Core databases from inception to November 30, 2020. The search terms included breast neoplasms, survival, machine learning, and specific algorithm names. The included studies related to the use of ML to build a breast cancer survival prediction model and model performance that can be measured with the value of said verification results. The excluded studies in which the modeling process were not explained clearly and had incomplete information. The extracted information included literature information, database information, data preparation and modeling process information, model construction and performance evaluation information, and candidate predictor information. Results Thirty-one studies that met the inclusion criteria were included, most of which were published after 2013. The most frequently used ML methods were decision trees (19 studies, 61.3%), artificial neural networks (18 studies, 58.1%), support vector machines (16 studies, 51.6%), and ensemble learning (10 studies, 32.3%). The median sample size was 37256 (range 200 to 659820) patients, and the median predictor was 16 (range 3 to 625). The accuracy of 29 studies ranged from 0.510 to 0.971. The sensitivity of 25 studies ranged from 0.037 to 1. The specificity of 24 studies ranged from 0.008 to 0.993. The AUC of 20 studies ranged from 0.500 to 0.972. The precision of 6 studies ranged from 0.549 to 1. All of the models were internally validated, and only one was externally validated. Conclusions Overall, compared with traditional statistical methods, the performance of ML models does not necessarily show any improvement, and this area of research still faces limitations related to a lack of data preprocessing steps, the excessive differences of sample feature selection, and issues related to validation. Further optimization of the performance of the proposed model is also needed in the future, which requires more standardization and subsequent validation.},
  doi      = {10.1371/journal.pone.0250370},
  editor   = {Baltzer, Pascal A. T.},
  pmid     = {33861809},
  url      = {https://dx.plos.org/10.1371/journal.pone.0250370},
}

@Article{art/SuttonE_202012,
  author   = {Sutton, Elizabeth J. and Onishi, Natsuko and Fehr, Duc A. and Dashevsky, Brittany Z. and Sadinski, Meredith and Pinker, Katja and Martinez, Danny F. and Brogi, Edi and Braunstein, Lior and Razavi, Pedram and El-Tamer, Mahmoud and Sacchini, Virgilio and Deasy, Joseph O. and Morris, Elizabeth A. and Veeraraghavan, Harini},
  journal  = {Breast Cancer Research},
  title    = {{A machine learning model that classifies breast cancer pathologic complete response on MRI post-neoadjuvant chemotherapy}},
  year     = {2020},
  issn     = {1465542X},
  month    = {dec},
  number   = {1},
  pages    = {57},
  volume   = {22},
  abstract = {Background: For breast cancer patients undergoing neoadjuvant chemotherapy (NAC), pathologic complete response (pCR; no invasive or in situ) cannot be assessed non-invasively so all patients undergo surgery. The aim of our study was to develop and validate a radiomics classifier that classifies breast cancer pCR post-NAC on MRI prior to surgery. Methods: This retrospective study included women treated with NAC for breast cancer from 2014 to 2016 with (1) pre- and post-NAC breast MRI and (2) post-NAC surgical pathology report assessing response. Automated radiomics analysis of pre- and post-NAC breast MRI involved image segmentation, radiomics feature extraction, feature pre-filtering, and classifier building through recursive feature elimination random forest (RFE-RF) machine learning. The RFE-RF classifier was trained with nested five-fold cross-validation using (a) radiomics only (model 1) and (b) radiomics and molecular subtype (model 2). Class imbalance was addressed using the synthetic minority oversampling technique. Results: Two hundred seventy-three women with 278 invasive breast cancers were included; the training set consisted of 222 cancers (61 pCR, 161 no-pCR; mean age 51.8 years, SD 11.8), and the independent test set consisted of 56 cancers (13 pCR, 43 no-pCR; mean age 51.3 years, SD 11.8). There was no significant difference in pCR or molecular subtype between the training and test sets. Model 1 achieved a cross-validation AUROC of 0.72 (95% CI 0.64, 0.79) and a similarly accurate (P = 0.1) AUROC of 0.83 (95% CI 0.71, 0.94) in both the training and test sets. Model 2 achieved a cross-validation AUROC of 0.80 (95% CI 0.72, 0.87) and a similar (P = 0.9) AUROC of 0.78 (95% CI 0.62, 0.94) in both the training and test sets. Conclusions: This study validated a radiomics classifier combining radiomics with molecular subtypes that accurately classifies pCR on MRI post-NAC.},
  doi      = {10.1186/s13058-020-01291-w},
  keywords = {Breast cancer,MRI,Machine learning,Neoadjuvant chemotherapy,Radiomics},
  pmid     = {32466777},
  url      = {https://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-020-01291-w},
}

@Article{art/DodingtonD_202104,
  author   = {Dodington, David W. and Lagree, Andrew and Tabbarah, Sami and Mohebpour, Majid and Sadeghi-Naini, Ali and Tran, William T. and Lu, Fang I.},
  journal  = {Breast Cancer Research and Treatment},
  title    = {{Analysis of tumor nuclear features using artificial intelligence to predict response to neoadjuvant chemotherapy in high-risk breast cancer patients}},
  year     = {2021},
  issn     = {15737217},
  month    = {apr},
  number   = {2},
  pages    = {379--389},
  volume   = {186},
  abstract = {Purpose: Neoadjuvant chemotherapy (NAC) is used to treat patients with high-risk breast cancer. The tumor response to NAC can be classified as either a pathological partial response (pPR) or pathological complete response (pCR), defined as complete eradication of invasive tumor cells, with a pCR conferring a significantly lower risk of recurrence. Predicting the response to NAC, however, remains a significant clinical challenge. The objective of this study was to determine if analysis of nuclear features on core biopsies using artificial intelligence (AI) can predict response to NAC. Methods: Fifty-eight HER2-positive or triple-negative breast cancer patients were included in this study (pCR n = 37, pPR n = 21). Multiple deep convolutional neural networks were developed to automate tumor detection and nuclear segmentation. Nuclear count, area, and circularity, as well as image-based first- and second-order features including mean pixel intensity and correlation of the gray-level co-occurrence matrix (GLCM-COR) were determined. Results: In univariate analysis, the pCR group had fewer multifocal/multicentric tumors, higher nuclear intensity, and lower GLCM-COR compared to the pPR group. In multivariate binary logistic regression, tumor multifocality/multicentricity (OR = 0.14, p = 0.012), nuclear intensity (OR = 1.23, p = 0.018), and GLCM-COR (OR = 0.96, p = 0.043) were each independently associated with likelihood of achieving a pCR, and the model was able to successful classify 79% of cases (62% for pPR and 89% for pCR). Conclusion: Analysis of tumor nuclear features using digital pathology/AI can significantly improve models to predict pathological response to NAC.},
  doi      = {10.1007/s10549-020-06093-4},
  keywords = {Artificial intelligence,Breast cancer,Digital pathology,Neoadjuvant chemotherapy},
  pmid     = {33486639},
  url      = {http://link.springer.com/10.1007/s10549-020-06093-4},
}

@Article{art/LuoX_201806,
  author   = {Luo, Xiongbiao and Mori, Kensaku and Peters, Terry M.},
  journal  = {Annual Review of Biomedical Engineering},
  title    = {{Advanced Endoscopic Navigation: Surgical Big Data, Methodology, and Applications}},
  year     = {2018},
  issn     = {15454274},
  month    = {jun},
  number   = {1},
  pages    = {221--251},
  volume   = {20},
  abstract = {Interventional endoscopy (e.g., bronchoscopy, colonoscopy, laparoscopy, cystoscopy) is a widely performed procedure that involves either diagnosis of suspicious lesions or guidance for minimally invasive surgery in a variety of organs within the body cavity. Endoscopy may also be used to guide the introduction of certain items (e.g., stents) into the body. Endoscopic navigation systems seek to integrate big data with multimodal information (e.g., computed tomography, magnetic resonance images, endoscopic video sequences, ultrasound images, external trackers) relative to the patient's anatomy, control the movement of medical endoscopes and surgical tools, and guide the surgeon's actions during endoscopic interventions. Nevertheless, it remains challenging to realize the next generation of context-aware navigated endoscopy. This review presents a broad survey of various aspects of endoscopic navigation, particularly with respect to the development of endoscopic navigation techniques. First, we investigate big data with multimodal information involved in endoscopic navigation. Next, we focus on numerous methodologies used for endoscopic navigation. We then review different endoscopic procedures in clinical applications. Finally, we discuss novel techniques and promising directions for the development of endoscopic navigation.},
  doi      = {10.1146/annurev-bioeng-062117-120917},
  keywords = {3D printing,artificial intelligence,augmented reality,big data,deep learning,endoscopic vision,endoscopy,image registration,image-guided intervention,surgical navigation,surgical robotics},
  pmid     = {29505729},
  url      = {https://www.annualreviews.org/doi/10.1146/annurev-bioeng-062117-120917},
}

@Article{art/YangL_202009,
  author   = {Yang, Liangjing and Etsuko, Kobayashi},
  journal  = {IET Cyber-Systems and Robotics},
  title    = {{Review on vision‐based tracking in surgical navigation}},
  year     = {2020},
  issn     = {2631-6315},
  month    = {sep},
  number   = {3},
  pages    = {107--121},
  volume   = {2},
  abstract = {Computer vision is an important cornerstone for the foundation of many modern technologies. The development of modern computer-aided-surgery, especially in the context of surgical navigation for minimally invasive surgery, is one example. Surgical navigation provides the necessary spatial information in computer-aided-surgery. Amongst the various forms of perception, vision-based sensing has been proposed as a promising candidate for tracking and localisation application largely due to its ability to provide timely intra-operative feedback and contactless sensing. The motivation for vision-based sensing in surgical navigation stems from many factors, including the challenges faced by other forms of navigation systems. A common surgical navigation system performs tracking of surgical tools with external tracking systems, which may suffer from both technical and usability issues. Vision-based tracking offers a relatively streamlined framework compared to those approaches implemented with external tracking systems. This review study aims to discuss contemporary research and development in vision-based sensing for surgical navigation. The selected review materials are expected to provide a comprehensive appreciation of state-of-the-art technology and technical issues enabling holistic discussions of the challenges and knowledge gaps in contemporary development. Original views on the significance and development prospect of vision-based sensing in surgical navigation are presented.},
  doi      = {10.1049/iet-csr.2020.0013},
  url      = {https://onlinelibrary.wiley.com/doi/10.1049/iet-csr.2020.0013},
}

@Article{art/YimJ_202006,
  author   = {Yim, Jason and Chopra, Reena and Spitz, Terry and Winkens, Jim and Obika, Annette and Kelly, Christopher and Askham, Harry and Lukic, Marko and Huemer, Josef and Fasler, Katrin and Moraes, Gabriella and Meyer, Clemens and Wilson, Marc and Dixon, Jonathan and Hughes, Cian and Rees, Geraint and Khaw, Peng T. and Karthikesalingam, Alan and King, Dominic and Hassabis, Demis and Suleyman, Mustafa and Back, Trevor and Ledsam, Joseph R. and Keane, Pearse A. and {De Fauw}, Jeffrey},
  journal  = {Nature Medicine},
  title    = {{Predicting conversion to wet age-related macular degeneration using deep learning}},
  year     = {2020},
  issn     = {1546170X},
  month    = {jun},
  number   = {6},
  pages    = {892--899},
  volume   = {26},
  abstract = {Progression to exudative ‘wet' age-related macular degeneration (exAMD) is a major cause of visual deterioration. In patients diagnosed with exAMD in one eye, we introduce an artificial intelligence (AI) system to predict progression to exAMD in the second eye. By combining models based on three-dimensional (3D) optical coherence tomography images and corresponding automatic tissue maps, our system predicts conversion to exAMD within a clinically actionable 6-month time window, achieving a per-volumetric-scan sensitivity of 80% at 55% specificity, and 34% sensitivity at 90% specificity. This level of performance corresponds to true positives in 78% and 41% of individual eyes, and false positives in 56% and 17% of individual eyes at the high sensitivity and high specificity points, respectively. Moreover, we show that automatic tissue segmentation can identify anatomical changes before conversion and high-risk subgroups. This AI system overcomes substantial interobserver variability in expert predictions, performing better than five out of six experts, and demonstrates the potential of using AI to predict disease progression.},
  doi      = {10.1038/s41591-020-0867-7},
  pmid     = {32424211},
  url      = {http://www.nature.com/articles/s41591-020-0867-7},
}

@Article{art/KimJ_202112,
  author   = {Kim, Jeoung Kun and Choo, Yoo Jin and Shin, Hyunkwang and Choi, Gyu Sang and Chang, Min Cheol},
  journal  = {Scientific Reports},
  title    = {{Prediction of ambulatory outcome in patients with corona radiata infarction using deep learning}},
  year     = {2021},
  issn     = {2045-2322},
  month    = {dec},
  number   = {1},
  pages    = {7989},
  volume   = {11},
  abstract = {Deep learning (DL) is an advanced machine learning approach used in diverse areas such as bioinformatics, image analysis, and natural language processing. Here, using brain magnetic resonance imaging (MRI) data obtained at early stages of infarcts, we attempted to develop a convolutional neural network (CNN) to predict the ambulatory outcome of corona radiata infarction at six months after onset. We retrospectively recruited 221 patients with corona radiata infarcts. A favorable outcome of ambulatory function was defined as a functional ambulation category (FAC) score of ≥ 4 (able to walk without a guardian's assistance), and a poor outcome of ambulatory function was defined as an FAC score of < 4. We used a CNN algorithm. Of the included subjects, 69.7% (n = 154) were assigned randomly to the training set and the remaining 30.3% (n = 67) were assigned to the validation set to measure the model performance. The area under the curve was 0.751 (95% CI 0.649–0.852) for the prediction of ambulatory function with the validation dataset using the CNN model. We demonstrated that a CNN model trained using brain MRIs captured at an early stage after corona radiata infarction could be helpful in predicting long-term ambulatory outcomes.},
  doi      = {10.1038/s41598-021-87176-0},
  url      = {http://www.nature.com/articles/s41598-021-87176-0},
}

@Article{art/MehdizadehM_202104,
  author   = {Mehdizadeh, Maryam and MacNish, Cara and Xiao, Di and Alonso-Caneiro, David and Kugelman, Jason and Bennamoun, Mohammed},
  journal  = {Journal of Biomedical Optics},
  title    = {{Deep feature loss to denoise OCT images using deep neural networks}},
  year     = {2021},
  issn     = {15602281},
  month    = {apr},
  number   = {04},
  volume   = {26},
  abstract = {Significance: Speckle noise is an inherent limitation of optical coherence tomography (OCT) images that makes clinical interpretation challenging. The recent emergence of deep learning could offer a reliable method to reduce noise in OCT images. Aim: We sought to investigate the use of deep features (VGG) to limit the effect of blurriness and increase perceptual sharpness and to evaluate its impact on the performance of OCT image denoising (DnCNN). Approach: Fifty-one macula-centered OCT pairs were used in training of the network. Another set of 20 OCT pair was used for testing. The DnCNN model was cascaded with a VGG network that acted as a perceptual loss function instead of the traditional losses of L1 and L2. The VGG network remains fixed during the training process. We focused on the individual layers of the VGG-16 network to decipher the contribution of each distinctive layer as a loss function to produce denoised OCT images that were perceptually sharp and that preserved the faint features (retinal layer boundaries) essential for interpretation. The peak signal-to-noise ratio (PSNR), edge-preserving index, and no-reference image sharpness/blurriness [perceptual sharpness index (PSI), just noticeable blur (JNB), and spectral and spatial sharpness measure (S3)] metrics were used to compare deep feature losses with the traditional losses. Results: The deep feature loss produced images with high perceptual sharpness measures at the cost of less smoothness (PSNR) in OCT images. The deep feature loss outperformed the traditional losses (L1 and L2) for all of the evaluation metrics except for PSNR. The PSI, S3, and JNB estimates of deep feature loss performance were 0.31, 0.30, and 16.53, respectively. For L1 and L2 losses performance, the PSI, S3, and JNB were 0.21 and 0.21, 0.17 and 0.16, and 14.46 and 14.34, respectively. Conclusions: We demonstrate the potential of deep feature loss in denoising OCT images. Our preliminary findings suggest research directions for further investigation. {\textcopyright} The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.},
  doi      = {10.1117/1.jbo.26.4.046003},
  pmid     = {33893726},
  url      = {https://www.spiedigitallibrary.org/journals/journal-of-biomedical-optics/volume-26/issue-04/046003/Deep-feature-loss-to-denoise-OCT-images-using-deep-neural/10.1117/1.JBO.26.4.046003.full},
}

@Article{art/TangZ_201912,
  author   = {Tang, Ziqi and Chuang, Kangway V. and DeCarli, Charles and Jin, Lee Way and Beckett, Laurel and Keiser, Michael J. and Dugger, Brittany N.},
  journal  = {Nature Communications},
  title    = {{Interpretable classification of Alzheimer's disease pathologies with a convolutional neural network pipeline}},
  year     = {2019},
  issn     = {20411723},
  month    = {dec},
  number   = {1},
  pages    = {2173},
  volume   = {10},
  abstract = {Neuropathologists assess vast brain areas to identify diverse and subtly-differentiated morphologies. Standard semi-quantitative scoring approaches, however, are coarse-grained and lack precise neuroanatomic localization. We report a proof-of-concept deep learning pipeline that identifies specific neuropathologies—amyloid plaques and cerebral amyloid angiopathy—in immunohistochemically-stained archival slides. Using automated segmentation of stained objects and a cloud-based interface, we annotate > 70,000 plaque candidates from 43 whole slide images (WSIs) to train and evaluate convolutional neural networks. Networks achieve strong plaque classification on a 10-WSI hold-out set (0.993 and 0.743 areas under the receiver operating characteristic and precision recall curve, respectively). Prediction confidence maps visualize morphology distributions at high resolution. Resulting network-derived amyloid beta (A$\beta$)-burden scores correlate well with established semi-quantitative scores on a 30-WSI blinded hold-out. Finally, saliency mapping demonstrates that networks learn patterns agreeing with accepted pathologic features. This scalable means to augment a neuropathologist's ability suggests a route to neuropathologic deep phenotyping.},
  doi      = {10.1038/s41467-019-10212-1},
  pmid     = {31092819},
  url      = {http://www.nature.com/articles/s41467-019-10212-1},
}

@Article{art/WangB_202101,
  author   = {Wang, Bo and Jin, Shuo and Yan, Qingsen and Xu, Haibo and Luo, Chuan and Wei, Lai and Zhao, Wei and Hou, Xuexue and Ma, Wenshuo and Xu, Zhengqing and Zheng, Zhuozhao and Sun, Wenbo and Lan, Lan and Zhang, Wei and Mu, Xiangdong and Shi, Chenxi and Wang, Zhongxiao and Lee, Jihae and Jin, Zijian and Lin, Minggui and Jin, Hongbo and Zhang, Liang and Guo, Jun and Zhao, Benqi and Ren, Zhizhong and Wang, Shuhao and Xu, Wei and Wang, Xinghuan and Wang, Jianming and You, Zheng and Dong, Jiahong},
  journal  = {Applied Soft Computing},
  title    = {{AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system}},
  year     = {2021},
  issn     = {15684946},
  month    = {jan},
  pages    = {106897},
  volume   = {98},
  abstract = {The sudden outbreak of novel coronavirus 2019 (COVID-19) increased the diagnostic burden of radiologists. In the time of an epidemic crisis, we hope artificial intelligence (AI) to reduce physician workload in regions with the outbreak, and improve the diagnosis accuracy for physicians before they could acquire enough experience with the new disease. In this paper, we present our experience in building and deploying an AI system that automatically analyzes CT images and provides the probability of infection to rapidly detect COVID-19 pneumonia. The proposed system which consists of classification and segmentation will save about 30%–40% of the detection time for physicians and promote the performance of COVID-19 detection. Specifically, working in an interdisciplinary team of over 30 people with medical and/or AI background, geographically distributed in Beijing and Wuhan, we are able to overcome a series of challenges (e.g. data discrepancy, testing time-effectiveness of model, data security, etc.) in this particular situation and deploy the system in four weeks. In addition, since the proposed AI system provides the priority of each CT image with probability of infection, the physicians can confirm and segregate the infected patients in time. Using 1,136 training cases (723 positives for COVID-19) from five hospitals, we are able to achieve a sensitivity of 0.974 and specificity of 0.922 on the test dataset, which included a variety of pulmonary diseases.},
  doi      = {10.1016/j.asoc.2020.106897},
  keywords = {COVID-19,Classification,Deep learning,Medical assistance system,Neural network,Segmentation},
  pmid     = {33199977},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1568494620308358},
}

@Article{art/MaoC_202009,
  author  = {Mao, Cui P. and Chen, Fen R. and Huo, Jiao H. and Zhang, Liang and Zhang, Gui R. and Zhang, Bing and Zhou, Xiao Q.},
  journal = {Human Brain Mapping},
  title   = {{Altered resting‐state functional connectivity and effective connectivity of the habenula in irritable bowel syndrome: A cross‐sectional and machine learning study}},
  year    = {2020},
  issn    = {1065-9471},
  month   = {sep},
  number  = {13},
  pages   = {3655--3666},
  volume  = {41},
  doi     = {10.1002/hbm.25038},
  url     = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.25038},
}

@Article{art/LiuN_201912,
  author   = {Liu, Ning and Liu, Ying and Logan, Brent and Xu, Zhiyuan and Tang, Jian and Wang, Yanzhi},
  journal  = {Scientific Reports},
  title    = {{Learning the Dynamic Treatment Regimes from Medical Registry Data through Deep Q-network}},
  year     = {2019},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {1495},
  volume   = {9},
  abstract = {This paper presents the deep reinforcement learning (DRL) framework to estimate the optimal Dynamic Treatment Regimes from observational medical data. This framework is more flexible and adaptive for high dimensional action and state spaces than existing reinforcement learning methods to model real-life complexity in heterogeneous disease progression and treatment choices, with the goal of providing doctors and patients the data-driven personalized decision recommendations. The proposed DRL framework comprises (i) a supervised learning step to predict expert actions, and (ii) a deep reinforcement learning step to estimate the long-term value function of Dynamic Treatment Regimes. Both steps depend on deep neural networks. As a key motivational example, we have implemented the proposed framework on a data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database, focusing on the sequence of prevention and treatments for acute and chronic graft versus host disease after transplantation. In the experimental results, we have demonstrated promising accuracy in predicting human experts' decisions, as well as the high expected reward function in the DRL-based dynamic treatment regimes.},
  doi      = {10.1038/s41598-018-37142-0},
  pmid     = {30728403},
  url      = {http://www.nature.com/articles/s41598-018-37142-0},
}

@Article{art/PengH_202105,
  author   = {Peng, Hong and Huo, Jiaohua and Li, Bo and Cui, Yuanyuan and Zhang, Hao and Zhang, Liang and Ma, Lin},
  journal  = {Journal of Magnetic Resonance Imaging},
  title    = {{Predicting Isocitrate Dehydrogenase (IDH) Mutation Status in Gliomas Using Multiparameter MRI Radiomics Features}},
  year     = {2021},
  issn     = {15222586},
  month    = {may},
  number   = {5},
  pages    = {1399--1407},
  volume   = {53},
  abstract = {Background: Accurate and noninvasive detection of isocitrate dehydrogenase (IDH, including IDH1 and IDH2) status is clinically meaningful for molecular stratification of glioma, but remains challenging. Purpose: To establish a model for classifying IDH status in gliomas based on multiparametric MRI. Study Type: Retrospective, radiomics. Population: In all, 105 consecutive cases of grade II–IV glioma with 50 IDH1 or IDH2 mutant (IDHm) and 55 IDH wildtype (IDHw) were separated into a training cohort (n = 73) and a test cohort (n = 32). Field Strength/Sequence: Contrast-enhanced T1-weighted (CE-T1W), T2-weighted (T2W), and arterial spin labeling (ASL) images were acquired at 3.0T. Assessment: Two doctors manually labeled the volume of interest (VOI) on CE-T1W, then T2W and ASL were coregistered to CE-T1W. A total of 851 radiomics features were extracted on each VOI of three sequences. From the training cohort, all radiomics features with age and gender were processed by the Mann–Whitney U-test, Pearson test, and least absolute shrinkage and selection operator to obtain optimal feature groups to train support vector machine models. The accuracy and area under curve (AUC) of all models for classifying the IDH status were calculated on the test cohort. Two subtasks were performed to verify the efficiency of texture features and the Pearson test in IDH status classification, respectively. Statistical Tests: The permutation test with Bonferroni correction; chi-square test. Results: The accuracy and AUC of the classifier, which combines the features of all three sequences, achieved 0.823 and 0.770 (P < 0.05), respectively. The best model established by texture features only had an AUC of 0.819 and an accuracy of 0.761. The best model established without the Pearson test got an AUC of 0.747 and an accuracy of 0.719. Data Conclusion: IDH genotypes of glioma can be identified by radiomics features from multiparameter MRI. The Pearson test improved the performance of the IDH classification models. Level of Evidence: 4. Technical Efficacy Stage: 1.},
  doi      = {10.1002/jmri.27434},
  keywords = {IDH,glioma,multiparametric MRI,support vector machine},
  pmid     = {33179832},
  url      = {https://onlinelibrary.wiley.com/doi/10.1002/jmri.27434},
}

@Article{art/ShiZ_202012,
  author   = {Shi, Zhao and Miao, Chongchang and Schoepf, U. Joseph and Savage, Rock H. and Dargis, Danielle M. and Pan, Chengwei and Chai, Xue and Li, Xiu Li and Xia, Shuang and Zhang, Xin and Gu, Yan and Zhang, Yonggang and Hu, Bin and Xu, Wenda and Zhou, Changsheng and Luo, Song and Wang, Hao and Mao, Li and Liang, Kongming and Wen, Lili and Zhou, Longjiang and Yu, Yizhou and Lu, Guang Ming and Zhang, Long Jiang},
  journal  = {Nature Communications},
  title    = {{A clinically applicable deep-learning model for detecting intracranial aneurysm in computed tomography angiography images}},
  year     = {2020},
  issn     = {20411723},
  month    = {dec},
  number   = {1},
  pages    = {6090},
  volume   = {11},
  abstract = {Intracranial aneurysm is a common life-threatening disease. Computed tomography angiography is recommended as the standard diagnosis tool; yet, interpretation can be time-consuming and challenging. We present a specific deep-learning-based model trained on 1,177 digital subtraction angiography verified bone-removal computed tomography angiography cases. The model has good tolerance to image quality and is tested with different manufacturers. Simulated real-world studies are conducted in consecutive internal and external cohorts, in which it achieves an improved patient-level sensitivity and lesion-level sensitivity compared to that of radiologists and expert neurosurgeons. A specific cohort of suspected acute ischemic stroke is employed and it is found that 99.0% predicted-negative cases can be trusted with high confidence, leading to a potential reduction in human workload. A prospective study is warranted to determine whether the algorithm could improve patients' care in comparison to clinicians' assessment.},
  doi      = {10.1038/s41467-020-19527-w},
  pmid     = {33257700},
  url      = {http://www.nature.com/articles/s41467-020-19527-w},
}

@Article{art/BaeJ_202012,
  author   = {Bae, Jong Bin and Lee, Subin and Jung, Wonmo and Park, Sejin and Kim, Weonjin and Oh, Hyunwoo and Han, Ji Won and Kim, Grace Eun and Kim, Jun Sung and Kim, Jae Hyoung and Kim, Ki Woong},
  journal  = {Scientific Reports},
  title    = {{Identification of Alzheimer's disease using a convolutional neural network model based on T1-weighted magnetic resonance imaging}},
  year     = {2020},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {22252},
  volume   = {10},
  abstract = {The classification of Alzheimer's disease (AD) using deep learning methods has shown promising results, but successful application in clinical settings requires a combination of high accuracy, short processing time, and generalizability to various populations. In this study, we developed a convolutional neural network (CNN)-based AD classification algorithm using magnetic resonance imaging (MRI) scans from AD patients and age/gender-matched cognitively normal controls from two populations that differ in ethnicity and education level. These populations come from the Seoul National University Bundang Hospital (SNUBH) and Alzheimer's Disease Neuroimaging Initiative (ADNI). For each population, we trained CNNs on five subsets using coronal slices of T1-weighted images that cover the medial temporal lobe. We evaluated the models on validation subsets from both the same population (within-dataset validation) and other population (between-dataset validation). Our models achieved average areas under the curves of 0.91–0.94 for within-dataset validation and 0.88–0.89 for between-dataset validation. The mean processing time per person was 23–24 s. The within-dataset and between-dataset performances were comparable between the ADNI-derived and SNUBH-derived models. These results demonstrate the generalizability of our models to different patients with different ethnicities and education levels, as well as their potential for deployment as fast and accurate diagnostic support tools for AD.},
  doi      = {10.1038/s41598-020-79243-9},
  pmid     = {33335244},
  url      = {http://www.nature.com/articles/s41598-020-79243-9},
}

@Article{art/DemirciogluA_202112,
  author   = {Demircioğlu, Aydin and Stein, Magdalena Charis and Kim, Moon Sung and Geske, Henrike and Quinsten, Anton S. and Blex, Sebastian and Umutlu, Lale and Nassenstein, Kai},
  journal  = {Scientific Reports},
  title    = {{Detecting the pulmonary trunk in CT scout views using deep learning}},
  year     = {2021},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {10215},
  volume   = {11},
  abstract = {For CT pulmonary angiograms, a scout view obtained in anterior–posterior projection is usually used for planning. For bolus tracking the radiographer manually locates a position in the CT scout view where the pulmonary trunk will be visible in an axial CT pre-scan. We automate the task of localizing the pulmonary trunk in CT scout views by deep learning methods. In 620 eligible CT scout views of 563 patients between March 2003 and February 2020 the region of the pulmonary trunk as well as an optimal slice (“reference standard”) for bolus tracking, in which the pulmonary trunk was clearly visible, was annotated and used to train a U-Net predicting the region of the pulmonary trunk in the CT scout view. The networks' performance was subsequently evaluated on 239 CT scout views from 213 patients and was compared with the annotations of three radiographers. The network was able to localize the region of the pulmonary trunk with high accuracy, yielding an accuracy of 97.5% of localizing a slice in the region of the pulmonary trunk on the validation cohort. On average, the selected position had a distance of 5.3 mm from the reference standard. Compared to radiographers, using a non-inferiority test (one-sided, paired Wilcoxon rank-sum test) the network performed as well as each radiographer (P < 0.001 in all cases). Automated localization of the region of the pulmonary trunk in CT scout views is possible with high accuracy and is non-inferior to three radiographers.},
  doi      = {10.1038/s41598-021-89647-w},
  pmid     = {33986402},
  url      = {http://www.nature.com/articles/s41598-021-89647-w},
}

@Article{art/KhanS_201802,
  author   = {Khan, Salman and Rahmani, Hossein and Shah, Syed Afaq Ali and Bennamoun, Mohammed},
  journal  = {Synthesis Lectures on Computer Vision},
  title    = {{A Guide to Convolutional Neural Networks for Computer Vision}},
  year     = {2018},
  issn     = {2153-1056},
  month    = {feb},
  number   = {1},
  pages    = {1--207},
  volume   = {8},
  abstract = {Computer vision has become increasingly important and effective in recent years due to its wide-ranging applications in areas as diverse as smart surveillance and monitoring, health and medicine, sports and recreation, robotics, drones, and self-driving cars. Visual recognition tasks, such as image classification, localization, and detection, are the core building blocks of many of these applications, and recent developments in Convolutional Neural Networks (CNNs) have led to outstanding performance in these state-of-the-art visual recognition tasks and systems. As a result, CNNs now form the crux of deep learning algorithms in computer vision. is self-contained guide will benefit those who seek to both understand the theory be- hind CNNs and to gain hands-on experience on the application of CNNs in computer vision. It provides a comprehensive introduction to CNNs starting with the essential concepts behind neural networks: training, regularization, and optimization of CNNs. e book also discusses a wide range of loss functions, network layers, and popular CNN architectures, reviews the differ- ent techniques for the evaluation of CNNs, and presents some popular CNN tools and libraries that are commonly used in computer vision. Further, this text describes and discusses case stud- ies that are related to the application of CNN in computer vision, including image classification, object detection, semantic segmentation, scene understanding, and image generation. is book is ideal for undergraduate and graduate students, as no prior background knowl- edge in the field is required to follow the material, as well as new researchers, developers, engi- neers, and practitioners who are interested in gaining a quick understanding of CNN models.},
  doi      = {10.2200/s00822ed1v01y201712cov015},
  url      = {http://www.morganclaypool.com/doi/10.2200/S00822ED1V01Y201712COV015},
}

@Article{art/HouQ_202103,
  author        = {Hou, Qibin and Zhou, Daquan and Feng, Jiashi},
  title         = {{Coordinate Attention for Efficient Mobile Network Design}},
  year          = {2021},
  month         = {mar},
  abstract      = {Recent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call "coordinate attention". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention.},
  archiveprefix = {arXiv},
  arxivid       = {2103.02907},
  eprint        = {2103.02907},
  url           = {http://arxiv.org/abs/2103.02907},
}

@Article{art/XieE_2021,
  author  = {Xie, Enze and Wang, Wenhai and Ding, Mingyu and Zhang, Ruimao and Luo, Ping},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {{PolarMask++: Enhanced Polar Representation for Single-Shot Instance Segmentation and Beyond}},
  year    = {2021},
  issn    = {0162-8828},
  pages   = {1--1},
  doi     = {10.1109/TPAMI.2021.3080324},
  url     = {https://ieeexplore.ieee.org/document/9431650/},
}

@Article{art/XieE_201909,
  author        = {Xie, Enze and Sun, Peize and Song, Xiaoge and Wang, Wenhai and Liang, Ding and Shen, Chunhua and Luo, Ping},
  title         = {{PolarMask: Single Shot Instance Segmentation with Polar Representation}},
  year          = {2019},
  month         = {sep},
  abstract      = {In this paper, we introduce an anchor-box free and single shot instance segmentation method, which is conceptually simple, fully convolutional and can be used as a mask prediction module for instance segmentation, by easily embedding it into most off-the-shelf detection methods. Our method, termed PolarMask, formulates the instance segmentation problem as instance center classification and dense distance regression in a polar coordinate. Moreover, we propose two effective approaches to deal with sampling high-quality center examples and optimization for dense distance regression, respectively, which can significantly improve the performance and simplify the training process. Without any bells and whistles, PolarMask achieves 32.9% in mask mAP with single-model and single-scale training/testing on challenging COCO dataset. For the first time, we demonstrate a much simpler and flexible instance segmentation framework achieving competitive accuracy. We hope that the proposed PolarMask framework can serve as a fundamental and strong baseline for single shot instance segmentation tasks. Code is available at: github.com/xieenze/PolarMask.},
  archiveprefix = {arXiv},
  arxivid       = {1909.13226},
  eprint        = {1909.13226},
  url           = {http://arxiv.org/abs/1909.13226},
}

@Article{art/BertinettoL_201606,
  author        = {Bertinetto, Luca and Henriques, Jo{\~{a}}o F. and Valmadre, Jack and Torr, Philip H.S. and Vedaldi, Andrea},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {{Learning feed-forward one-shot learners}},
  year          = {2016},
  issn          = {10495258},
  month         = {jun},
  pages         = {523--531},
  abstract      = {One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1606.05233},
  eprint        = {1606.05233},
  url           = {http://arxiv.org/abs/1606.05233},
}

@Misc{art/NicholA_201803,
  author        = {Nichol, Alex and Achiam, Joshua and Schulman, John},
  month         = {mar},
  title         = {{On First-Order Meta-Learning Algorithms}},
  year          = {2018},
  abstract      = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
  archiveprefix = {arXiv},
  arxivid       = {1803.02999},
  booktitle     = {arXiv},
  eprint        = {1803.02999},
  issn          = {23318422},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/1803.02999},
}

@Article{art/RakellyK_201805,
  author        = {Rakelly, Kate and Shelhamer, Evan and Darrell, Trevor and Efros, Alexei A. and Levine, Sergey},
  journal       = {arXiv},
  title         = {{Few-Shot Segmentation Propagation with Guided Networks}},
  year          = {2018},
  issn          = {23318422},
  month         = {may},
  abstract      = {Learning-based methods for visual segmentation have made progress on particular types of segmentation tasks, but are limited by the necessary supervision, the narrow definitions of fixed tasks, and the lack of control during inference for correcting errors. To remedy the rigidity and annotation burden of standard approaches, we address the problem of few-shot segmentation: given few image and few pixel supervision, segment any images accordingly. We propose guided networks, which extract a latent task representation from any amount of supervision, and optimize our architecture end-to-end for fast, accurate few-shot segmentation. Our method can switch tasks without further optimization and quickly update when given more guidance. We report the first results for segmentation from one pixel per concept and show real-time interactive video segmentation. Our unified approach propagates pixel annotations across space for interactive segmentation, across time for video segmentation, and across scenes for semantic segmentation. Our guided segmentor is state-of-the-art in accuracy for the amount of annotation and time. See http://github.com/shelhamer/revolver for code, models, and more details.},
  archiveprefix = {arXiv},
  arxivid       = {1806.07373},
  eprint        = {1806.07373},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rakelly et al. - 2018 - Few-Shot Segmentation Propagation with Guided Networks.pdf:pdf},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/1806.07373},
}

@InProceedings{art/RakellyK_2018,
  author    = {Rakelly, Kate and Shelhamer, Evan and Darrell, Trevor and Efros, Alexei and Levine, Sergey},
  booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings},
  title     = {{Conditional networks for few-shot semantic segmentation}},
  year      = {2018},
  publisher = {International Conference on Learning Representations, ICLR},
  abstract  = {Few-shot learning methods aim for good performance in the low-data regime. Structured output tasks such as segmentation present difficulties for few-shot learning because of their high dimensionality and the statistical dependencies among outputs. To tackle this problem, we propose the co-FCN, a conditional network learned by end-to-end optimization to perform fast, accurate few-shot segmentation. The network conditions on an annotated support set of images via feature fusion to do inference on an unannotated query image. Once learned, our conditioning approach requires no further optimization for new data. Annotations are instead conditioned on in a single forward pass, making our method suitable for interactive use. We evaluate our co-FCN with dense and sparse annotations, and it achieves competitive accuracy even when given only one positive pixel and one negative pixel, reducing the annotation burden for segmenting new concepts.},
}

@InProceedings{art/HaoS_201610,
  author    = {Hao, Shijie and Li, Gang and Wang, Li and Meng, Yu and Shen, Dinggang},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Learning-based topological correction for infant cortical surfaces}},
  year      = {2016},
  month     = {oct},
  pages     = {219--227},
  publisher = {Springer Verlag},
  volume    = {9900 LNCS},
  abstract  = {Reconstruction of topologically correct and accurate cortical surfaces from infant MR images is of great importance in neuroimaging mapping of early brain development. However,due to rapid growth and ongoing myelination,infant MR images exhibit extremely low tissue contrast and dynamic appearance patterns,thus leading to much more topological errors (holes and handles) in the cortical surfaces derived from tissue segmentation results,in comparison to adult MR images which typically have good tissue contrast. Existing methods for topological correction either rely on the minimal correction criteria,or ad hoc rules based on image intensity priori,thus often resulting in erroneous correction and large anatomical errors in reconstructed infant cortical surfaces. To address these issues,we propose to correct topological errors by learning information from the anatomical references,i.e.,manually corrected images. Specifically,in our method,we first locate candidate voxels of topologically defected regions by using a topology-preserving level set method. Then,by leveraging rich information of the corresponding patches from reference images,we build regionspecific dictionaries from the anatomical references and infer the correct labels of candidate voxels using sparse representation. Notably,we further integrate these two steps into an iterative framework to enable gradual correction of large topological errors,which are frequently occurred in infant images and cannot be completely corrected using one-shot sparse representation. Extensive experiments on infant cortical surfaces demonstrate that our method not only effectively corrects the topological defects,but also leads to better anatomical consistency,compared to the state-of-the-art methods.},
  doi       = {10.1007/978-3-319-46720-7_26},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hao et al. - 2016 - Learning-based topological correction for infant cortical surfaces.pdf:pdf},
  isbn      = {9783319467191},
  issn      = {16113349},
  url       = {https://link.springer.com/chapter/10.1007/978-3-319-46720-7_26},
}

@InProceedings{art/SavioliN_201909,
  author        = {Savioli, Nicol{\'{o}} and Montana, Giovanni and Lamata, Pablo},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{V-FCNN: Volumetric Fully Convolution Neural Network for Automatic Atrial Segmentation}},
  year          = {2019},
  month         = {sep},
  pages         = {273--281},
  publisher     = {Springer Verlag},
  volume        = {11395 LNCS},
  abstract      = {Atrial Fibrillation (AF) is a common electro-physiological cardiac disorder that causes changes in the anatomy of the atria. A better characterization of these changes is desirable for the definition of clinical biomarkers. There is thus a need for its fully automatic segmentation from clinical images. This work presents an architecture based on 3D-convolution kernels, a Volumetric Fully Convolution Neural Network (V-FCNN), able to segment the entire atrial anatomy in a one-shot from high-resolution images ((Formula Presented) pixels). A loss function based on the mixture of both Mean Square Error (MSE) and Dice Loss (DL) is used, in an attempt to combine the ability to capture the bulk shape as well as the reduction of local errors caused by over-segmentation. Results demonstrate a good performance in the middle region of the atria along with the challenges impact of capturing the pulmonary veins variability or valve plane identification that separates the atria to the ventricle. Despite the need to reduce the original image resolution to fit into Graphics Processing Unit (GPU) hardware constraints, $$92.5\%$$ and $$85.1\%$$ were obtained respectively in the 2D and 3D Dice metric in 54 test patients (4752 atria test slices in total), making the V-FCNN a reasonable model to be used in clinical practice.},
  archiveprefix = {arXiv},
  arxivid       = {1808.01944},
  doi           = {10.1007/978-3-030-12029-0_30},
  eprint        = {1808.01944},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Savioli, Montana, Lamata - 2019 - V-FCNN Volumetric Fully Convolution Neural Network for Automatic Atrial Segmentation.pdf:pdf},
  isbn          = {9783030120283},
  issn          = {16113349},
  keywords      = {Anatomy,Atria,Cardiac imaging,Clinical biomarkers,Deep learning,FCNN,Fibrillation,MRI scanner,Segmentation,Shape},
  url           = {https://doi.org/10.1007/978-3-030-12029-0_30},
}

@InProceedings{art/MirikharajiZ_201910,
  author        = {Mirikharaji, Zahra and Yan, Yiqi and Hamarneh, Ghassan},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Learning to segment skin lesions from noisy annotations}},
  year          = {2019},
  month         = {oct},
  pages         = {207--215},
  publisher     = {Springer},
  volume        = {11795 LNCS},
  abstract      = {Deep convolutional neural networks have driven substantial advancements in the automatic understanding of images. Requiring a large collection of images and their associated annotations is one of the main bottlenecks limiting the adoption of deep networks. In the task of medical image segmentation, requiring pixel-level semantic annotations performed by human experts exacerbate this difficulty. This paper proposes a new framework to train a fully convolutional segmentation network from a large set of cheap unreliable annotations and a small set of expert-level clean annotations. We propose a spatially adaptive reweighting approach to treat clean and noisy pixel-level annotations commensurately in the loss function. We deploy a meta-learning approach to assign higher importance to pixels whose loss gradient direction is closer to those of clean data. Our experiments on training the network using segmentation ground truth corrupted with different levels of annotation noise show how spatial reweighting improves the robustness of deep networks to noisy annotations.},
  archiveprefix = {arXiv},
  arxivid       = {1906.03815},
  doi           = {10.1007/978-3-030-33391-1_24},
  eprint        = {1906.03815},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mirikharaji, Yan, Hamarneh - 2019 - Learning to segment skin lesions from noisy annotations.pdf:pdf},
  isbn          = {9783030333904},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-030-33391-1_24},
}

@Article{art/WangG_201807,
  author        = {Wang, Guotai and Li, Wenqi and Zuluaga, Maria A. and Pratt, Rosalind and Patel, Premal A. and Aertsen, Michael and Doel, Tom and David, Anna L. and Deprest, Jan and Ourselin, Sebastien and Vercauteren, Tom},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Interactive Medical Image Segmentation Using Deep Learning with Image-Specific Fine Tuning}},
  year          = {2018},
  issn          = {1558254X},
  month         = {jul},
  number        = {7},
  pages         = {1562--1573},
  volume        = {37},
  abstract      = {Convolutional neural networks (CNNs) have achieved state-of-the-art performance for automatic medical image segmentation. However, they have not demonstrated sufficiently accurate and robust results for clinical use. In addition, they are limited by the lack of image-specific adaptation and the lack of generalizability to previously unseen object classes (a.k.a. zero-shot learning). To address these problems, we propose a novel deep learning-based interactive segmentation framework by incorporating CNNs into a bounding box and scribble-based segmentation pipeline. We propose image-specific fine tuning to make a CNN model adaptive to a specific test image, which can be either unsupervised (without additional user interactions) or supervised (with additional scribbles). We also propose a weighted loss function considering network and interaction-based uncertainty for the fine tuning. We applied this framework to two applications: 2-D segmentation of multiple organs from fetal magnetic resonance (MR) slices, where only two types of these organs were annotated for training and 3-D segmentation of brain tumor core (excluding edema) and whole brain tumor (including edema) from different MR sequences, where only the tumor core in one MR sequence was annotated for training. Experimental results show that: 1) our model is more robust to segment previously unseen objects than state-of-the-art CNNs; 2) image-specific fine tuning with the proposed weighted loss function significantly improves segmentation accuracy; and 3) our method leads to accurate results with fewer user interactions and less user time than traditional interactive segmentation methods.},
  archiveprefix = {arXiv},
  arxivid       = {1710.04043},
  doi           = {10.1109/TMI.2018.2791721},
  eprint        = {1710.04043},
  keywords      = {Interactive image segmentation,brain tumor,convolutional neural network,fetal MRI,fine-tuning},
  pmid          = {29969407},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
}

@InProceedings{art/SunL_201809,
  author    = {Sun, Liang and Zhang, Daoqiang and Wang, Li and Shao, Wei and Chen, Zengsi and Lin, Weili and Shen, Dinggang and Li, Gang},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Topological correction of infant cortical surfaces using anatomically constrained U-net}},
  year      = {2018},
  month     = {sep},
  pages     = {125--133},
  publisher = {Springer Verlag},
  volume    = {11046 LNCS},
  abstract  = {Reconstruction of accurate cortical surfaces with minimal topological errors (i.e., handles and holes) from infant brain MR images is important in early brain development studies. However, infant brain MR images usually exhibit extremely low tissue contrast (especially from 3 to 9 months of age) and dynamic imaging appearance patterns. Thus, it is inevitable to have large amounts of topological errors in the infant brain tissue segmentation results, thus leading to inaccurate surface reconstruction. To address these issues, inspired by recent advances in deep learning methods, we propose an anatomically constrained U-Net method for topological correction of infant cortical surfaces. Specifically, in our method, we first extract candidate voxels with potential topological errors, by leveraging a topology-preserving level set method. Then, we propose a U-Net with anatomical constraints to correct those located candidate voxels. Due to the fact that infant cortical surfaces often contain large handles or holes, it is difficult to completely correct all errors using one-shot correction. Therefore, we further gather these two steps into an iterative framework to correct large topological errors gradually. To our knowledge, this is the first work introducing deep learning for infant cortical topological correction. We compare our method with the state-of-the-art method on infant cortical topology and show the superior performance of our method.},
  doi       = {10.1007/978-3-030-00919-9_15},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2018 - Topological correction of infant cortical surfaces using anatomically constrained U-net.pdf:pdf},
  isbn      = {9783030009182},
  issn      = {16113349},
  url       = {https://doi.org/10.1007/978-3-030-00919-9_15},
}

@InProceedings{art/LauzeF_201903,
  author    = {Lauze, Francois B. and Ren, Jintao and {Moaddel H.}, Arash and Hauge, Ellen M. and Keller, Kresten K. and Jensen, Rasmus K.},
  booktitle = {Medical Imaging 2019: Computer-Aided Diagnosis},
  title     = {{Automatic detection and localization of bone erosion in hand HR-pQCT}},
  year      = {2019},
  editor    = {Hahn, Horst K. and Mori, Kensaku},
  month     = {mar},
  number    = {13},
  pages     = {74},
  publisher = {SPIE},
  volume    = {2019},
  abstract  = {{\textcopyright} 2019 SPIE. Rheumatoid arthritis (RA) is an inflammatory disease which afflicts the joints with arthritis and periarticular bone destruction as a result. One of its central features is bone erosion, a consequence of excessive bone resorption and insufficient bone formation. High-resolution peripheral quantitative computed tomography (HR-pQCT) is a promising tool for monitoring RA. Quantification of bone erosions and detection of possible progression is essential in the management of treatment. Detection is performed manually and is a very demanding task as rheumatologists must annotate hundreds of 2D images and inspect any region of the bone structure that is suspected to be a sign of RA. We propose a 2D based method which combines an accurate segmentation of bone surface boundary and classification of patches along the surface as healthy or eroded. We use a series of classical image processing methods to segment CT volumes semi-automatically. They are used as training data for a U-Net. We train a Siamese net to learn the difference between healthy and eroded patches. The Siamese net alleviates the problem of highly imbalanced class labels by providing a base for one-shot learning of differences between patches. We trained and tested the method using 3 full HR-pQCT scans with bone erosion of various size. The proposed pipeline succeeded in classifying healthy and eroded patches with high precision and recall. The proposed algorithm is a preliminary work to demonstrate the potential of our pipeline in automating the process of detecting and locating the eroded regions of bone surfaces affected by RA.},
  doi       = {10.1117/12.2512876},
  isbn      = {9781510625471},
  issn      = {1605-7422},
  keywords  = {Active contous,Bone erosion,HR-pQCT,Rheumatoid arthritis,Siamese-nets,U-nets},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10950/2512876/Automatic-detection-and-localization-of-bone-erosion-in-hand-HR/10.1117/12.2512876.full},
}

@InProceedings{art/KamnitsasK_2018,
  author        = {Kamnitsas, K. and Bai, W. and Ferrante, E. and McDonagh, S. and Sinclair, M. and Pawlowski, N. and Rajchl, M. and Lee, M. and Kainz, B. and Rueckert, D. and Glocker, B.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Ensembles of multiple models and architectures for robust brain tumour segmentation}},
  year          = {2018},
  pages         = {450--462},
  publisher     = {Springer Verlag},
  volume        = {10670 LNCS},
  abstract      = {Deep learning approaches such as convolutional neural nets have consistently outperformed previous methods on challenging tasks such as dense, semantic segmentation. However, the various proposed networks perform differently, with behaviour largely influenced by architectural choices and training settings. This paper explores Ensembles of Multiple Models and Architectures (EMMA) for robust performance through aggregation of predictions from a wide range of methods. The approach reduces the influence of the meta-parameters of individual models and the risk of overfitting the configuration to a particular database. EMMA can be seen as an unbiased, generic deep learning model which is shown to yield excellent performance, winning the first position in the BRATS 2017 competition among 50+ participating teams.},
  archiveprefix = {arXiv},
  arxivid       = {1711.01468},
  doi           = {10.1007/978-3-319-75238-9_38},
  eprint        = {1711.01468},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamnitsas et al. - 2018 - Ensembles of multiple models and architectures for robust brain tumour segmentation.pdf:pdf},
  isbn          = {9783319752372},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-319-75238-9_38},
}

@InProceedings{art/DuY_202008,
  author        = {Du, Yingjun and Xu, Jun and Xiong, Huan and Qiu, Qiang and Zhen, Xiantong and Snoek, Cees G.M. and Shao, Ling},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Learning to Learn with Variational Information Bottleneck for Domain Generalization}},
  year          = {2020},
  month         = {aug},
  pages         = {200--216},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12355 LNCS},
  abstract      = {Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.},
  archiveprefix = {arXiv},
  arxivid       = {2007.07645},
  doi           = {10.1007/978-3-030-58607-2_12},
  eprint        = {2007.07645},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - 2020 - Learning to Learn with Variational Information Bottleneck for Domain Generalization.pdf:pdf},
  isbn          = {9783030586065},
  issn          = {16113349},
  keywords      = {Domain generalization,Information bottleneck,Meta learning,Variational inference},
  url           = {https://link.springer.com/chapter/10.1007/978-3-030-58607-2_12},
}

@Article{art/DouQ_201910,
  author        = {Dou, Qi and Castro, Daniel C. and Kamnitsas, Konstantinos and Glocker, Ben},
  journal       = {arXiv},
  title         = {{Domain generalization via model-agnostic learning of semantic features}},
  year          = {2019},
  issn          = {23318422},
  month         = {oct},
  abstract      = {Generalization capability to unseen domains is crucial for machine learning models when deploying to real-world conditions. We investigate the challenging problem of domain generalization, i.e., training a model on multi-domain source data such that it can directly generalize to target domains with unknown statistics. We adopt a model-agnostic learning paradigm with gradient-based meta-train and meta-test procedures to expose the optimization to domain shift. Further, we introduce two complementary losses which explicitly regularize the semantic structure of the feature space. Globally, we align a derived soft confusion matrix to preserve general knowledge about inter-class relationships. Locally, we promote domain-independent class-specific cohesion and separation of sample features with a metric-learning component. The effectiveness of our method is demonstrated with new state-of-the-art results on two common object recognition benchmarks. Our method also shows consistent improvement on a medical image segmentation task.},
  archiveprefix = {arXiv},
  arxivid       = {1910.13580},
  eprint        = {1910.13580},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/1910.13580},
}

@Article{art/HanZ_201812,
  author    = {Han, Zhongyi and Wei, Benzheng and Mercado, Ashley and Leung, Stephanie and Li, Shuo},
  journal   = {Medical Image Analysis},
  title     = {{Spine-GAN: Semantic segmentation of multiple spinal structures}},
  year      = {2018},
  issn      = {13618423},
  month     = {dec},
  pages     = {23--35},
  volume    = {50},
  abstract  = {Spinal clinicians still rely on laborious workloads to conduct comprehensive assessments of multiple spinal structures in MRIs, in order to detect abnormalities and discover possible pathological factors. The objective of this work is to perform automated segmentation and classification (i.e., normal and abnormal) of intervertebral discs, vertebrae, and neural foramen in MRIs in one shot, which is called semantic segmentation that is extremely urgent to assist spinal clinicians in diagnosing neural foraminal stenosis, disc degeneration, and vertebral deformity as well as discovering possible pathological factors. However, no work has simultaneously achieved the semantic segmentation of intervertebral discs, vertebrae, and neural foramen due to three-fold unusual challenges: 1) Multiple tasks, i.e., simultaneous semantic segmentation of multiple spinal structures, are more difficult than individual tasks; 2) Multiple targets: average 21 spinal structures per MRI require automated analysis yet have high variety and variability; 3) Weak spatial correlations and subtle differences between normal and abnormal structures generate dynamic complexity and indeterminacy. In this paper, we propose a Recurrent Generative Adversarial Network called Spine-GAN for resolving above-aforementioned challenges. Firstly, Spine-GAN explicitly solves the high variety and variability of complex spinal structures through an atrous convolution (i.e., convolution with holes) autoencoder module that is capable of obtaining semantic task-aware representation and preserving fine-grained structural information. Secondly, Spine-GAN dynamically models the spatial pathological correlations between both normal and abnormal structures thanks to a specially designed long short-term memory module. Thirdly, Spine-GAN obtains reliable performance and efficient generalization by leveraging a discriminative network that is capable of correcting predicted errors and global-level contiguity. Extensive experiments on MRIs of 253 patients have demonstrated that Spine-GAN achieves high pixel accuracy of 96.2%, Dice coefficient of 87.1%, Sensitivity of 89.1% and Specificity of 86.0%, which reveals its effectiveness and potential as a clinical tool.},
  doi       = {10.1016/j.media.2018.08.005},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2018 - Spine-GAN Semantic segmentation of multiple spinal structures.pdf:pdf},
  keywords  = {Autoencoder,Classification,Computer-aided detection and diagnosis,Generative adversarial network,LSTM,Magnetic resonance imaging,Segmentation,Spine},
  pmid      = {30176546},
  publisher = {Elsevier B.V.},
}

@Article{art/ZhuW_201902,
  author        = {Zhu, Wentao and Huang, Yufang and Zeng, Liang and Chen, Xuming and Liu, Yong and Qian, Zhen and Du, Nan and Fan, Wei and Xie, Xiaohui},
  journal       = {Medical Physics},
  title         = {{AnatomyNet: Deep learning for fast and fully automated whole-volume segmentation of head and neck anatomy}},
  year          = {2019},
  issn          = {00942405},
  month         = {feb},
  number        = {2},
  pages         = {576--589},
  volume        = {46},
  abstract      = {Purpose: Radiation therapy (RT) is a common treatment option for head and neck (HaN) cancer. An important step involved in RT planning is the delineation of organs-at-risks (OARs) based on HaN computed tomography (CT). However, manually delineating OARs is time-consuming as each slice of CT images needs to be individually examined and a typical CT consists of hundreds of slices. Automating OARs segmentation has the benefit of both reducing the time and improving the quality of RT planning. Existing anatomy autosegmentation algorithms use primarily atlas-based methods, which require sophisticated atlas creation and cannot adequately account for anatomy variations among patients. In this work, we propose an end-to-end, atlas-free three-dimensional (3D) convolutional deep learning framework for fast and fully automated whole-volume HaN anatomy segmentation. Methods: Our deep learning model, called AnatomyNet, segments OARs from head and neck CT images in an end-to-end fashion, receiving whole-volume HaN CT images as input and generating masks of all OARs of interest in one shot. AnatomyNet is built upon the popular 3D U-net architecture, but extends it in three important ways: (a) a new encoding scheme to allow autosegmentation on whole-volume CT images instead of local patches or subsets of slices, (b) incorporating 3D squeeze-and-excitation residual blocks in encoding layers for better feature representation, and (c) a new loss function combining Dice scores and focal loss to facilitate the training of the neural model. These features are designed to address two main challenges in deep learning-based HaN segmentation: (a) segmenting small anatomies (i.e., optic chiasm and optic nerves) occupying only a few slices, and (b) training with inconsistent data annotations with missing ground truth for some anatomical structures. Results: We collected 261 HaN CT images to train AnatomyNet and used MICCAI Head and Neck Auto Segmentation Challenge 2015 as a benchmark dataset to evaluate the performance of AnatomyNet. The objective is to segment nine anatomies: brain stem, chiasm, mandible, optic nerve left, optic nerve right, parotid gland left, parotid gland right, submandibular gland left, and submandibular gland right. Compared to previous state-of-the-art results from the MICCAI 2015 competition, AnatomyNet increases Dice similarity coefficient by 3.3% on average. AnatomyNet takes about 0.12 s to fully segment a head and neck CT image of dimension 178 × 302 × 225, significantly faster than previous methods. In addition, the model is able to process whole-volume CT images and delineate all OARs in one pass, requiring little pre- or postprocessing. Conclusion: Deep learning models offer a feasible solution to the problem of delineating OARs from CT images. We demonstrate that our proposed model can improve segmentation accuracy and simplify the autosegmentation pipeline. With this method, it is possible to delineate OARs of a head and neck CT within a fraction of a second.},
  archiveprefix = {arXiv},
  arxivid       = {1808.05238},
  doi           = {10.1002/mp.13300},
  eprint        = {1808.05238},
  keywords      = {U-Net,automated anatomy segmentation,deep learning,head and neck cancer,radiation therapy},
  pmid          = {30480818},
  publisher     = {John Wiley and Sons Ltd},
  url           = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mp.13300},
}

@Article{art/ZhaoB_201911,
  author    = {Zhao, Bonan and Zhang, Xiaoshan and Li, Zheng and Hu, Xianliang},
  journal   = {Neurocomputing},
  title     = {{A multi-scale strategy for deep semantic segmentation with convolutional neural networks}},
  year      = {2019},
  issn      = {18728286},
  month     = {nov},
  pages     = {273--284},
  volume    = {365},
  abstract  = {A novel multi-scale scheme is proposed to improve the performance of deep semantic segmentation based on Convolutional Neural Networks (CNNs). The fundamental idea is to combine the information from different intermediate layers by introducing new multi-scale loss (mLoss) function. We also show that it can be calculated by three different modules. The advantage of mLoss functions is that the loss of all layers could be optimized in one-shot without additional modifications of the training algorithm. The proposed strategy is also applied to improve the performance of Unet and FCN, and the structures of multi-scale loss functions are presented as well. Numerical validations are performed on two datasets, including the benchmark Pascal VOC 2012 dataset and the PICC dataset from medical treatment. It is illustrated that our multi-scale approach yields faster learning convergence rate and better accuracy.},
  doi       = {10.1016/j.neucom.2019.07.078},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2019 - A multi-scale strategy for deep semantic segmentation with convolutional neural networks.pdf:pdf},
  keywords  = {Convolutional neural network,Markov Random Field,Multi-scale loss function,PICC,Semantic segmentation},
  publisher = {Elsevier B.V.},
}

@InProceedings{art/BlendowskiM_201910,
  author    = {Blendowski, Maximilian and Nickisch, Hannes and Heinrich, Mattias P.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{How to Learn from Unlabeled Volume Data: Self-supervised 3D Context Feature Learning}},
  year      = {2019},
  month     = {oct},
  pages     = {649--657},
  publisher = {Springer},
  volume    = {11769 LNCS},
  abstract  = {The vast majority of 3D medical images lacks detailed image-based expert annotations. The ongoing advances of deep convolutional neural networks clearly demonstrate the benefit of supervised learning to successfully extract relevant anatomical information and aid image-based analysis and interventions, but it heavily relies on labeled data. Self-supervised learning, that requires no expert labels, provides an appealing way to discover data-inherent patterns and leverage anatomical information freely available from medical images themselves. In this work, we propose a new approach to train effective convolutional feature extractors based on a new concept of image-intrinsic spatial offset relations with an auxiliary heatmap regression loss. The learned features successfully capture semantic, anatomical information and enable state-of-the-art accuracy for a k-NN based one-shot segmentation task without any subsequent fine-tuning.},
  doi       = {10.1007/978-3-030-32226-7_72},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blendowski, Nickisch, Heinrich - 2019 - How to Learn from Unlabeled Volume Data Self-supervised 3D Context Feature Learning.pdf:pdf},
  isbn      = {9783030322250},
  issn      = {16113349},
  keywords  = {Self-supervised learning,Volumetric image segmentation},
  url       = {https://doi.org/10.1007/978-3-030-32226-7_72},
}

@InProceedings{art/JoyceT_201910,
  author    = {Joyce, Thomas and Kozerke, Sebastian},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{3D medical image synthesis by factorised representation and deformable model learning}},
  year      = {2019},
  month     = {oct},
  pages     = {110--119},
  publisher = {Springer},
  volume    = {11827 LNCS},
  abstract  = {In this paper we propose a model for controllable synthesis of 3D (volumetric) medical image data. The model is comprised of three components which are learnt simultaneously from unlabelled data through self-supervision: (i) a multi-tissue anatomical model, (ii) a probability distribution over deformations of this anatomical model, and, (iii) a probability distribution over ‘renderings' of the anatomical model (where a rendering defines the relationship between anatomy and resulting pixel intensities). After training, synthetic data can be generated by sampling the deformation and rendering distributions. To encourage meaningful correspondence in the learnt anatomical model the renderer is kept simple during training, however once trained the (deformed) anatomical model provides dense multi-class segmentation masks for all training volumes, which can be used directly for state-of-the-art conditional image synthesis. This factored model based approach to data synthesis has a number of advantages: Firstly, it allows for coherent synthesis of realistic 3D data, as it is only necessary to learn low dimensional generative models (over deformations and renderings) rather than over the high dimensional 3D images themselves. Secondly, as a by-product of the anatomical model we implicitly learn a dense correspondence between all training volumes, which can be used for registration, or one-shot segmentation (through label transfer). Lastly, the factored representation allows for modality transfer (rendering one image in the modality of another), and meaningful interpolation between volumes. We demonstrate the proposed approach on cardiac MR, and multi-modal abdominal MR/CT datasets.},
  doi       = {10.1007/978-3-030-32778-1_12},
  isbn      = {9783030327774},
  issn      = {16113349},
  keywords  = {3D image synthesis,Anatomical model,Cardiac magnetic resonance,Conditional image generation,Generative model},
  url       = {https://doi.org/10.1007/978-3-030-32778-1_12},
}

@InProceedings{art/YarlagaddaD_201903,
  author    = {Yarlagadda, Dig Vijay Kumar and Rao, Praveen and Rao, Deepthi and Tawfik, Ossama},
  booktitle = {Medical Imaging 2019: Digital Pathology},
  title     = {{A system for one-shot learning of cervical cancer cell classification in histopathology images}},
  year      = {2019},
  editor    = {Tomaszewski, John E. and Ward, Aaron D.},
  month     = {mar},
  pages     = {36},
  publisher = {SPIE},
  volume    = {10956},
  abstract  = {{\textcopyright} 2019 SPIE. Convolutional neural networks (CNNs) have been popularly used to solve the problem of cell/nuclei classification and segmentation in histopathology images. Despite their pervasiveness, CNNs are fine-tuned on specific, large and labeled datasets as these datasets are hard to collect and annotate. However, this is not a scalable approach. In this work, we aim to gain deeper insights into the nature of the problem. We used a cervical cancer dataset with cells labeled into four classes by an expert pathologist. By employing pre-training on this dataset, we propose a one-shot learning model for cervical cell classification in histopathology tissue images. We extract regional maximum activation of convolutions (R-MAC) global descriptors and train a one-shot learning memory module with the goal of using it for various cancer types and eliminate the need for expensive, difficult to collect, large, labeled whole slide image (WSI) datasets. Our model achieved 94.6% accuracy in detecting the four cell classes on the test dataset. Further, we present our analysis of the dataset and features to better understand and visualize the problem in general.},
  doi       = {10.1117/12.2512963},
  isbn      = {9781510625594},
  issn      = {16057422},
  keywords  = {One-shot learning,cervical cancer,whole slide images},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10956/2512963/A-system-for-one-shot-learning-of-cervical-cancer-cell/10.1117/12.2512963.full},
}

@InProceedings{art/XuZ_201910,
  author        = {Xu, Zhenlin and Niethammer, Marc},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{DeepAtlas: Joint Semi-supervised Learning of Image Registration and Segmentation}},
  year          = {2019},
  month         = {oct},
  pages         = {420--429},
  publisher     = {Springer},
  volume        = {11765 LNCS},
  abstract      = {Deep convolutional neural networks (CNNs) are state-of-the-art for semantic image segmentation, but typically require many labeled training samples. Obtaining 3D segmentations of medical images for supervised training is difficult and labor intensive. Motivated by classical approaches for joint segmentation and registration we therefore propose a deep learning framework that jointly learns networks for image registration and image segmentation. In contrast to previous work on deep unsupervised image registration, which showed the benefit of weak supervision via image segmentations, our approach can use existing segmentations when available and computes them via the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training benefits from the registration, which essentially provides a realistic form of data augmentation. Experiments on knee and brain 3D magnetic resonance (MR) images show that our approach achieves large simultaneous improvements of segmentation and registration accuracy (over independently trained networks) and allows training high-quality models with very limited training data. Specifically, in a one-shot-scenario (with only one manually labeled image) our approach increases Dice scores (%) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images respectively.},
  archiveprefix = {arXiv},
  arxivid       = {1904.08465},
  doi           = {10.1007/978-3-030-32245-8_47},
  eprint        = {1904.08465},
  isbn          = {9783030322441},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-030-32245-8_47},
}

@InProceedings{art/RutterE_201910,
  author    = {Rutter, Erica M. and Lagergren, John H. and Flores, Kevin B.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{A convolutional neural network method for boundary optimization enables few-shot learning for biomedical image segmentation}},
  year      = {2019},
  month     = {oct},
  pages     = {190--198},
  publisher = {Springer},
  volume    = {11795 LNCS},
  abstract  = {Obtaining large amounts of annotated biomedical data to train convolutional neural networks (CNNs) for image segmentation is expensive. We propose a method that requires only a few segmentation examples to accurately train a semi-automated segmentation algorithm. Our algorithm, a convolutional neural network method for boundary optimization (CoMBO), can be used to rapidly outline object boundaries using orders of magnitude less annotation than full segmentation masks, i.e., only a few pixels per image. We found that CoMBO is significantly more accurate than state-of-the-art machine learning methods such as Mask R-CNN. We also show how we can use CoMBO predictions, when CoMBO is trained on just 3 images, to rapidly create large amounts of accurate training data for Mask R-CNN. Our few-shot method is demonstrated on ISBI cell tracking challenge datasets.},
  doi       = {10.1007/978-3-030-33391-1_22},
  isbn      = {9783030333904},
  issn      = {16113349},
  keywords  = {Biomedical image segmentation,Convolutional neural network,Few shot learning},
  url       = {https://doi.org/10.1007/978-3-030-33391-1_22},
}

@Article{art/DongZ_202010,
  author    = {Dong, Zihao and Zhang, Ruixun and Shao, Xiuli and Kuang, Zengsheng},
  journal   = {Knowledge-Based Systems},
  title     = {{Learning sparse features with lightweight ScatterNet for small sample training}},
  year      = {2020},
  issn      = {09507051},
  month     = {oct},
  pages     = {106315},
  volume    = {205},
  abstract  = {Convolutional neural networks (CNNs) have recently achieved impressive performances in image processing tasks such as image classification and object recognition. However, CNNs typically have a large number of parameters, leading to their requirement of a large number of training samples to extract spatial features. To address these limitations, we propose a lightweight ScatterNet with the learnable weight matrix and sparse transformation such as scale transformation and translation to learn sparse filters. This filter based on ScatterNet uses He initialization algorithm and learns from input images which are viewed as two-directional sequential data in the initial stage of model training. A Strip-Recurrent module sweeps both horizontally and vertically across the image to compress feature matrices. Then, ScatterNet decomposes the above feature matrices as a learned mixture of different harmonic functions to integrate the spectral analysis into CNNs. Finally, we combine the sequential and spectral features to build our hybrid architectures to complete image classification and segmentation. These architectures can obtain good classification accuracy on both small and large training datasets. Our proposed method is evaluated at both layer and network levels on five widely-used benchmark datasets: MNIST, CIFAR-10, CIFAR-100, Small NORB and Tiny ImageNet. We also study other small sample problems such as medical image segmentation and image classification based on few-shot learning. Experiments show that our proposed layer and hybrid model achieves better accuracy for small sample training.},
  doi       = {10.1016/j.knosys.2020.106315},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong et al. - 2020 - Learning sparse features with lightweight ScatterNet for small sample training.pdf:pdf},
  keywords  = {Hybrid architecture,Learnable filters,Lightweight,ScatterNet,Sparse features},
  publisher = {Elsevier B.V.},
}

@Article{art/ShenT_202012,
  author    = {Shen, Tianyu and Wang, Jiangong and Gou, Chao and Wang, Fei Yue},
  journal   = {IEEE Transactions on Fuzzy Systems},
  title     = {{Hierarchical Fused Model with Deep Learning and Type-2 Fuzzy Learning for Breast Cancer Diagnosis}},
  year      = {2020},
  issn      = {19410034},
  month     = {dec},
  number    = {12},
  pages     = {3204--3218},
  volume    = {28},
  abstract  = {Breast cancer diagnosis based on medical imaging necessitates both fine-grained lesion segmentation and disease grading. Although deep learning (DL) offers an emerging and powerful paradigm of feature learning for these two tasks, it is hampered from popularizing in practical application due to the lack of interpretability, generalization ability, and large labeled training sets. In this article, we propose a hierarchical fused model based on DL and fuzzy learning to overcome the drawbacks for pixelwise segmentation and disease grading of mammography breast images. The proposed system consists of a segmentation model (ResU-segNet) and a hierarchical fuzzy classifier (HFC) that is a fusion of interval type-2 possibilistic fuzzy c-means and fuzzy neural network. The ResU-segNet segments the masks of mass regions from the images through convolutional neural networks, while the HFC encodes the features from mass images and masks to obtain the disease grading through fuzzy representation and rule-based learning. Through the integration of feature extraction aided by domain knowledge and fuzzy learning, the system achieves favorable performance in a few-shot learning manner, and the deterioration of cross-dataset generalization ability is alleviated. In addition, the interpretability is further enhanced. The effectiveness of the proposed system is analyzed on the publicly available mammogram database of INbreast and a private database through cross-validation. Thorough comparative experiments are also conducted and demonstrated.},
  doi       = {10.1109/TFUZZ.2020.3013681},
  keywords  = {Breast cancer,deep learning (DL),fuzzy classifier (FC),interval type-2 possibilistic fuzzy c-means (IT2PF},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@InProceedings{art/ShenZ_202010,
  author        = {Shen, Zhengyang and Xu, Zhenlin and Olut, Sahin and Niethammer, Marc},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Anatomical Data Augmentation via Fluid-Based Image Registration}},
  year          = {2020},
  month         = {oct},
  pages         = {318--328},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12263 LNCS},
  abstract      = {We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.},
  archiveprefix = {arXiv},
  arxivid       = {2007.02447},
  doi           = {10.1007/978-3-030-59716-0_31},
  eprint        = {2007.02447},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen et al. - 2020 - Anatomical Data Augmentation via Fluid-Based Image Registration.pdf:pdf},
  isbn          = {9783030597153},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-030-59716-0_31},
}

@InProceedings{art/DingW_202010,
  author        = {Ding, Wangbin and Li, Lei and Zhuang, Xiahai and Huang, Liqin},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Cross-Modality Multi-atlas Segmentation Using Deep Neural Networks}},
  year          = {2020},
  month         = {oct},
  pages         = {233--242},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12263 LNCS},
  abstract      = {Both image registration and label fusion in the multi-atlas segmentation (MAS) rely on the intensity similarity between target and atlas images. However, such similarity can be problematic when target and atlas images are acquired using different imaging protocols. High-level structure information can provide reliable similarity measurement for cross-modality images when cooperating with deep neural networks (DNNs). This work presents a new MAS framework for cross-modality images, where both image registration and label fusion are achieved by DNNs. For image registration, we propose a consistent registration network, which can jointly estimate forward and backward dense displacement fields (DDFs). Additionally, an invertible constraint is employed in the network to reduce the correspondence ambiguity of the estimated DDFs. For label fusion, we adapt a few-shot learning network to measure the similarity of atlas and target patches. Moreover, the network can be seamlessly integrated into the patch-based label fusion. The proposed framework is evaluated on the MM-WHS dataset of MICCAI 2017. Results show that the framework is effective in both cross-modality registration and segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {2008.08946},
  doi           = {10.1007/978-3-030-59716-0_23},
  eprint        = {2008.08946},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2020 - Cross-Modality Multi-atlas Segmentation Using Deep Neural Networks.pdf:pdf},
  isbn          = {9783030597153},
  issn          = {16113349},
  keywords      = {Cross-modality,MAS,Similarity},
  url           = {https://doi.org/10.1007/978-3-030-59716-0_23},
}

@InProceedings{art/LuY_202010,
  author        = {Lu, Yuhang and Li, Weijian and Zheng, Kang and Wang, Yirui and Harrison, Adam P. and Lin, Chihung and Wang, Song and Xiao, Jing and Lu, Le and Kuo, Chang Fu and Miao, Shun},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Learning to segment anatomical structures accurately from one exemplar}},
  year          = {2020},
  month         = {oct},
  pages         = {678--688},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12261 LNCS},
  abstract      = {Accurate segmentation of critical anatomical structures is at the core of medical image analysis. The main bottleneck lies in gathering the requisite expert-labeled image annotations in a scalable manner. Methods that permit to produce accurate anatomical structure segmentation without using a large amount of fully annotated training images are highly desirable. In this work, we propose a novel contribution of Contour Transformer Network (CTN), a one-shot anatomy segmentor including a naturally built-in human-in-the-loop mechanism. Segmentation is formulated by learning a contour evolution behavior process based on graph convolutional networks (GCN). Training of our CTN model requires only one labeled image exemplar and leverages additional unlabeled data through newly introduced loss functions that measure the global shape and appearance consistency of contours. We demonstrate that our one-shot learning method significantly outperforms non-learning-based methods and performs competitively to the state-of-the-art fully supervised deep learning approaches. With minimal human-in-the-loop editing feedback, the segmentation performance can be further improved and tailored towards the observer desired outcomes. This can facilitate the clinician designed imaging-based biomarker assessments (to support personalized quantitative clinical diagnosis) and outperforms fully supervised baselines.},
  archiveprefix = {arXiv},
  arxivid       = {2007.03052},
  doi           = {10.1007/978-3-030-59710-8_66},
  eprint        = {2007.03052},
  isbn          = {9783030597092},
  issn          = {16113349},
  keywords      = {Contour Transformer Network,Graph convolutional network,One-shot segmentation},
  url           = {https://doi.org/10.1007/978-3-030-59710-8_66},
}

@InProceedings{art/LiuQ_202010,
  author        = {Liu, Quande and Dou, Qi and Heng, Pheng Ann},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Shape-Aware Meta-learning for Generalizing Prostate MRI Segmentation to Unseen Domains}},
  year          = {2020},
  month         = {oct},
  pages         = {475--485},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12262 LNCS},
  abstract      = {Model generalization capacity at domain shift (e.g., various imaging protocols and scanners) is crucial for deep learning methods in real-world clinical deployment. This paper tackles the challenging problem of domain generalization, i.e., learning a model from multi-domain source data such that it can directly generalize to an unseen target domain. We present a novel shape-aware meta-learning scheme to improve the model generalization in prostate MRI segmentation. Our learning scheme roots in the gradient-based meta-learning, by explicitly simulating domain shift with virtual meta-train and meta-test during training. Importantly, considering the deficiencies encountered when applying a segmentation model to unseen domains (i.e., incomplete shape and ambiguous boundary of the prediction masks), we further introduce two complementary loss objectives to enhance the meta-optimization, by particularly encouraging the shape compactness and shape smoothness of the segmentations under simulated domain shift. We evaluate our method on prostate MRI data from six different institutions with distribution shifts acquired from public datasets. Experimental results show that our approach outperforms many state-of-the-art generalization methods consistently across all six settings of unseen domains (Code and dataset are available at https://github.com/liuquande/SAML).},
  archiveprefix = {arXiv},
  arxivid       = {2007.02035},
  doi           = {10.1007/978-3-030-59713-9_46},
  eprint        = {2007.02035},
  isbn          = {9783030597122},
  issn          = {16113349},
  keywords      = {Domain generalization,Meta-learning,Prostate MRI segmentation},
  url           = {https://doi.org/10.1007/978-3-030-59713-9_46},
}

@InProceedings{art/WangJ_202010,
  author        = {Wang, Jixin and Zhou, Sanping and Fang, Chaowei and Wang, Le and Wang, Jinjun},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Meta corrupted pixels mining for medical image segmentation}},
  year          = {2020},
  month         = {oct},
  pages         = {335--345},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12261 LNCS},
  abstract      = {Deep neural networks have achieved satisfactory performance in piles of medical image analysis tasks. However the training of deep neural network requires a large amount of samples with high-quality annotations. In medical image segmentation, it is very laborious and expensive to acquire precise pixel-level annotations. Aiming at training deep segmentation models on datasets with probably corrupted annotations, we propose a novel Meta Corrupted Pixels Mining (MCPM) method based on a simple meta mask network. Our method is targeted at automatically estimate a weighting map to evaluate the importance of every pixel in the learning of segmentation network. The meta mask network which regards the loss value map of the predicted segmentation results as input, is capable of identifying out corrupted layers and allocating small weights to them. An alternative algorithm is adopted to train the segmentation network and the meta mask network, simultaneously. Extensive experimental results on LIDC-IDRI and LiTS datasets show that our method outperforms state-of-the-art approaches which are devised for coping with corrupted annotations.},
  archiveprefix = {arXiv},
  arxivid       = {2007.03538},
  doi           = {10.1007/978-3-030-59710-8_33},
  eprint        = {2007.03538},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Meta corrupted pixels mining for medical image segmentation.pdf:pdf},
  isbn          = {9783030597092},
  issn          = {16113349},
  keywords      = {Deep neural network,Medical image segmentation,Meta Corrupted Pixels Mining},
  url           = {https://doi.org/10.1007/978-3-030-59710-8_33},
}

@InProceedings{art/SonsbeekT_202010,
  author        = {van Sonsbeek, Tom and Cheplygina, Veronika},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Predicting Scores of Medical Imaging Segmentation Methods with Meta-learning}},
  year          = {2020},
  month         = {oct},
  pages         = {242--253},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12446 LNCS},
  abstract      = {Deep learning has led to state-of-the-art results for many medical imaging tasks, such as segmentation of different anatomical structures. With the increased numbers of deep learning publications and openly available code, the approach to choosing a model for a new task becomes more complicated, while time and (computational) resources are limited. A possible solution to choosing a model efficiently is meta-learning, a learning method in which prior performance of a model is used to predict the performance for new tasks. We investigate meta-learning for segmentation across ten datasets of different organs and modalities. We propose four ways to represent each dataset by meta-features: one based on statistical features of the images and three are based on deep learning features. We use support vector regression and deep neural networks to learn the relationship between the meta-features and prior model performance. On three external test datasets these methods give Dice scores within 0.10 of the true performance. These results demonstrate the potential of meta-learning in medical imaging.},
  archiveprefix = {arXiv},
  arxivid       = {2005.08869},
  doi           = {10.1007/978-3-030-61166-8_26},
  eprint        = {2005.08869},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Sonsbeek, Cheplygina - 2020 - Predicting Scores of Medical Imaging Segmentation Methods with Meta-learning.pdf:pdf},
  isbn          = {9783030611651},
  issn          = {16113349},
  keywords      = {Feature extraction,Meta-learning,Segmentation},
  url           = {https://doi.org/10.1007/978-3-030-61166-8_26},
}

@Article{art/CuiH_2021,
  author        = {Cui, Hengji and Wei, Dong and Ma, Kai and Gu, Shi and Zheng, Yefeng},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{A Unified Framework for Generalized Low-Shot Medical Image Segmentation with Scarce Data}},
  year          = {2021},
  issn          = {1558254X},
  number        = {10},
  pages         = {2656--2671},
  volume        = {40},
  abstract      = {Medical image segmentation has achieved remarkable advancements using deep neural networks (DNNs). However, DNNs often need big amounts of data and annotations for training, both of which can be difficult and costly to obtain. In this work, we propose a unified framework for generalized low-shot (one-and few-shot) medical image segmentation based on distance metric learning (DML). Unlike most existing methods which only deal with the lack of annotations while assuming abundance of data, our framework works with extreme scarcity of both, which is ideal for rare diseases. Via DML, the framework learns a multimodal mixture representation for each category, and performs dense predictions based on cosine distances between the pixels' deep embeddings and the category representations. The multimodal representations effectively utilize the inter-subject similarities and intraclass variations to overcome overfitting due to extremely limited data. In addition, we propose adaptive mixing coefficients for the multimodal mixture distributions to adaptively emphasize the modes better suited to the current input. The representations are implicitly embedded as weights of the fc layer, such that the cosine distances can be computed efficiently via forward propagation. In our experiments on brain MRI and abdominal CT datasets, the proposed framework achieves superior performances for low-shot segmentation towards standard DNN-based (3D U-Net) and classical registration-based (ANTs) methods, e.g., achieving mean Dice coefficients of 81%/69% for brain tissue/abdominal multi-organ segmentation using a single training sample, as compared to 52%/31% and 72%/35% by the U-Net and ANTs, respectively.},
  archiveprefix = {arXiv},
  arxivid       = {2110.09260},
  doi           = {10.1109/TMI.2020.3045775},
  eprint        = {2110.09260},
  keywords      = {Semantic segmentation,adaptive mixing coefficients,distance metric learning,low-shot learning,multimodal representation},
  pmid          = {33338014},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
}

@InProceedings{art/HeY_202008,
  author        = {He, Yuting and Li, Tiantian and Yang, Guanyu and Kong, Youyong and Chen, Yang and Shu, Huazhong and Coatrieux, Jean Louis and Dillenseger, Jean Louis and Li, Shuo},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Deep Complementary Joint Model for Complex Scene Registration and Few-Shot Segmentation on Medical Images}},
  year          = {2020},
  month         = {aug},
  pages         = {770--786},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12363 LNCS},
  abstract      = {Deep learning-based medical image registration and segmentation joint models utilize the complementarity (augmentation data or weakly supervised data from registration, region constraints from segmentation) to bring mutual improvement in complex scene and few-shot situation. However, further adoption of the joint models are hindered: 1) the diversity of augmentation data is reduced limiting the further enhancement of segmentation, 2) misaligned regions in weakly supervised data disturb the training process, 3) lack of label-based region constraints in few-shot situation limits the registration performance. We propose a novel Deep Complementary Joint Model (DeepRS) for complex scene registration and few-shot segmentation. We embed a perturbation factor in the registration to increase the activity of deformation thus maintaining the augmentation data diversity. We take a pixel-wise discriminator to extract alignment confidence maps which highlight aligned regions in weakly supervised data so the misaligned regions' disturbance will be suppressed via weighting. The outputs from segmentation model are utilized to implement deep-based region constraints thus relieving the label requirements and bringing fine registration. Extensive experiments on the CT dataset of MM-WHS 2017 Challenge[42] show great advantages of our DeepRS that outperforms the existing state-of-the-art models.},
  archiveprefix = {arXiv},
  arxivid       = {2008.00710},
  doi           = {10.1007/978-3-030-58523-5_45},
  eprint        = {2008.00710},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2020 - Deep Complementary Joint Model for Complex Scene Registration and Few-Shot Segmentation on Medical Images.pdf:pdf},
  isbn          = {9783030585228},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-030-58523-5_45},
}

@Article{art/ChenX_202003,
  author    = {Chen, Xu and Xia, James J. and Shen, Dinggang and Lian, Chunfeng and Wang, Li and Deng, Hannah and Fung, Steve H. and Nie, Dong and Thung, Kim Han and Yap, Pew Thian and Gateno, Jaime},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{One-shot generative adversarial learning for MRI segmentation of craniomaxillofacial bony structures}},
  year      = {2020},
  issn      = {1558254X},
  month     = {mar},
  number    = {3},
  pages     = {787--796},
  volume    = {39},
  abstract  = {Compared to computed tomography (CT), magnetic resonance imaging (MRI) delineation of craniomaxillofacial (CMF) bony structures can avoid harmful radiation exposure. However, bony boundaries are blurry in MRI, and structural information needs to be borrowed from CT during the training. This is challenging since paired MRI-CT data are typically scarce. In this paper, we propose to make full use of unpaired data, which are typically abundant, along with a single paired MRI-CT data to construct a one-shot generative adversarial model for automated MRI segmentation of CMF bony structures. Our model consists of a cross-modality image synthesis sub-network, which learns the mapping between CT and MRI, and an MRI segmentation sub-network. These two sub-networks are trained jointly in an end-to-end manner. Moreover, in the training phase, a neighbor-based anchoring method is proposed to reduce the ambiguity problem inherent in cross-modality synthesis, and a feature-matching-based semantic consistency constraint is proposed to encourage segmentation-oriented MRI synthesis. Experimental results demonstrate the superiority of our method both qualitatively and quantitatively in comparison with the state-of-the-art MRI segmentation methods.},
  doi       = {10.1109/TMI.2019.2935409},
  keywords  = {Craniomaxillofacial bone segmentation,MRI,generative adversarial learning,one-shot learning},
  pmid      = {31425025},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@InProceedings{art/WangS_2020,
  author        = {Wang, Shuxin and Cao, Shilei and Wei, Dong and Wang, Renzhen and Ma, Kai and Wang, Liansheng and Meng, Deyu and Zheng, Yefeng},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{LT-net: Label transfer by learning reversible voxel-wise correspondence for one-shot medical image segmentation}},
  year          = {2020},
  pages         = {9159--9168},
  publisher     = {IEEE Computer Society},
  abstract      = {We introduce a one-shot segmentation method to alleviate the burden of manual annotation for medical images. The main idea is to treat one-shot segmentation as a classical atlas-based segmentation problem, where voxel-wise correspondence from the atlas to the unlabelled data is learned. Subsequently, segmentation label of the atlas can be transferred to the unlabelled data with the learned correspondence. However, since ground truth correspondence between images is usually unavailable, the learning system must be well-supervised to avoid mode collapse and convergence failure. To overcome this difficulty, we resort to the forward-backward consistency, which is widely used in correspondence problems, and additionally learn the backward correspondences from the warped atlases back to the original atlas. This cycle-correspondence learning design enables a variety of extra, cycle-consistency-based supervision signals to make the training process stable, while also boost the performance. We demonstrate the superiority of our method over both deep learning-based one-shot segmentation methods and a classical multi-atlas segmentation method via thorough experiments.},
  archiveprefix = {arXiv},
  arxivid       = {2003.07072},
  doi           = {10.1109/CVPR42600.2020.00918},
  eprint        = {2003.07072},
  issn          = {10636919},
}

@Article{art/ChantiD_2021,
  author        = {Chanti, Dawood Al and Duque, Vanessa Gonzalez and Crouzier, Marion and Nordez, Antoine and Lacourpaille, Lilian and Mateus, Diana},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{IFSS-Net: Interactive Few-Shot Siamese Network for Faster Muscle Segmentation and Propagation in Volumetric Ultrasound}},
  year          = {2021},
  issn          = {1558254X},
  abstract      = {We present an accurate, fast and efficient method for segmentation and muscle mask propagation in 3D freehand ultrasound data, towards accurate volume quantification. A deep Siamese 3D Encoder-Decoder network that captures the evolution of the muscle appearance and shape for contiguous slices is deployed. We use it to propagate a reference mask annotated by a clinical expert. To handle longer changes of the muscle shape over the entire volume and to provide an accurate propagation, we devise a Bidirectional Long Short Term Memory module. Also, to train our model with a minimal amount of training samples, we propose a strategy combining learning from few annotated 2D ultrasound slices with sequential pseudo-labelling of the unannotated slices. We introduce a decremental update of the objective function to guide the model convergence in the absence of large amounts of annotated data. After training with a few volumes, the decremental update strategy switches from a weak supervised training to a few-shot setting. Finally, to handle the class-imbalance between foreground and background muscle pixels, we propose a parametric Tversky loss function that learns to penalize adaptively the false positives and the false negatives. We validate our approach for the segmentation, label propagation, and volume computation of the three low-limb muscles on a dataset of 61600 images from 44 subjects. We achieve a Dice score coefficient of over 95 % and a volumetric error of 1.6035&#x00B1;0.587%.},
  archiveprefix = {arXiv},
  arxivid       = {2011.13246},
  doi           = {10.1109/TMI.2021.3058303},
  eprint        = {2011.13246},
  keywords      = {3D Ultrasound,Annotations,Few-Shot annotation,Image segmentation,Mask propagation,Muscles,Pseudo-labelling,Segmentation,Task analysis,Three-dimensional displays,Two dimensional displays,Ultrasonic imaging},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/CaoZ_2019,
  author    = {Cao, Zhiying and Zhang, Tengfei and DIao, Wenhui and Zhang, Yue and Lyu, Xiaode and Fu, Kun and Sun, Xian},
  journal   = {IEEE Access},
  title     = {{Meta-Seg: A Generalized Meta-Learning Framework for Multi-Class Few-Shot Semantic Segmentation}},
  year      = {2019},
  issn      = {21693536},
  pages     = {166109--166121},
  volume    = {7},
  abstract  = {Semantic segmentation performs pixel-wise classification for given images, which can be widely used in autonomous driving, robotics, medical diagnostics and etc. The recent advanced approaches have witnessed rapid progress in semantic segmentation. However, these supervised learning based methods rely heavily on large-scale datasets to acquire strong generalizing ability, such that they are coupled with some constraints. Firstly, human annotation of pixel-level segmentation masks is laborious and time-consuming, which causes relatively expensive training data and make it hard to deal with urgent tasks in dynamic environment. Secondly, the outstanding performance of the above data-hungry methods will decrease with few available training examples. In order to overcome the limitations of the supervised learning semantic segmentation methods, this paper proposes a generalized meta-learning framework, named Meta-Seg. It consists of a meta-learner and a base-learner. Specifically, the meta-learner learns a good initialization and a parameter update strategy from a distribution of few-shot semantic segmentation tasks. The base-learner can be any semantic segmentation models theoretically and can implement fast adaptation (that is updating parameters with few iterations) under the guidance of the meta-learner. In this work, the successful semantic segmentation model FCN8s is integrated into Meta-Seg. Experiments on the famous few-shot semantic segmentation dataset PASCAL 5i prove Meta-Seg is a promising framework for few-shot semantic segmentation. Besides, this method can provide with reference for the relevant researches of meta-learning semantic segmentation.},
  doi       = {10.1109/ACCESS.2019.2953465},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2019 - Meta-Seg A Generalized Meta-Learning Framework for Multi-Class Few-Shot Semantic Segmentation.pdf:pdf},
  keywords  = {Meta-learning,few-shot,semantic segmentation},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@InProceedings{art/RaviS_2017,
  author    = {Ravi, Sachin and Larochelle, Hugo},
  booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  title     = {{Optimization as a model for few-shot learning}},
  year      = {2017},
  abstract  = {Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a classifier has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity classifiers requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.},
}

@TechReport{art/FinnC_201707,
  author    = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  title     = {{Model-agnostic meta-learning for fast adaptation of deep networks}},
  year      = {2017},
  month     = {jul},
  abstract  = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  booktitle = {arXiv},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - 2017 - Model-agnostic meta-learning for fast adaptation of deep networks.pdf:pdf},
  issn      = {23318422},
  pages     = {1126--1135},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v70/finn17a.html},
}

@InProceedings{art/PatraA_2020,
  author    = {Patra, Arijit and Noble, J. Alison},
  booktitle = {Communications in Computer and Information Science},
  title     = {{Incremental Learning of Fetal Heart Anatomies Using Interpretable Saliency Maps}},
  year      = {2020},
  pages     = {129--141},
  publisher = {Springer},
  volume    = {1065 CCIS},
  abstract  = {While medical image analysis has seen extensive use of deep neural networks, learning over multiple tasks is a challenge for connectionist networks due to tendencies of degradation in performance over old tasks while adapting to novel tasks. It is pertinent that adaptations to new data distributions over time are tractable with automated analysis methods as medical imaging data acquisition is typically not a static problem. So, one needs to ensure that a continual learning paradigm be ensured in machine learning methods deployed for medical imaging. To explore interpretable lifelong learning for deep neural networks in medical imaging, we introduce a perspective of understanding forgetting in neural networks used in ultrasound image analysis through the notions of attention and saliency. Concretely, we propose quantification of forgetting as a decline in the quality of class specific saliency maps after each subsequent task schedule. We also introduce a knowledge transfer from past tasks to present by a saliency guided retention of past exemplars which improve the ability to retain past knowledge while optimizing parameters for current tasks. Experiments on a clinical fetal echocardiography dataset demonstrate a state-of-the-art performance for our protocols.},
  doi       = {10.1007/978-3-030-39343-4_11},
  isbn      = {9783030393427},
  issn      = {18650937},
  keywords  = {Continual learning,Interpretability,Saliency},
}

@Article{art/BenavidesPradoD_202005,
  author    = {Benavides-Prado, Diana and Koh, Yun Sing and Riddle, Patricia},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {{Towards Knowledgeable Supervised Lifelong Learning Systems}},
  year      = {2020},
  issn      = {1076-9757},
  month     = {may},
  pages     = {159--224},
  volume    = {68},
  abstract  = {Learning a sequence of tasks is a long-standing challenge in machine learning. This setting applies to learning systems that observe examples of a range of tasks at different points in time. A learning system should become more knowledgeable as more related tasks are learned. Although the problem of learning sequentially was acknowledged for the first time decades ago, the research in this area has been rather limited. Research in transfer learning, multitask learning, metalearning and deep learning has studied some challenges of these kinds of systems. Recent research in lifelong machine learning and continual learning has revived interest in this problem. We propose Proficiente, a full framework for long-term learning systems. Proficiente relies on knowledge transferred between hypotheses learned with Support Vector Machines. The first component of the framework is focused on transferring forward selectively from a set of existing hypotheses or functions representing knowledge acquired during previous tasks to a new target task. A second component of Proficiente is focused on transferring backward, a novel ability of long-term learning systems that aim to exploit knowledge derived from recent tasks to encourage refinement of existing knowledge. We propose a method that transfers selectively from a task learned recently to existing hypotheses representing previous tasks. The method encourages retention of existing knowledge whilst refining. We analyse the theoretical properties of the proposed framework. Proficiente is accompanied by an agnostic metric that can be used to determine if a long-term learning system is becoming more knowledgeable. We evaluate Proficiente in both synthetic and real-world datasets, and demonstrate scenarios where knowledgeable supervised learning systems can be achieved by means of transfer.},
  doi       = {10.1613/jair.1.11432},
  publisher = {AI Access Foundation},
}

@InProceedings{art/BaeH_202010,
  author    = {Bae, Heechul and Song, Soonyong and Park, Junhee},
  booktitle = {International Conference on ICT Convergence},
  title     = {{The Present and Future of Continual Learning}},
  year      = {2020},
  month     = {oct},
  pages     = {1193--1195},
  publisher = {IEEE Computer Society},
  volume    = {2020-Octob},
  abstract  = {This paper addresses a continual lifelong learning problem that learns incremental multiple tasks in real-world environments. We overview and summarize representative approaches and categorization of the state-of-the-art in continual learning. Comparable scenarios, benchmark datasets, and baseline approaches for different continual scenarios introduced in this paper. We suggested a comparison of the differences and similarities with other machine learning methods. We also report real-world applications, especially robots and healthcare fields. We summarize current states and suggest future direction of continual learning problems.},
  doi       = {10.1109/ICTC49870.2020.9289549},
  isbn      = {9781728167589},
  issn      = {21621241},
  keywords  = {catastrophic forgetting,continual lifelong learning,incremental multi-task classification},
}

@Misc{art/LuoY_202011,
  author    = {Luo, Yong and Yin, Liancheng and Bai, Wenchao and Mao, Keming},
  month     = {nov},
  title     = {{An appraisal of incremental learning methods}},
  year      = {2020},
  abstract  = {As a special case of machine learning, incremental learning can acquire useful knowledge from incoming data continuously while it does not need to access the original data. It is expected to have the ability of memorization and it is regarded as one of the ultimate goals of artificial intelligence technology. However, incremental learning remains a long term challenge. Modern deep neural network models achieve outstanding performance on stationary data distributions with batch training. This restriction leads to catastrophic forgetting for incremental learning scenarios since the distribution of incoming data is unknown and has a highly different probability from the old data. Therefore, a model must be both plastic to acquire new knowledge and stable to consolidate existing knowledge. This review aims to draw a systematic review of the state of the art of incremental learning methods. Published reports are selected from Web of Science, IEEEXplore, and DBLP databases up to May 2020. Each paper is reviewed according to the types: architectural strategy, regularization strategy and rehearsal and pseudo-rehearsal strategy. We compare and discuss different methods. Moreover, the development trend and research focus are given. It is concluded that incremental learning is still a hot research area and will be for a long period. More attention should be paid to the exploration of both biological systems and computational models.},
  booktitle = {Entropy},
  doi       = {10.3390/e22111190},
  issn      = {10994300},
  keywords  = {Catastrophic forgetting,Incremental learning,Lifelong learning},
  number    = {11},
  pages     = {1--27},
  publisher = {MDPI AG},
  volume    = {22},
}

@Misc{art/ShusterK_202008,
  author        = {Shuster, Kurt and Urbanek, Jack and Dinan, Emily and Szlam, Arthur and Weston, Jason},
  month         = {aug},
  title         = {{Deploying lifelong open-domain dialogue learning}},
  year          = {2020},
  abstract      = {Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect.},
  archiveprefix = {arXiv},
  arxivid       = {2008.08076},
  booktitle     = {arXiv},
  eprint        = {2008.08076},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shuster et al. - 2020 - Deploying lifelong open-domain dialogue learning.pdf:pdf},
  issn          = {23318422},
  publisher     = {arXiv},
  url           = {https://arxiv.org/abs/2008.08076v2},
}

@InProceedings{art/YeF_202011,
  author    = {Ye, Fei and Bors, Adrian G.},
  booktitle = {2020 10th International Conference on Image Processing Theory, Tools and Applications, IPTA 2020},
  title     = {{Lifelong learning of interpretable image representations}},
  year      = {2020},
  month     = {nov},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  abstract  = {Existing machine learning systems are trained to adapt to a single database and their ability to acquire additional information is limited. Catastrophic forgetting occurs in all deep learning systems when attempting to train them with additional databases. The information learnt previously is forgotten and no longer recognized when such a learning systems is trained using a new database. In this paper, we develop a new image generation approach defined under the lifelong learning framework which prevents forgetting. We employ the mutual information maximization between the latent variable space and the outputs of the generator network in order to learn interpretable representations, when learning using the data from a series of databases sequentially. We also provide the theoretical framework for the generative replay mechanism, under the lifelong learning setting. We perform a series of experiments showing that the proposed approach is able to learn a set of disjoint data distributions in a sequential manner while also capturing meaningful data representations across domains.},
  doi       = {10.1109/IPTA50016.2020.9286663},
  isbn      = {9781728187501},
  keywords  = {Generative Adversarial Networks,Lifelong learning,Mutual information,Representation learning},
}

@Article{art/PanJ_202004,
  author        = {Pan, Jing and Pham, Vincent and Dorairaj, Mohan and Chen, Huigang and Lee, Jeong-Yoon},
  title         = {{Adversarial Validation Approach to Concept Drift Problem in User Targeting Automation Systems at Uber}},
  year          = {2020},
  month         = {apr},
  volume        = {20},
  abstract      = {In user targeting automation systems, concept drift in input data is one of the main challenges. It deteriorates model performance on new data over time. Previous research on concept drift mostly proposed model retraining after observing performance decreases. However, this approach is suboptimal because the system fixes the problem only after suffering from poor performance on new data. Here, we introduce an adversarial validation approach to concept drift problems in user targeting automation systems. With our approach, the system detects concept drift in new data before making inference, trains a model, and produces predictions adapted to the new data. We show that our approach addresses concept drift effectively with the AutoML3 Lifelong Machine Learning challenge data as well as in Uber's internal user targeting automation system, MaLTA.},
  archiveprefix = {arXiv},
  arxivid       = {2004.03045},
  eprint        = {2004.03045},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan et al. - 2020 - Adversarial Validation Approach to Concept Drift Problem in User Targeting Automation Systems at Uber.pdf:pdf},
  keywords      = {AutoML,adversarial validation,concept drift,machine learning},
  url           = {http://arxiv.org/abs/2004.03045},
}

@InProceedings{art/KaraniN_201809,
  author        = {Karani, Neerav and Chaitanya, Krishna and Baumgartner, Christian and Konukoglu, Ender},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{A lifelong learning approach to brain MR segmentation across scanners and protocols}},
  year          = {2018},
  month         = {sep},
  pages         = {476--484},
  publisher     = {Springer Verlag},
  volume        = {11070 LNCS},
  abstract      = {Convolutional neural networks (CNNs) have shown promising results on several segmentation tasks in magnetic resonance (MR) images. However, the accuracy of CNNs may degrade severely when segmenting images acquired with different scanners and/or protocols as compared to the training data, thus limiting their practical utility. We address this shortcoming in a lifelong multi-domain learning setting by treating images acquired with different scanners or protocols as samples from different, but related domains. Our solution is a single CNN with shared convolutional filters and domain-specific batch normalization layers, which can be tuned to new domains with only a few (≈ 4) labelled images. Importantly, this is achieved while retaining performance on the older domains whose training data may no longer be available. We evaluate the method for brain structure segmentation in MR images. Results demonstrate that the proposed method largely closes the gap to the benchmark, which is training a dedicated CNN for each scanner.},
  archiveprefix = {arXiv},
  arxivid       = {1805.10170},
  doi           = {10.1007/978-3-030-00928-1_54},
  eprint        = {1805.10170},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Karani et al. - 2018 - A lifelong learning approach to brain MR segmentation across scanners and protocols.pdf:pdf},
  isbn          = {9783030009274},
  issn          = {16113349},
  url           = {https://doi.org/10.1007/978-3-030-00928-1_54},
}

@Article{art/HuangJ_202102,
  author    = {Huang, Jennifer and Bingham, Brian and Jordanov, Martin},
  journal   = {Academic Radiology},
  title     = {{The “Look Ahead” Technique: A Novel Way to Engage Medical Students in the Radiology Reading Room}},
  year      = {2021},
  issn      = {18784046},
  month     = {feb},
  number    = {2},
  pages     = {250--254},
  volume    = {28},
  abstract  = {Rationale and Objectives: Engaging medical students during a radiology course can be challenging. We sought a way to actively engage students with live cases, allow them to interact with the picture archiving and communication system workstation, and experience what it is like to be a radiologist. Materials and Methods: Medical students enrolled in one of three radiology courses between May 2016 and June 2017 were eligible. The “Look Ahead” technique is as follows: a preceptor identifies several nonurgent imaging studies and allows the students to view the images first and make independent observations and conclusions. When ready, the students present their findings, receive feedback, and observe the preceptor generate a final report. Students completed the postcourse survey comparing the “Look Ahead” technique with the current standard (observing a preceptor interpret imaging studies with accompanying teaching points). Results: Thirty-four (56.7%) of 60 potential respondents completed the postcourse survey. Of these 34, 24 (70.6%) reported at least one reading room case (mean 4.6) in which the technique was employed, with a mean of 2.4 unique preceptors. When compared to the current standard (0 = not to 100 = very interested/engaged/valuable/memorable), the “Look Ahead” technique was associated with increased student-reported interest (92.5 vs 75.1, p < 0.01), engagement (94.0 vs 70.3, p < 0.01), educational value (92.5 vs 73.2, p < 0.01), memorability of the case (88.5 vs 73.2, p < 0.01) and of accompanying teaching points (90.1 vs 76.7, p < 0.01). Conclusion: The “Look Ahead” technique is a meaningful and engaging teaching method, which students find “interesting,” “valuable,” and “memorable.”},
  doi       = {10.1016/j.acra.2019.12.021},
  keywords  = {Active learning,Radiology education,Situated learning},
  pmid      = {32061470},
  publisher = {Elsevier Inc.},
}

@Article{art/LiuJ_2021,
  author    = {Liu, Jianfei and Shen, Christine and Aguilera, Nancy and Cukras, Catherine and Hufnagel, Robert B. and Zein, Wadih M. and Liua, Tao and Tam, Johnny},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{Active Cell Appearance Model Induced Generative Adversarial Networks for Annotation-Efficient Cell Segmentation and Identification on Adaptive Optics Retinal Images}},
  year      = {2021},
  issn      = {1558254X},
  abstract  = {Data annotation is a fundamental precursor for establishing large training sets to effectively apply deep learning methods to medical image analysis. For cell segmentation, obtaining high quality annotations is an expensive process that usually requires manual grading by experts. This work introduces an approach to efficiently generate annotated images, called &#x201C;A-GANs&#x201D;, created by combining an active cell appearance model (ACAM) with conditional generative adversarial networks (C-GANs). ACAM is a statistical model that captures a realistic range of cell characteristics and is used to ensure that the image statistics of generated cells are guided by real data. C-GANs utilize cell contours generated by ACAM to produce cells that match input contours. By pairing ACAM-generated contours with A-GANs-based generated images, high quality annotated images can be efficiently generated. Experimental results on adaptive optics (AO) retinal images showed that A-GANs robustly synthesizes realistic, artificial images whose cell distributions are exquisitely specified by ACAM. The cell segmentation performance using as few as 64 manually-annotated real AO images combined with 248 artificially-generated images from A-GANs were similar to the case of using 248 manually-annotated real images alone (Dice coefficients of 88% for both). Finally, application to rare diseases in which images exhibit never-seen characteristics demonstrated improvements in cell segmentation without the need for incorporating manual annotations from these new retinal images. Overall, A-GANs introduce a methodology for generating high quality annotated data that statistically captures the characteristics of any desired dataset and can be used to more efficiently train deep-learning-based medical image analysis applications.},
  doi       = {10.1109/TMI.2021.3055483},
  keywords  = {Active appearance model,Adaptation models,Adaptive optics,Adaptive optics retinal imaging,Annotations,Cell segmentation,Data annotation,Data augmentation,Generative adversarial networks,Image segmentation,Retina,Shape,Training data},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Misc{art/HaoR_202011,
  author        = {Hao, Ruqian and Namdar, Khashayar and Liu, Lin and Khalvati, Farzad},
  month         = {nov},
  title         = {{A transfer learning based active learning framework for brain tumor classification}},
  year          = {2020},
  abstract      = {Brain tumor is one of the leading causes of cancer-related death globally among children and adults. Precise classification of brain tumor grade (low-grade and high-grade glioma) at early stage plays a key role in successful prognosis and treatment planning. With recent advances in deep learning, Artificial Intelligence-enabled brain tumor grading systems can assist radiologists in the interpretation of medical images within seconds. The performance of deep learning techniques is, however, highly depended on the size of the annotated dataset. It is extremely challenging to label a large quantity of medical images given the complexity and volume of medical data. In this work, we propose a novel transfer learning based active learning framework to reduce the annotation cost while maintaining stability and robustness of the model performance for brain tumor classification. We employed a 2D slice-based approach to train and finetune our model on the Magnetic Resonance Imaging (MRI) training dataset of 203 patients and a validation dataset of 66 patients which was used as the baseline. With our proposed method, the model achieved Area Under Receiver Operating Characteristic (ROC) Curve (AUC) of 82.89% on a separate test dataset of 66 patients, which was 2.92% higher than the baseline AUC while saving at least 40% of labeling cost. In order to further examine the robustness of our method, we created a balanced dataset, which underwent the same procedure. The model achieved AUC of 82% compared with AUC of 78.48% for the baseline, which reassures the robustness and stability of our proposed transfer learning augmented with active learning framework while significantly reducing the size of training data.},
  archiveprefix = {arXiv},
  arxivid       = {2011.09265},
  booktitle     = {arXiv},
  eprint        = {2011.09265},
  issn          = {23318422},
  keywords      = {Active learning,Brain tumor classification,Convolutional neural network (CNN),Deep learning,Magnetic resonance imaging (MRI),Transfer learning},
  publisher     = {arXiv},
}

@Article{art/KiyassehD_202004,
  author        = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A.},
  journal       = {arXiv},
  title         = {{ALPS: Active learning via perturbations}},
  year          = {2020},
  issn          = {23318422},
  month         = {apr},
  abstract      = {Small, labelled datasets in the presence of larger, unlabelled datasets pose challenges to data-hungry deep learning algorithms. Such scenarios are prevalent in healthcare where labelling is expensive, time-consuming, and requires expert medical professionals. To tackle this challenge, we propose a family of active learning methodologies and acquisition functions dependent upon input and parameter perturbations which we call Active Learning via Perturbations (ALPS). We test our methods on six diverse time-series and image datasets and illustrate their benefit in the presence and absence of an oracle. We also show that acquisition functions that incorporate temporal information have the potential to predict the ability of networks to generalize.},
  archiveprefix = {arXiv},
  arxivid       = {2004.09557},
  eprint        = {2004.09557},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiyasseh, Zhu, Clifton - 2020 - ALPS Active learning via perturbations.pdf:pdf},
  url           = {http://arxiv.org/abs/2004.09557},
}

@Misc{art/RenP_202008,
  author        = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  month         = {aug},
  title         = {{A survey of deep active learning}},
  year          = {2020},
  abstract      = {Active learning (AL) attempts to maximize a model's performance gain while annotating the fewest samples possible. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize a massive number of parameters if the model is to learn how to extract high-quality features. In recent years, due to the rapid development of internet technology, we have entered an era of information abundance characterized by massive amounts of available data. As a result, DL has attracted significant attention from researchers and has been rapidly developed. Compared with DL, however, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples, meaning that early AL is rarely accorded the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the large number of publicly available annotated datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, making it unfeasible in fields that require high levels of expertise (such as speech recognition, information extraction, medical images, etc.) Therefore, AL is gradually coming to receive the attention it is due. It is therefore natural to investigate whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. As a result of such investigations, deep active learning (DAL) has emerged. Although research on this topic is quite abundant, there has not yet been a comprehensive survey of DAL-related works; accordingly, this article aims to fill this gap. We provide a formal classification method for the existing work, along with a comprehensive and systematic overview. In addition, we also analyze and summarize the development of DAL from an application perspective. Finally, we discuss the confusion and problems associated with DAL and provide some possible development directions.},
  archiveprefix = {arXiv},
  arxivid       = {2009.00236},
  booktitle     = {arXiv},
  eprint        = {2009.00236},
  issn          = {23318422},
  keywords      = {Active Learning,Deep Active Learning,Deep Learning},
  publisher     = {arXiv},
}

@Article{art/KimT_202012,
  author    = {Kim, Taehun and Lee, Kyunghwa and Ham, Sungwon and Park, Beomhee and Lee, Sangwook and Hong, Dayeong and Kim, Guk Bae and Kyung, Yoon Soo and Kim, Choung Soo and Kim, Namkug},
  journal   = {Scientific Reports},
  title     = {{Active learning for accuracy enhancement of semantic segmentation with CNN-corrected label curations: Evaluation on kidney segmentation in abdominal CT}},
  year      = {2020},
  issn      = {20452322},
  month     = {dec},
  number    = {1},
  volume    = {10},
  abstract  = {Segmentation is fundamental to medical image analysis. Recent advances in fully convolutional networks has enabled automatic segmentation; however, high labeling efforts and difficulty in acquiring sufficient and high-quality training data is still a challenge. In this study, a cascaded 3D U-Net with active learning to increase training efficiency with exceedingly limited data and reduce labeling efforts is proposed. Abdominal computed tomography images of 50 kidneys were used for training. In stage I, 20 kidneys with renal cell carcinoma and four substructures were used for training by manually labelling ground truths. In stage II, 20 kidneys from the previous stage and 20 newly added kidneys were used with convolutional neural net (CNN)-corrected labelling for the newly added data. Similarly, in stage III, 50 kidneys were used. The Dice similarity coefficient was increased with the completion of each stage, and shows superior performance when compared with a recent segmentation network based on 3D U-Net. The labeling time for CNN-corrected segmentation was reduced by more than half compared to that in manual segmentation. Active learning was therefore concluded to be capable of reducing labeling efforts through CNN-corrected segmentation and increase training efficiency by iterative learning with limited data.},
  doi       = {10.1038/s41598-019-57242-9},
  pmid      = {31941938},
  publisher = {Nature Research},
}

@Article{art/ZhaoZ_2021,
  author    = {Zhao, Ziyuan and Zeng, Zeng and Xu, Kaixin and Chen, Cen and Guan, Cuntai},
  journal   = {IEEE Journal of Biomedical and Health Informatics},
  title     = {{DSAL: Deeply Supervised Active Learning from Strong and Weak Labelers for Biomedical Image Segmentation}},
  year      = {2021},
  issn      = {21682208},
  abstract  = {Image segmentation is one of the most essential biomedical image processing problems for different imaging modalities, including microscopy and X-ray in the Internet-of-Medical-Things (IoMT) domain. However, annotating biomedical images is knowledge-driven, time-consuming, and labor-intensive, making it difficult to obtain abundant labels with limited costs. Active learning strategies come into ease the burden of human annotation, which queries only a subset of training data for annotation. Despite receiving attention, most of active learning methods generally still require huge computational costs and utilize unlabeled data inefficiently. They also tend to ignore the intermediate knowledge within networks. In this work, we propose a deep active semi-supervised learning framework, DSAL, combining active learning and semi-supervised learning strategies. In DSAL, a new criterion based on deep supervision mechanism is proposed to select informative samples with high uncertainties and low uncertainties for strong labelers and weak labelers respectively. The internal criterion leverages the disagreement of intermediate features within the deep learning network for active sample selection, which subsequently reduces the computational costs. We use the proposed criteria to select samples for strong and weak labelers to produce oracle labels and pseudo labels simultaneously at each active learning iteration in an ensemble learning manner, which can be examined with IoMT Platform. Extensive experiments on multiple medical image datasets demonstrate the superiority of the proposed method over state-of-the-art active learning methods.},
  doi       = {10.1109/JBHI.2021.3052320},
  keywords  = {Active Learning,Annotations,Biomedical Image Segmentation,Biomedical imaging,Data models,Ensemble Learning,Image segmentation,Internet of Medical Things,Labeling,Semi-supervised Learning,Training,Uncertainty},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Misc{art/KimS_202004,
  author        = {Kim, Seong Tae and Mushtaq, Farrukh and Navab, Nassir},
  month         = {apr},
  title         = {{Confident Coreset for Active Learning in Medical Image Analysis}},
  year          = {2020},
  abstract      = {Recent advances in deep learning have resulted in great successes in various applications. Although semi-supervised or unsupervised learning methods have been widely investigated, the performance of deep neural networks highly depends on the annotated data. The problem is that the budget for annotation is usually limited due to the annotation time and expensive annotation cost in medical data. Active learning is one of the solutions to this problem where an active learner is designed to indicate which samples need to be annotated to effectively train a target model. In this paper, we propose a novel active learning method, confident coreset, which considers both uncertainty and distribution for effectively selecting informative samples. By comparative experiments on two medical image analysis tasks, we show that our method outperforms other active learning methods.},
  archiveprefix = {arXiv},
  arxivid       = {2004.02200},
  booktitle     = {arXiv},
  eprint        = {2004.02200},
  issn          = {23318422},
  keywords      = {Active learning,Coreset,Multiclass annotation},
  publisher     = {arXiv},
}

@Misc{art/SmailagicA_201908,
  author        = {Smailagic, Asim and Costa, Pedro and Gaudio, Alex and Khandelwal, Kartik and Mirshekari, Mostafa and Fagert, Jonathon and Walawalkar, Devesh and Xu, Susu and Galdran, Adrian and Zhang, Pei and Campilho, Aur{\'{e}}lio and Noh, Hae Young},
  month         = {aug},
  title         = {{O-MedAL: Online active deep learning for medical image analysis}},
  year          = {2019},
  abstract      = {Active Learning methods create an optimized and labeled training set from unlabeled data. We introduce a novel Online Active Deep Learning method for Medical Image Analysis. We extend our MedAL active learning framework to present new results in this paper. Experiments on three medical image datasets show that our novel online active learning model requires significantly less labelings, is more accurate, and is more robust to class imbalances than existing methods. Our method is also more accurate and computationally efficient than the baseline model. Compared to random sampling and uncertainty sampling, the method uses 275 and 200 (out of 768) fewer labeled examples, respectively. For Diabetic Retinopathy detection, our method attains a 5.88% accuracy improvement over the baseline model when 80% of the dataset is labeled, and the model reaches baseline accuracy when only 40% is labeled.},
  archiveprefix = {arXiv},
  arxivid       = {1908.10508},
  booktitle     = {arXiv},
  eprint        = {1908.10508},
  issn          = {23318422},
  keywords      = {Active Learning,Deep Learning,Medical Image Analysis,Online Learning},
  publisher     = {arXiv},
}

@InProceedings{art/WangJ_2020,
  author    = {Wang, Jingwen and Yan, Yuguang and Zhang, Yubing and Cao, Guiping and Yang, Ming and Ng, Michael K.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Deep reinforcement active learning for medical image classification}},
  year      = {2020},
  pages     = {33--42},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = {12261 LNCS},
  abstract  = {In this paper, we propose a deep reinforcement learning algorithm for active learning on medical image data. Although deep learning has achieved great success on medical image processing, it relies on a large number of labeled data for training, which is expensive and time-consuming. Active learning, which follows a strategy to select and annotate informative samples, is an effective approach to alleviate this issue. However, most existing methods of active learning adopt a hand-design strategy, which cannot handle the dynamic procedure of classifier training. To address this issue, we model the procedure of active learning as a Markov decision process, and propose a deep reinforcement learning algorithm to learn a dynamic policy for active learning. To achieve this, we employ the actor-critic approach, and apply the deep deterministic policy gradient algorithm to train the model. We conduct experiments on two kinds of medical image data sets, and the results demonstrate that our method is able to learn better strategy compared with the existing hand-design ones.},
  doi       = {10.1007/978-3-030-59710-8_4},
  isbn      = {9783030597092},
  issn      = {16113349},
  keywords  = {Active learning,Deep reinforcement learning,Medical image classification},
}

@Article{art/AnY_2021,
  author    = {An, Yang and Zhang, Liang and Yang, Haoyu and Sun, Leilei and Jin, Bo and Liu, Chuanren and Yu, Ruiyun and Wei, Xiaopeng},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  title     = {{Prediction of Treatment Medicines with Dual Adaptive Sequential Networks}},
  year      = {2021},
  issn      = {15582191},
  abstract  = {Predicting treatment medicines is a key aspect of many intelligent healthcare systems. It's a very challenging task due to the following reasons: (1) heterogeneous nature of EHR data that typically include laboratory results, treatment medicines, disease conditions, and demographic details collected from disparate sources; (2) complex correlations among medical sequences, including inter-correlations between sequences and temporal intra-correlations within each sequence; (3) temporal diversity of these correlations, which is highly affected by changing disease progression. We proposes a dual adaptive sequential network, entitled DASNet, to dynamically predict treatment medicines for patients. Specifically, DASNet comprises the following three components. Decomposed Adaptive Long Short-Term Memory network (DA-LSTM) is designed to capture the intra- and inter-correlations in multiple heterogeneous temporal sequences. Then, we develop an Attentive Meta learning Network (AT-MetaNet), which produces location- and context-specific dynamic weight parameters for DA-LSTM, thus enabling it to model the time-varying multi-level correlations. Finally, we employ an ATtentive Fusion Network (AT-FuNet) to retrieve historical information and collectively fuse heterogeneous data representation embeddings to predict treatment medicines. The results of extensive experiments on the public MIMIC-III dataset covering 11 medical conditions demonstrate that the proposed end-to-end model can achieve the state-of-the-art prediction performance while providing clinically useful insights.},
  doi       = {10.1109/TKDE.2021.3052992},
  keywords  = {Adaptive systems,Correlation,Diseases,Electronic Health Records,Medical diagnostic imaging,Medical services,Predictive models,Sequential Network,Task analysis,Temporal Correlations,Treatment Prediction},
  publisher = {IEEE Computer Society},
}

@Article{art/HanZ_202101,
  author    = {Han, Zhongyi and Wei, Benzheng and Xi, Xiaoming and Chen, Bo and Yin, Yilong and Li, Shuo},
  journal   = {Medical Image Analysis},
  title     = {{Unifying neural learning and symbolic reasoning for spinal medical report generation}},
  year      = {2021},
  issn      = {13618423},
  month     = {jan},
  volume    = {67},
  abstract  = {Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation and show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.},
  doi       = {10.1016/j.media.2020.101872},
  keywords  = {Adversarial training,Graph neural network,Logical reasoning,Medical image analysis,Medical report generation},
  pmid      = {33142134},
  publisher = {Elsevier B.V.},
}

@Article{art/PaulA_202102,
  author    = {Paul, Angshuman and Tang, Yu Xing and Shen, Thomas C. and Summers, Ronald M.},
  journal   = {Medical Image Analysis},
  title     = {{Discriminative ensemble learning for few-shot chest x-ray diagnosis}},
  year      = {2021},
  issn      = {13618423},
  month     = {feb},
  volume    = {68},
  abstract  = {Few-shot learning is an almost unexplored area in the field of medical image analysis. We propose a method for few-shot diagnosis of diseases and conditions from chest x-rays using discriminative ensemble learning. Our design involves a CNN-based coarse-learner in the first step to learn the general characteristics of chest x-rays. In the second step, we introduce a saliency-based classifier to extract disease-specific salient features from the output of the coarse-learner and classify based on the salient features. We propose a novel discriminative autoencoder ensemble to design the saliency-based classifier. The classification of the diseases is performed based on the salient features. Our algorithm proceeds through meta-training and meta-testing. During the training phase of meta-training, we train the coarse-learner. However, during the training phase of meta-testing, we train only the saliency-based classifier. Thus, our method is first-of-its-kind where the training phase of meta-training and the training phase of meta-testing are architecturally disjoint, making the method modular and easily adaptable to new tasks requiring the training of only the saliency-based classifier. Experiments show as high as ∼19% improvement in terms of F1 score compared to the baseline in the diagnosis of chest x-rays from publicly available datasets.},
  doi       = {10.1016/j.media.2020.101911},
  keywords  = {Autoencoder,Discriminative,Ensemble,Few-shot,X-ray},
  pmid      = {33264714},
  publisher = {Elsevier B.V.},
}

@InProceedings{art/KhandelwalP_202010,
  author        = {Khandelwal, Pulkit and Yushkevich, Paul},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Domain Generalizer: A Few-Shot Meta Learning Framework for Domain Generalization in Medical Imaging}},
  year          = {2020},
  month         = {oct},
  pages         = {73--84},
  publisher     = {Springer Science and Business Media Deutschland GmbH},
  volume        = {12444 LNCS},
  abstract      = {Deep learning models perform best when tested on target (test) data domains whose distribution is similar to the set of source (train) domains. However, model generalization can be hindered when there is significant difference in the underlying statistics between the target and source domains. In this work, we adapt a domain generalization method based on a model-agnostic meta-learning framework[1] to biomedical imaging. The method learns a domain-agnostic feature representation to improve generalization of models to the unseen test distribution. The method can be used for any imaging task, as it does not depend on the underlying model architecture. We validate the approach through a computed tomography (CT) vertebrae segmentation task across healthy and pathological cases on three datasets. Next, we employ few-shot learning, i.e. training the generalized model using very few examples from the unseen domain, to quickly adapt the model to new unseen data distribution. Our results suggest that the method could help generalize models across different medical centers, image acquisition protocols, anatomies, different regions in a given scan, healthy and diseased populations across varied imaging modalities.},
  archiveprefix = {arXiv},
  arxivid       = {2008.07724},
  doi           = {10.1007/978-3-030-60548-3_8},
  eprint        = {2008.07724},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khandelwal, Yushkevich - 2020 - Domain Generalizer A Few-Shot Meta Learning Framework for Domain Generalization in Medical Imaging.pdf:pdf},
  isbn          = {9783030605476},
  issn          = {16113349},
  keywords      = {Computed tomography,Domain adaptation,Domain generalization,Meta learning,Vertebrae segmentation},
  url           = {https://doi.org/10.1007/978-3-030-60548-3_8},
}

@Article{art/TustisonN_201410,
  author    = {Tustison, Nicholas J. and Cook, Philip A. and Klein, Arno and Song, Gang and Das, Sandhitsu R. and Duda, Jeffrey T. and Kandel, Benjamin M. and van Strien, Niels and Stone, James R. and Gee, James C. and Avants, Brian B.},
  journal   = {NeuroImage},
  title     = {{Large-scale evaluation of ANTs and FreeSurfer cortical thickness measurements}},
  year      = {2014},
  issn      = {10959572},
  month     = {oct},
  pages     = {166--179},
  volume    = {99},
  abstract  = {Many studies of the human brain have explored the relationship between cortical thickness and cognition, phenotype, or disease. Due to the subjectivity and time requirements in manual measurement of cortical thickness, scientists have relied on robust software tools for automation which facilitate the testing and refinement of neuroscientific hypotheses. The most widely used tool for cortical thickness studies is the publicly available, surface-based FreeSurfer package. Critical to the adoption of such tools is a demonstration of their reproducibility, validity, and the documentation of specific implementations that are robust across large, diverse imaging datasets. To this end, we have developed the automated, volume-based Advanced Normalization Tools (ANTs) cortical thickness pipeline comprising well-vetted components such as SyGN (multivariate template construction), SyN (image registration), N4 (bias correction), Atropos ( n-tissue segmentation), and DiReCT (cortical thickness estimation). In this work, we have conducted the largest evaluation of automated cortical thickness measures in publicly available data, comparing FreeSurfer and ANTs measures computed on 1205 images from four open data sets (IXI, MMRR, NKI, and OASIS), with parcellation based on the recently proposed Desikan-Killiany-Tourville (DKT) cortical labeling protocol. We found good scan-rescan repeatability with both FreeSurfer and ANTs measures. Given that such assessments of precision do not necessarily reflect accuracy or an ability to make statistical inferences, we further tested the neurobiological validity of these approaches by evaluating thickness-based prediction of age and gender. ANTs is shown to have a higher predictive performance than FreeSurfer for both of these measures. In promotion of open science, we make all of our scripts, data, and results publicly available which complements the use of open image data sets and the open source availability of the proposed ANTs cortical thickness pipeline. {\textcopyright} 2014 Elsevier Inc.},
  doi       = {10.1016/j.neuroimage.2014.05.044},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tustison et al. - 2014 - Large-scale evaluation of ANTs and FreeSurfer cortical thickness measurements.pdf:pdf},
  keywords  = {Advanced Normalization Tools,Age prediction,Gender prediction,MRI,Open science,Scientific reproducibility},
  pmid      = {24879923},
  publisher = {Academic Press Inc.},
  url       = {https://pubmed.ncbi.nlm.nih.gov/24879923/},
}

@Article{art/ScheieD_200607,
  author   = {Scheie, David and Andresen, Per Arne and Cvancarova, Milada and B{\o}, Anne Signe and Helseth, Eirik and Skullerud, Kari and Beiske, Klaus},
  journal  = {American Journal of Surgical Pathology},
  title    = {{Fluorescence in situ hybridization (FISH) on touch preparations: A reliable method for detecting loss of heterozygosity at 1p and 19q in oligodendroglial tumors}},
  year     = {2006},
  issn     = {01475185},
  month    = {jul},
  number   = {7},
  pages    = {828--837},
  volume   = {30},
  abstract = {Combined loss of heterozygosity (LOH) on 1p and 19q is reported in 50% to 90% of oligodendroglial tumors and has emerged as a strong and favorable prognostic factor. Fluorescence in situ hybridization (FISH) and polymerase chain reaction (PCR) are the most widely used techniques. The aim of this study was to evaluate the reliability of FISH to predict LOH at 1p and 19q when performed on touch preparations from 40 oligodendroglial tumors, even if the majority of the nuclei showed chromosomal imbalance. PCR was used as the gold standard. The presence of none or one target signal was reported as FISH-LOH, whereas all other losses were defined as FISH-imbalance. The sum of nuclei with FISH-LOH and imbalance was calculated in each case (FISH-sum) and cut-off values were defined as the mean FISH-sum value in controls plus 3 standard deviations; 27.7% for 1p and 33.2% for 19q. These corresponded well with the optimal cut-off values for our data, calculated using the minimum error rate classification procedure (35.6% for 1p and 33.1% for 19q). Concurrent FISH and PCR results were encountered in 95% for 1p and 87.5% for 19q. FISH-sum was the best and simplest discriminating variable for correct classification of LOH status. Under these conditions, even a dominant population of nuclei showing FISH-imbalance represented an LOH status in the tumor cells. FISH on touch preparations is a quick and reliable method for 1p/19q testing, does not require normal DNA and can be easily performed in an immunohistochemistry unit. Copyright {\textcopyright} 2006 by Lippincott Williams & Wilkins.},
  doi      = {10.1097/01.pas.0000213250.44822.2e},
  keywords = {19q,1p,FISH,LOH,Oligodendroglioma,Touch preparations},
  pmid     = {16819324},
}

@Article{art/KimJ_202012,
  author    = {Kim, Jinwoo and Hwang, Jeongbin and Chi, Seokho and Seo, Joon Oh},
  journal   = {Automation in Construction},
  title     = {{Towards database-free vision-based monitoring on construction sites: A deep active learning approach}},
  year      = {2020},
  issn      = {09265805},
  month     = {dec},
  volume    = {120},
  abstract  = {In order to achieve database-free (DB-free) vision-based monitoring on construction sites, this paper proposes a deep active learning approach that automatically evaluates the uncertainty of unlabeled training data, selects the most meaningful-to-learn instances, and eventually trains a deep learning model with the selected data. The proposed approach thus involves three sequential processes: (1) uncertainty evaluation of unlabeled data, (2) training data sampling and user-interactive labeling, and (3) model design and training. Two experiments were performed to validate the proposed method and confirm the positive effects of active learning: one experiment with active learning and the other without active learning (i.e., with random learning). In the experiments, the research team used a total of 17,000 images collected from actual construction sites. To achieve 80% mean Average Precision (mAP) for construction object detection, the random learning method required 720 training images, whereas only 180 images were sufficient when exploiting active learning. Moreover, the active learning could build a deep learning model with the mAP of 93.0%, while that of the random learning approach was limited to 89.1%. These results demonstrate the potential of the proposed method and highlight the considerable positive impacts of uncertainty-based data sampling on the model's performance. This research can improve the practicality of vision-based monitoring on construction sites, and the findings of this study can provide valuable insights and new research directions for construction researchers.},
  doi       = {10.1016/j.autcon.2020.103376},
  keywords  = {Active learning,Construction site,Database-free,Deep learning,Object detection,Vision-based monitoring},
  publisher = {Elsevier B.V.},
}

@Article{art/LiuY_202008,
  author        = {Liu, Yezheng and Li, Zhe and Zhou, Chong and Jiang, Yuanchun and Sun, Jianshan and Wang, Meng and He, Xiangnan},
  journal       = {IEEE Transactions on Knowledge and Data Engineering},
  title         = {{Generative Adversarial Active Learning for Unsupervised Outlier Detection}},
  year          = {2020},
  issn          = {15582191},
  month         = {aug},
  number        = {8},
  pages         = {1517--1528},
  volume        = {32},
  abstract      = {Outlier detection is an important topic in machine learning and has been used in a wide range of applications. In this paper, we approach outlier detection as a binary-classification issue by sampling potential outliers from a uniform reference distribution. However, due to the sparsity of data in high-dimensional space, a limited number of potential outliers may fail to provide sufficient information to assist the classifier in describing a boundary that can separate outliers from normal data effectively. To address this, we propose a novel Single-Objective Generative Adversarial Active Learning (SO-GAAL) method for outlier detection, which can directly generate informative potential outliers based on the mini-max game between a generator and a discriminator. Moreover, to prevent the generator from falling into the mode collapsing problem, the stop node of training should be determined when SO-GAAL is able to provide sufficient information. But without any prior information, it is extremely difficult for SO-GAAL. Therefore, we expand the network structure of SO-GAAL from a single generator to multiple generators with different objectives (MO-GAAL), which can generate a reasonable reference distribution for the whole dataset. We empirically compare the proposed approach with several state-of-the-art outlier detection methods on both synthetic and real-world datasets. The results show that MO-GAAL outperforms its competitors in the majority of cases, especially for datasets with various cluster types or high irrelevant variable ratio.},
  archiveprefix = {arXiv},
  arxivid       = {1809.10816},
  doi           = {10.1109/TKDE.2019.2905606},
  eprint        = {1809.10816},
  keywords      = {Outlier detection,curse of dimensionality,generate potential outliers,generative adversarial active learning,mode collapsing problem,multiple-objective generative adversarial active l},
  publisher     = {IEEE Computer Society},
}

@Article{art/GuoJ_2021,
  author    = {Guo, Jifeng and Pang, Zhiqi and Bai, Miaoyuan and Xie, Peijiao and Chen, Yu},
  journal   = {Applied Intelligence},
  title     = {{Dual generative adversarial active learning}},
  year      = {2021},
  issn      = {15737497},
  abstract  = {The purpose of active learning is to significantly reduce the cost of annotation while ensuring the good performance of the model. In this paper, we propose a novel active learning method based on the combination of pool and synthesis named dual generative adversarial active learning (DGAAL), which includes the functions of image generation and representation learning. This method includes two groups of generative adversarial network composed of a generator and two discriminators. One group is used for representation learning, and then this paper performs sampling based on the predicted value of the discriminator. The other group is used for image generation. The purpose is to generate samples which are similar to those obtained from sampling, so that samples with rich information can be fully utilized. In the sampling process, the two groups of network cooperate with each other to enable the generated samples to participate in sampling process, and to enable the discriminator for sampling to co-evolve. Thus, in the later stage of sampling, the problem of insufficient information for selecting samples based on the pool method is alleviated. In this paper, DGAAL is evaluated extensively on three data sets, and the results show that DGAAL not only has certain advantages over the existing methods in terms of model performance but can also further reduces the annotation cost.},
  doi       = {10.1007/s10489-020-02121-4},
  keywords  = {Active learning,Deep learning,Generative adversarial networks,Image generation},
  publisher = {Springer},
}

@Misc{art/MillerB_202010,
  author    = {Miller, Blake and Linder, Fridolin and Mebane, Walter R.},
  month     = {oct},
  title     = {{Active learning approaches for labeling text: Review and assessment of the performance of active learning approaches}},
  year      = {2020},
  abstract  = {Supervised machine learning methods are increasingly employed in political science. Such models require costly manual labeling of documents. In this paper, we introduce active learning, a framework in which data to be labeled by human coders are not chosen at random but rather targeted in such a way that the required amount of data to train a machine learning model can be minimized. We study the benefits of active learning using text data examples. We perform simulation studies that illustrate conditions where active learning can reduce the cost of labeling text data. We perform these simulations on three corpora that vary in size, document length, and domain. We find that in cases where the document class of interest is not balanced, researchers can label a fraction of the documents one would need using random sampling (or passive learning) to achieve equally performing classifiers. We further investigate how varying levels of intercoder reliability affect the active learning procedures and find that even with low reliability, active learning performs more efficiently than does random sampling.},
  booktitle = {Political Analysis},
  doi       = {10.1017/pan.2020.4},
  issn      = {14764989},
  keywords  = {Active learning,Intercoder reliability,Statistical analysis of texts,Supervised learning},
  number    = {4},
  pages     = {532--551},
  publisher = {Cambridge University Press},
  volume    = {28},
}

@Article{art/ZhangT_202102,
  author    = {Zhang, Tong and Su, Guoxi and Qing, Chunmei and Xu, Xiangmin and Cai, Bolun and Xing, Xiaofen},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  title     = {{Hierarchical Lifelong Learning by Sharing Representations and Integrating Hypothesis}},
  year      = {2021},
  issn      = {21682232},
  month     = {feb},
  number    = {2},
  pages     = {1004--1014},
  volume    = {51},
  abstract  = {In lifelong machine learning (LML) systems, consecutive new tasks from changing circumstances are learned and added to the system. However, sufficiently labeled data are indispensable for extracting intertask relationships before transferring knowledge in classical supervised LML systems. Inadequate labels may deteriorate the performance due to the poor initial approximation. In order to extend the typical LML system, we propose a novel hierarchical lifelong learning algorithm (HLLA) consisting of two following layers: 1) the knowledge layer consisted of shared representations and integrated knowledge basis at the bottom and 2) parameterized hypothesis functions with features at the top. Unlabeled data is leveraged in HLLA for pretraining of the shared representations. We also have considered a selective inherited updating method to deal with intertask distribution shifting. Experiments show that our HLLA method outperforms many other recent LML algorithms, especially when dealing with higher dimensional, lower correlation, and fewer labeled data problems.},
  doi       = {10.1109/TSMC.2018.2884996},
  keywords  = {Deep learning,image processing,lifelong machine learning (LML),representations learning},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/SunG_202101,
  author    = {Sun, Gan and Cong, Yang and Zhang, Yulun and Zhao, Guoshuai and Fu, Yun},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  title     = {{Continual Multiview Task Learning via Deep Matrix Factorization}},
  year      = {2021},
  issn      = {21622388},
  month     = {jan},
  number    = {1},
  pages     = {139--150},
  volume    = {32},
  abstract  = {The state-of-the-art multitask multiview (MTMV) learning tackles a scenario where multiple tasks are related to each other via multiple shared feature views. However, in many real-world scenarios where a sequence of the multiview task comes, the higher storage requirement and computational cost of retraining previous tasks with MTMV models have presented a formidable challenge for this lifelong learning scenario. To address this challenge, in this article, we propose a new continual multiview task learning model that integrates deep matrix factorization and sparse subspace learning in a unified framework, which is termed deep continual multiview task learning (DCMvTL). More specifically, as a new multiview task arrives, DCMvTL first adopts a deep matrix factorization technique to capture hidden and hierarchical representations for this new coming multiview task while accumulating the fresh multiview knowledge in a layerwise manner. Then, a sparse subspace learning model is employed for the extracted factors at each layer and further reveals cross-view correlations via a self-expressive constraint. For model optimization, we derive a general multiview learning formulation when a new multiview task comes and apply an alternating minimization strategy to achieve lifelong learning. Extensive experiments on benchmark data sets demonstrate the effectiveness of our proposed DCMvTL model compared with the existing state-of-the-art MTMV and lifelong multiview task learning models.},
  doi       = {10.1109/TNNLS.2020.2977497},
  keywords  = {Deep matrix factorization,lifelong machine learning,multiview learning,sparse subspace learning},
  pmid      = {32175877},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/HongX_202005,
  author    = {Hong, Xianbin and Guan, Sheng Uei and Man, Ka Lok and Wong, Prudence W.H.},
  journal   = {Symmetry},
  title     = {{Lifelong machine learning architecture for classification}},
  year      = {2020},
  issn      = {20738994},
  month     = {may},
  number    = {5},
  volume    = {12},
  abstract  = {Benefiting from the rapid development of big data and high-performance computing, more data is available and more tasks could be solved by machine learning now. Even so, it is still difficult to maximum the power of big data due to each dataset is isolated with others. Although open source datasets are available, algorithms' performance is asymmetric with the data volume. Hence, the AI community wishes to raise a symmetric continuous learning architecture which can automatically learn and adapt to different tasks. Such a learning architecture also is commonly called as lifelong machine learning (LML). This learning paradigm could manage the learning process and accumulate meta-knowledge by itself during learning different tasks. The meta-knowledge is shared among all tasks symmetrically to help them to improve performance. With the growth of meta-knowledge, the performance of each task is expected to be better and better. In order to demonstrate the application of lifelong machine learning, this paper proposed a novel and symmetric lifelong learning approach for sentiment classification as an example to show how it adapts different domains and keeps efficiency meanwhile.},
  doi       = {10.3390/SYM12050852},
  keywords  = {Continuous learning,Lifelong machine learning,Natural language processing,Sentiment classification},
  publisher = {MDPI AG},
}

@TechReport{art/ZhouA_201909,
  author        = {Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea},
  title         = {{Watch, try, learn: Meta-learning from demonstrations and rewards}},
  year          = {2019},
  month         = {sep},
  abstract      = {Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.},
  archiveprefix = {arXiv},
  arxivid       = {1906.03352},
  booktitle     = {arXiv},
  eprint        = {1906.03352},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2019 - Watch, try, learn Meta-learning from demonstrations and rewards.pdf:pdf},
  issn          = {23318422},
  url           = {https://sites.google.com/view/watch-try-learn-project},
}

@Misc{art/FakoorR_2019,
  author    = {Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J.},
  title     = {{Meta-Q-learning}},
  year      = {2019},
  abstract  = {This paper introduces Meta-Q-Learning (MQL), a new off-policy algorithm for meta-Reinforcement Learning (meta-RL). MQL builds upon three simple ideas. First, we show that Q-learning is competitive with state of the art meta-RL algorithms if given access to a context variable that is a representation of the past trajectory. Second, using a multi-task objective to maximize the average reward across the training tasks is an effective method to meta-train RL policies. Third, past data from the meta-training replay buffer can be recycled to adapt the policy on a new task using off-policy updates. MQL draws upon ideas in propensity estimation to do so and thereby amplifies the amount of available data for adaptation. Experiments on standard continuous-control benchmarks suggest that MQL compares favorably with state of the art meta-RL algorithms.},
  booktitle = {arXiv},
  issn      = {23318422},
}

@Misc{art/SylvainT_2019,
  author        = {Sylvain, Tristan and Petrini, Linda and Hjelm, R. Devon},
  title         = {{Locality and compositionality in zero-shot learning}},
  year          = {2019},
  abstract      = {In this work we study locality and compositionality in the context of learning representations for Zero Shot Learning (ZSL). In order to well-isolate the importance of these properties in learned representations, we impose the additional constraint that, differently from most recent work in ZSL, no pre-training on different datasets (e.g. ImageNet) is performed. The results of our experiments show how locality, in terms of small parts of the input, and compositionality, i.e. how well can the learned representations be expressed as a function of a smaller vocabulary, are both deeply related to generalization and motivate the focus on more local-aware models in future research directions for representation learning.},
  archiveprefix = {arXiv},
  arxivid       = {1912.12179},
  booktitle     = {arXiv},
  eprint        = {1912.12179},
  issn          = {23318422},
}

@Misc{art/FlennerhagS_201909,
  author        = {Flennerhag, Sebastian and Rusu, Andrei A. and Pascanu, Razvan and Yin, Hujun and Hadsell, Raia},
  month         = {sep},
  title         = {{Meta-learning with warped gradient descent}},
  year          = {2019},
  abstract      = {A versatile and effective approach to meta-learning is to infer a gradient-based update rule directly from data that promotes rapid learning of new tasks from the same distribution. Current methods rely on backpropagating through the learning process, limiting their scope to few-shot learning. In this work, we introduce Warped Gradient Descent (WarpGrad), a family of modular optimisers that can scale to arbitrary adaptation processes. WarpGrad methods meta-learn to warp task loss surfaces across the joint task-parameter distribution to facilitate gradient descent, which is achieved by a reparametrisation of neural networks that interleaves warp-layers in the architecture. These layers are shared across task learners and fixed during adaptation; they represent a projection of task parameters into a meta-learned space that is conducive to task adaptation and standard backpropagation induces a form of gradient preconditioning. WarpGrad methods are computationally efficient and easy to implement as they rely on parameter sharing and backpropagation. They are readily combined with other meta-learners and can scale both in terms of model size and length of adaptation trajectories as meta-learning warp parameters do not require differentiation through task adaptation processes. We show empirically that WarpGrad optimisers meta-learn a warped space where gradient descent is well behaved, with faster convergence and better performance in a variety of settings, including few-shot, standard supervised, continual, and reinforcement learning.},
  archiveprefix = {arXiv},
  arxivid       = {1909.00025},
  booktitle     = {arXiv},
  eprint        = {1909.00025},
  issn          = {23318422},
}

@Misc{art/AletF_202009,
  author        = {Alet, Ferran and Schneider, Martin F. and Lozano-Perez, Tomas and {Pack Kaelbling}, Leslie},
  month         = {sep},
  title         = {{Meta-learning curiosity algorithms}},
  year          = {2020},
  abstract      = {We hypothesize that curiosity is a mechanism found by evolution that encourages meaningful exploration early in an agent's life in order to expose it to experiences that enable it to obtain high rewards over the course of its lifetime. We formulate the problem of generating curious behavior as one of meta-learning: An outer loop will search over a space of curiosity mechanisms that dynamically adapt the agent's reward signal, and an inner loop will perform standard reinforcement learning using the adapted reward signal. However, current meta-RL methods based on transferring neural network weights have only generalized between very similar tasks. To broaden the generalization, we instead propose to meta-learn algorithms: Pieces of code similar to those designed by humans in ML papers. Our rich language of programs combines neural networks with other building blocks such as buffers, nearest-neighbor modules and custom loss functions. We demonstrate the effectiveness of the approach empirically, finding two novel curiosity algorithms that perform on par or better than human-designed published curiosity algorithms in domains as disparate as grid navigation with image inputs, acrobot, lunar lander, ant and hopper.},
  archiveprefix = {arXiv},
  arxivid       = {2003.05325},
  booktitle     = {arXiv},
  eprint        = {2003.05325},
  issn          = {23318422},
}

@Article{art/FeiFeiL_200604,
  author   = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {{One-shot learning of object categories}},
  year     = {2006},
  issn     = {01628828},
  month    = {apr},
  number   = {4},
  pages    = {594--611},
  volume   = {28},
  abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully. {\textcopyright} 2006 IEEE.},
  doi      = {10.1109/TPAMI.2006.79},
  keywords = {Few images,Learning,Object categories,Priors,Recognition,Unsupervised,Variational inference},
  pmid     = {16566508},
  url      = {http://ieeexplore.ieee.org/document/1597116/},
}

@Article{art/ShabanA_201709,
  author        = {Shaban, Amirreza and Bansal, Shray and Liu, Zhen and Essa, Irfan and Boots, Byron},
  journal       = {arXiv},
  title         = {{One-shot learning for semantic segmentation}},
  year          = {2017},
  issn          = {23318422},
  month         = {sep},
  abstract      = {Low-shot learning methods for image classification support learning from sparse data. We extend these techniques to support dense semantic image segmentation. Specifically, we train a network that, given a small set of annotated images, produces parameters for a Fully Convolutional Network (FCN). We use this FCN to perform dense pixel-level prediction on a test image for the new semantic class. Our architecture shows a 25% relative meanIoU improvement compared to the best baseline methods for one-shot segmentation on unseen classes in the PASCAL VOC 2012 dataset and is at least 3× faster. The code is publicly available at: https://github.com/lzzcd001/OSLSM.},
  archiveprefix = {arXiv},
  arxivid       = {1709.03410},
  eprint        = {1709.03410},
  url           = {http://arxiv.org/abs/1709.03410},
}

@Misc{art/NaD_201909,
  author        = {Na, Donghyun and Lee, Hae Beom and Kim, Saehoon and Park, Minseop and Yang, Eunho and Hwang, Sung Ju},
  month         = {sep},
  title         = {{Learning to balance: Bayesian meta-learning for imbalanced and out-of-distribution tasks}},
  year          = {2019},
  abstract      = {While tasks could come with varying number of instances in realistic settings, the existing meta-learning approaches for few-shot classfication assume even task distributions where the number of instances for each task and class are fixed. Due to such restriction, they learn to equally utilize the meta-knowledge across all the tasks, even when the number of instances per task and class largely varies. Moreover, they do not consider distributional difference in unseen tasks at the meta-test time, on which the meta-knowledge may have varying degree of usefulness depending on the task relatedness. To overcome these limitations, we propose a novel meta-learning model that adaptively balances the effect of the meta-learning and task-specific learning, and also class-specific learning within each task. Through the learning of the balancing variables, we can decide whether to obtain a solution close to the initial parameter or far from it. We formulate this objective into a Bayesian inference framework and solve it using variational inference. Our Bayesian Task-Adaptive Meta-Learning (Bayesian-TAML) significantly outperforms existing meta-learning approaches on benchmark datasets for both few-shot and realistic class- and task-imbalanced datasets, with especially higher gains on the latter.},
  archiveprefix = {arXiv},
  arxivid       = {1905.12917},
  booktitle     = {arXiv},
  eprint        = {1905.12917},
  issn          = {23318422},
}

@Article{art/LiX_202010,
  author        = {Li, Xiaoxu and Sun, Zhuo and Xue, Jing Hao and Ma, Zhanyu},
  journal       = {Neurocomputing},
  title         = {{A concise review of recent few-shot meta-learning methods}},
  year          = {2020},
  issn          = {18728286},
  month         = {oct},
  abstract      = {Few-shot meta-learning has been recently reviving with expectations to mimic humanity's fast adaption to new concepts based on prior knowledge. In this short communication, we give a concise review on recent representative methods in few-shot meta-learning, which are categorized into four branches according to their technical characteristics. We conclude this review with some vital current challenges and future prospects in few-shot meta-learning.},
  archiveprefix = {arXiv},
  arxivid       = {2005.10953},
  doi           = {10.1016/j.neucom.2020.05.114},
  eprint        = {2005.10953},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2020 - A concise review of recent few-shot meta-learning methods.pdf:pdf},
  keywords      = {Deep Neural Networks,Few-shot Learning,Image Classification,Meta Learning,Small-sample Learning},
  publisher     = {Elsevier B.V.},
}

@Article{art/VanschorenJ_201810,
  author        = {Vanschoren, Joaquin},
  journal       = {arXiv},
  title         = {{Meta-Learning: A Survey}},
  year          = {2018},
  issn          = {23318422},
  month         = {oct},
  abstract      = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  archiveprefix = {arXiv},
  arxivid       = {1810.03548},
  eprint        = {1810.03548},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanschoren - 2018 - Meta-Learning A Survey.pdf:pdf},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/1810.03548},
}

@Misc{art/ParisiG_201905,
  author        = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
  month         = {may},
  title         = {{Continual lifelong learning with neural networks: A review}},
  year          = {2019},
  abstract      = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
  archiveprefix = {arXiv},
  arxivid       = {1802.07569},
  booktitle     = {Neural Networks},
  doi           = {10.1016/j.neunet.2019.01.012},
  eprint        = {1802.07569},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisi et al. - 2019 - Continual lifelong learning with neural networks A review.pdf:pdf},
  issn          = {18792782},
  keywords      = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
  pages         = {54--71},
  pmid          = {30780045},
  publisher     = {Elsevier Ltd},
  volume        = {113},
}

@Article{art/AljundiR_201811,
  author        = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Memory Aware Synapses: Learning What (not) to Forget}},
  year          = {2018},
  issn          = {16113349},
  month         = {nov},
  pages         = {144--161},
  volume        = {11207 LNCS},
  abstract      = {Humans can learn in a continuous manner. Old rarely utilized knowledge can be overwritten by new incoming information while important, frequently used knowledge is prevented from being erased. In artificial learning systems, lifelong learning so far has focused mainly on accumulating knowledge over tasks and overcoming catastrophic forgetting. In this paper, we argue that, given the limited model capacity and the unlimited new information to be learned, knowledge has to be preserved or erased selectively. Inspired by neuroplasticity, we propose a novel approach for lifelong learning, coined Memory Aware Synapses (MAS). It computes the importance of the parameters of a neural network in an unsupervised and online manner. Given a new sample which is fed to the network, MAS accumulates an importance measure for each parameter of the network, based on how sensitive the predicted output function is to a change in this parameter. When learning a new task, changes to important parameters can then be penalized, effectively preventing important knowledge related to previous tasks from being overwritten. Further, we show an interesting connection between a local version of our method and Hebb's rule, which is a model for the learning process in the brain. We test our method on a sequence of object recognition tasks and on the challenging problem of learning an embedding for predicting <subject, predicate, object> triplets. We show state-of-the-art performance and, for the first time, the ability to adapt the importance of the parameters based on unlabeled data towards what the network needs (not) to forget, which may vary depending on test conditions.},
  archiveprefix = {arXiv},
  arxivid       = {1711.09601},
  doi           = {10.1007/978-3-030-01219-9_9},
  eprint        = {1711.09601},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aljundi et al. - 2018 - Memory Aware Synapses Learning What (not) to Forget.pdf:pdf},
  isbn          = {9783030012182},
  publisher     = {Springer Verlag},
  url           = {http://arxiv.org/abs/1711.09601},
}

@Article{art/GuhaRoyA_202001,
  author        = {{Guha Roy}, Abhijit and Siddiqui, Shayan and P{\"{o}}lsterl, Sebastian and Navab, Nassir and Wachinger, Christian},
  journal       = {Medical Image Analysis},
  title         = {{‘Squeeze & excite' guided few-shot segmentation of volumetric images}},
  year          = {2020},
  issn          = {13618423},
  month         = {jan},
  pages         = {101587},
  volume        = {59},
  abstract      = {Deep neural networks enable highly accurate image segmentation, but require large amounts of manually annotated data for supervised training. Few-shot learning aims to address this shortcoming by learning a new class from a few annotated support examples. We introduce, a novel few-shot framework, for the segmentation of volumetric medical images with only a few annotated slices. Compared to other related works in computer vision, the major challenges are the absence of pre-trained networks and the volumetric nature of medical scans. We address these challenges by proposing a new architecture for few-shot segmentation that incorporates ‘squeeze & excite' blocks. Our two-armed architecture consists of a conditioner arm, which processes the annotated support input and generates a task-specific representation. This representation is passed on to the segmenter arm that uses this information to segment the new query image. To facilitate efficient interaction between the conditioner and the segmenter arm, we propose to use ‘channel squeeze & spatial excitation' blocks – a light-weight computational module – that enables heavy interaction between both the arms with negligible increase in model complexity. This contribution allows us to perform image segmentation without relying on a pre-trained model, which generally is unavailable for medical scans. Furthermore, we propose an efficient strategy for volumetric segmentation by optimally pairing a few slices of the support volume to all the slices of the query volume. We perform experiments for organ segmentation on whole-body contrast-enhanced CT scans from the Visceral Dataset. Our proposed model outperforms multiple baselines and existing approaches with respect to the segmentation accuracy by a significant margin. The source code is available at https://github.com/abhi4ssj/few-shot-segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {1902.01314},
  doi           = {10.1016/j.media.2019.101587},
  eprint        = {1902.01314},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guha Roy et al. - 2020 - ‘Squeeze & excite' guided few-shot segmentation of volumetric images.pdf:pdf},
  keywords      = {Deep learning,Few-shot learning,Organ segmentation,Semantic segmentation,Squeeze & excite},
  pmid          = {31630012},
  publisher     = {Elsevier B.V.},
}

@Article{art/SchmitzR_202105,
  author        = {Schmitz, R{\"{u}}diger and Madesta, Frederic and Nielsen, Maximilian and Krause, Jenny and Steurer, Stefan and Werner, Ren{\'{e}} and R{\"{o}}sch, Thomas},
  journal       = {Medical Image Analysis},
  title         = {{Multi-scale fully convolutional neural networks for histopathology image segmentation: From nuclear aberrations to the global tissue architecture}},
  year          = {2021},
  issn          = {13618423},
  month         = {may},
  pages         = {101996},
  volume        = {70},
  abstract      = {Histopathologic diagnosis relies on simultaneous integration of information from a broad range of scales, ranging from nuclear aberrations (≈O(0.1$\mu$m)) through cellular structures (≈O(10$\mu$m)) to the global tissue architecture (⪆O(1mm)). To explicitly mimic how human pathologists combine multi-scale information, we introduce a family of multi-encoder fully-convolutional neural networks with deep fusion. We present a simple block for merging model paths with differing spatial scales in a spatial relationship-preserving fashion, which can readily be included in standard encoder-decoder networks. Additionally, a context classification gate block is proposed as an alternative for the incorporation of global context. Our experiments were performed on three publicly available whole-slide images of recent challenges (PAIP 2019: hepatocellular carcinoma segmentation; BACH 2020: breast cancer segmentation; CAMELYON 2016: metastasis detection in lymph nodes). The multi-scale architectures consistently outperformed the baseline single-scale U-Nets by a large margin. They benefit from local as well as global context and particularly a combination of both. If feature maps from different scales are fused, doing so in a manner preserving spatial relationships was found to be beneficial. Deep guidance by a context classification loss appeared to improve model training at low computational costs. All multi-scale models had a reduced GPU memory footprint compared to ensembles of individual U-Nets trained on different image scales. Additional path fusions were shown to be possible at low computational cost, opening up possibilities for further, systematic and task-specific architecture optimisation. The findings demonstrate the potential of the presented family of human-inspired, end-to-end trainable, multi-scale multi-encoder fully-convolutional neural networks to improve deep histopathologic diagnosis by extensive integration of largely different spatial scales.},
  archiveprefix = {arXiv},
  arxivid       = {1909.10726},
  doi           = {10.1016/j.media.2021.101996},
  eprint        = {1909.10726},
  keywords      = {Computational pathology,FCN,Fully-convolutional neural nets,Histopathology,Human-inspired computer vision,Multi-scale},
  pmid          = {33647783},
  url           = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000426},
}

@Article{art/YurtM_202105,
  author        = {Yurt, Mahmut and Dar, Salman UH and Erdem, Aykut and Erdem, Erkut and Oguz, Kader K. and {\c{C}}ukur, Tolga},
  journal       = {Medical Image Analysis},
  title         = {{Mustgan: Multi-stream generative adversarial networks for MR image synthesis}},
  year          = {2021},
  issn          = {13618423},
  month         = {may},
  pages         = {101944},
  volume        = {70},
  abstract      = {Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one methods take as input a single source contrast, and they learn a latent representation sensitive to unique features of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many-to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.},
  archiveprefix = {arXiv},
  arxivid       = {1909.11504},
  doi           = {10.1016/j.media.2020.101944},
  eprint        = {1909.11504},
  keywords      = {Fusion,Generative adversarial networks (GAN),Image synthesis,Magnetic resonance imaging (MRI),Multi-contrast,Multi-stream},
  pmid          = {33690024},
  url           = {https://linkinghub.elsevier.com/retrieve/pii/S136184152030308X},
}

@Article{art/LeeG_202105,
  author   = {Lee, Gyuhyun and Oh, Jeong Woo and Her, Nam Gu and Jeong, Won Ki},
  journal  = {Medical Image Analysis},
  title    = {{DeepHCS++: Bright-field to fluorescence microscopy image conversion using multi-task learning with adversarial losses for label-free high-content screening}},
  year     = {2021},
  issn     = {13618423},
  month    = {may},
  pages    = {101995},
  volume   = {70},
  abstract = {In this paper, we propose a novel microscopy image translation method for transforming a bright-field microscopy image into three different fluorescence images to observe the apoptosis, nuclei, and cytoplasm of cells, which visualize dead cells, nuclei of cells, and cytoplasm of cells, respectively. These biomarkers are commonly used in high-content drug screening to analyze drug response. The main contribution of the proposed work is the automatic generation of three fluorescence images from a conventional bright-field image; this can greatly reduce the time-consuming and laborious tissue preparation process and improve throughput of the screening process. Our proposed method uses only a single bright-field image and the corresponding fluorescence images as a set of image pairs for training an end-to-end deep convolutional neural network. By leveraging deep convolutional neural networks with a set of image pairs of bright-field and corresponding fluorescence images, our proposed method can produce synthetic fluorescence images comparable to real fluorescence microscopy images with high accuracy. Our proposed model uses multi-task learning with adversarial losses to generate more accurate and realistic microscopy images. We assess the efficacy of the proposed method using real bright-field and fluorescence microscopy image datasets from patient-driven samples of a glioblastoma, and validate the method's accuracy with various quality metrics including cell number correlation (CNC), peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), cell viability correlation (CVC), error maps, and R2 correlation.},
  doi      = {10.1016/j.media.2021.101995},
  keywords = {Apoptosis,Bright-field microscopy,Cytoplasm,DAPI,Deep learning,Fluorescence microscopy,High-content screening,Precision medicine},
  pmid     = {33640720},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000414},
}

@Article{art/ZhouY_202105,
  author   = {Zhou, Yue and Chen, Houjin and Li, Yanfeng and Liu, Qin and Xu, Xuanang and Wang, Shu and Yap, Pew Thian and Shen, Dinggang},
  journal  = {Medical Image Analysis},
  title    = {{Multi-task learning for segmentation and classification of tumors in 3D automated breast ultrasound images}},
  year     = {2021},
  issn     = {13618423},
  month    = {may},
  pages    = {101918},
  volume   = {70},
  abstract = {Tumor classification and segmentation are two important tasks for computer-aided diagnosis (CAD) using 3D automated breast ultrasound (ABUS) images. However, they are challenging due to the significant shape variation of breast tumors and the fuzzy nature of ultrasound images (e.g., low contrast and signal to noise ratio). Considering the correlation between tumor classification and segmentation, we argue that learning these two tasks jointly is able to improve the outcomes of both tasks. In this paper, we propose a novel multi-task learning framework for joint segmentation and classification of tumors in ABUS images. The proposed framework consists of two sub-networks: an encoder-decoder network for segmentation and a light-weight multi-scale network for classification. To account for the fuzzy boundaries of tumors in ABUS images, our framework uses an iterative training strategy to refine feature maps with the help of probability maps obtained from previous iterations. Experimental results based on a clinical dataset of 170 3D ABUS volumes collected from 107 patients indicate that the proposed multi-task framework improves tumor segmentation and classification over the single-task learning counterparts.},
  doi      = {10.1016/j.media.2020.101918},
  keywords = {AUBS image,Classification,Joint training,Multi-task learning,Segmentation},
  pmid     = {33676100},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841520302826},
}

@Article{art/WangX_202105,
  author   = {Wang, Xi and Chen, Hao and Xiang, Huiling and Lin, Huangjing and Lin, Xi and Heng, Pheng Ann},
  journal  = {Medical Image Analysis},
  title    = {{Deep virtual adversarial self-training with consistency regularization for semi-supervised medical image classification}},
  year     = {2021},
  issn     = {13618423},
  month    = {may},
  pages    = {102010},
  volume   = {70},
  abstract = {Convolutional neural networks have achieved prominent success on a variety of medical imaging tasks when a large amount of labeled training data is available. However, the acquisition of expert annotations for medical data is usually expensive and time-consuming, which poses a great challenge for supervised learning approaches. In this work, we proposed a novel semi-supervised deep learning method, i.e., deep virtual adversarial self-training with consistency regularization, for large-scale medical image classification. To effectively exploit useful information from unlabeled data, we leverage self-training and consistency regularization to harness the underlying knowledge, which helps improve the discrimination capability of training models. More concretely, the model first uses its prediction for pseudo-labeling on the weakly-augmented input image. A pseudo-label is kept only if the corresponding class probability is of high confidence. Then the model prediction is encouraged to be consistent with the strongly-augmented version of the same input image. To improve the robustness of the network against virtual adversarial perturbed input, we incorporate virtual adversarial training (VAT) on both labeled and unlabeled data into the course of training. Hence, the network is trained by minimizing a combination of three types of losses, including a standard supervised loss on labeled data, a consistency regularization loss on unlabeled data, and a VAT loss on both labeled and labeled data. We extensively evaluate the proposed semi-supervised deep learning methods on two challenging medical image classification tasks: breast cancer screening from ultrasound images and multi-class ophthalmic disease classification from optical coherence tomography B-scan images. Experimental results demonstrate that the proposed method outperforms both supervised baseline and other state-of-the-art methods by a large margin on all tasks.},
  doi      = {10.1016/j.media.2021.102010},
  keywords = {Consistency regularization,Deep learning,Semi-supervised classification},
  pmid     = {33677262},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000566},
}

@Article{art/HsuC_201807,
  author        = {Hsu, Chih-Chung and Lin, Chia-Wen and Su, Weng-Tai and Cheung, Gene},
  journal       = {IEEE Transactions on Image Processing},
  title         = {{SiGAN: Siamese Generative Adversarial Network for Identity-Preserving Face Hallucination}},
  year          = {2018},
  month         = {jul},
  number        = {12},
  pages         = {6225--6236},
  volume        = {28},
  abstract      = {Despite generative adversarial networks (GANs) can hallucinate photo-realistic high-resolution (HR) faces from low-resolution (LR) faces, they cannot guarantee preserving the identities of hallucinated HR faces, making the HR faces poorly recognizable. To address this problem, we propose a Siamese GAN (SiGAN) to reconstruct HR faces that visually resemble their corresponding identities. On top of a Siamese network, the proposed SiGAN consists of a pair of two identical generators and one discriminator. We incorporate reconstruction error and identity label information in the loss function of SiGAN in a pairwise manner. By iteratively optimizing the loss functions of the generator pair and discriminator of SiGAN, we cannot only achieve photo-realistic face reconstruction, but also ensures the reconstructed information is useful for identity recognition. Experimental results demonstrate that SiGAN significantly outperforms existing face hallucination GANs in objective face verification performance, while achieving photo-realistic reconstruction. Moreover, for input LR faces from unknown identities who are not included in training, SiGAN can still do a good job.},
  archiveprefix = {arXiv},
  arxivid       = {1807.08370},
  doi           = {10.1109/TIP.2019.2924554},
  eprint        = {1807.08370},
  keywords      = {Face hallucination,convolutional neural networks,generative adversarial networks,generative model,super-resolution},
  publisher     = {Institute of Electrical and Electronics Engineers Inc.},
  url           = {http://arxiv.org/abs/1807.08370 http://dx.doi.org/10.1109/TIP.2019.2924554},
}

@Article{art/QinY_2021,
  author        = {Qin, Yulei and Zheng, Hao and Gu, Yun and Huang, Xiaolin and Yang, Jie and Wang, Lihui and Yao, Feng and Zhu, Yue Min and Yang, Guang Zhong},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Learning Tubule-Sensitive CNNs for Pulmonary Airway and Artery-Vein Segmentation in CT}},
  year          = {2021},
  issn          = {1558254X},
  pages         = {1--1},
  abstract      = {Training convolutional neural networks (CNNs) for segmentation of pulmonary airway, artery, and vein is challenging due to sparse supervisory signals caused by the severe class imbalance between tubular targets and background. We present a CNNs-based method for accurate airway and artery-vein segmentation in non-contrast computed tomography. It enjoys superior sensitivity to tenuous peripheral bronchioles, arterioles, and venules. The method first uses a feature recalibration module to make the best use of features learned from the neural networks. Spatial information of features is properly integrated to retain relative priority of activated regions, which benefits the subsequent channel-wise recalibration. Then, attention distillation module is introduced to reinforce representation learning of tubular objects. Fine-grained details in high-resolution attention maps are passing down from one layer to its previous layer recursively to enrich context. Anatomy prior of lung context map and distance transform map is designed and incorporated for better artery-vein differentiation capacity. Extensive experiments demonstrated considerable performance gains brought by these components. Compared with state-of-the-art methods, our method extracted much more branches while maintaining competitive overall segmentation performance. Codes and models are available at http://www.pami.sjtu.edu.cn/News/56.},
  archiveprefix = {arXiv},
  arxivid       = {2012.05767},
  doi           = {10.1109/TMI.2021.3062280},
  eprint        = {2012.05767},
  keywords      = {Arteries,Atmospheric modeling,Computed tomography,Convolution,Image segmentation,Task analysis,Veins,convolutional neural networks,image segmentation,lung},
  url           = {https://ieeexplore.ieee.org/document/9363945/},
}

@Article{art/HeX_2021,
  author   = {He, Xingxin and Deng, Ying and Fang, Leyuan and Peng, Qinghua},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Multi-Modal Retinal Image Classification with Modality-Specific Attention Network}},
  year     = {2021},
  issn     = {1558254X},
  pages    = {1--1},
  abstract = {Recently, automatic diagnostic approaches have been widely used to classify ocular diseases. Most of these approaches are based on a single imaging modality (e.g., fundus photography or optical coherence tomography (OCT)), which usually only reflect the oculopathy to a certain extent, and neglect the modality-specific information among different imaging modalities. This paper proposes a novel modality-specific attention network (MSAN) for multi-modal retinal image classification, which can effectively utilize the modality-specific diagnostic features from fundus and OCT images. The MSAN comprises two attention modules to extract the modality-specific features from fundus and OCT images, respectively. Specifically, for the fundus image, ophthalmologists need to observe local and global pathologies at multiple scales (e.g., from microaneurysms at the micrometer level, optic disc at millimeter level to blood vessels through the whole eye). Therefore, we propose a multi-scale attention module to extract both the local and global features from fundus images. Moreover, large background regions exist in the OCT image, which is meaningless for diagnosis. Thus, a region-guided attention module is proposed to encode the retinal layer-related features and ignore the background in OCT images. Finally, we fuse the modality-specific features to form a multi-modal feature and train the multi-modal retinal image classification network. The fusion of modality-specific features allows the model to combine the advantages of fundus and OCT modality for a more accurate diagnosis. Experimental results on a clinically acquired multi-modal retinal image (fundus and OCT) dataset demonstrate that our MSAN outperforms other well-known single-modal and multi-modal retinal image classification methods.},
  doi      = {10.1109/TMI.2021.3059956},
  keywords = {Attention,Classification,Convolutional neural network,Fundus Photography,Multi-modal,Optical coherence tomography},
  url      = {https://ieeexplore.ieee.org/document/9363019/},
}

@Article{art/ZhangY_2021,
  author  = {Zhang, Yongtao and Li, Haimei and Du, Jie and Qin, Jing and Wang, Tianfu and Chen, Yue and Liu, Bing and Gao, Wenwen and Ma, Guolin and Lei, Baiying},
  journal = {IEEE Transactions on Medical Imaging},
  title   = {{3D Multi-attention Guided Multi-task Learning Network for Automatic Gastric Tumor Segmentation and Lymph Node Classification}},
  year    = {2021},
  issn    = {0278-0062},
  pages   = {1--1},
  doi     = {10.1109/TMI.2021.3062902},
  url     = {https://ieeexplore.ieee.org/document/9366506/},
}

@Article{art/AganjI_2021,
  author   = {Aganj, Iman and Fischl, Bruce},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Multi-Atlas Image Soft Segmentation via Computation of the Expected Label Value}},
  year     = {2021},
  issn     = {1558254X},
  pages    = {1--1},
  abstract = {The use of multiple atlases is common in medical image segmentation. This typically requires deformable registration of the atlases (or the average atlas) to the new image, which is computationally expensive and susceptible to entrapment in local optima. We propose to instead consider the probability of all possible atlas-to-image transformations and compute the expected label value (ELV), thereby not relying merely on the transformation deemed &#x201C;optimal&#x201D; by the registration method. Moreover, we do so without actually performing deformable registration, thus avoiding the associated computational costs. We evaluate our ELV computation approach by applying it to brain, liver, and pancreas segmentation on datasets of magnetic resonance and computed tomography images.},
  doi      = {10.1109/TMI.2021.3064661},
  keywords = {Biomedical imaging,CT,Computational efficiency,Convolution,Expected label value (ELV),Image segmentation,MRI,Strain,Training,Training data,atlas,soft segmentation,supervised image segmentation},
  url      = {https://ieeexplore.ieee.org/document/9373379/},
}

@Article{art/ZhangD_202105,
  author   = {Zhang, Dong and Chen, Bo and Chong, Jaron and Li, Shuo},
  journal  = {Medical Image Analysis},
  title    = {{Weakly-Supervised teacher-Student network for liver tumor segmentation from non-enhanced images}},
  year     = {2021},
  issn     = {13618423},
  month    = {may},
  pages    = {102005},
  volume   = {70},
  abstract = {Accurate liver tumor segmentation without contrast agents (non-enhanced images) avoids the contrast-agent-associated time-consuming and high risk, which offers radiologists quick and safe assistance to diagnose and treat the liver tumor. However, without contrast agents enhancing, the tumor in liver images presents low contrast and even invisible to naked eyes. Thus the liver tumor segmentation from non-enhanced images is quite challenging. We propose a Weakly-Supervised Teacher-Student network (WSTS) to address the liver tumor segmentation in non-enhanced images by leveraging additional box-level-labeled data (labeled with a tumor bounding-box). WSTS deploys a weakly-supervised teacher-student framework (TCH-ST), namely, a Teacher Module learns to detect and segment the tumor in enhanced images during training, which facilitates a Student Module to detect and segment the tumor in non-enhanced images independently during testing. To detect the tumor accurately, the WSTS proposes a Dual-strategy DRL (DDRL), which develops two tumor detection strategies by creatively introducing a relative-entropy bias in the DRL. To accurately predict a tumor mask for the box-level-labeled enhanced image and thus improve tumor segmentation in non-enhanced images, the WSTS proposes an Uncertainty-Sifting Self-Ensembling (USSE). The USSE exploits the weakly-labeled data with self-ensembling and evaluates the prediction reliability with a newly-designed Multi-scale Uncertainty-estimation. WSTS is validated with a 2D MRI dataset, where the experiment achieves 83.11% of Dice and 85.12% of Recall in 50 patient testing data after training by 200 patient data (half amount data is box-level-labeled). Such a great result illustrates the competence of WSTS to segment the liver tumor from non-enhanced images. Thus, WSTS has excellent potential to assist radiologists by liver tumor segmentation without contrast-agents.},
  doi      = {10.1016/j.media.2021.102005},
  keywords = {Deep reinforcement learning,Liver tumor segmentation,Self-ensembling,Teacher-student,Uncertainty-estimation},
  pmid     = {33676099},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000517},
}

@Article{art/KuangH_202105,
  author   = {Kuang, Hulin and Menon, Bijoy K and Sohn, Sung Il and Qiu, Wu},
  journal  = {Medical image analysis},
  title    = {{EIS-Net: Segmenting early infarct and scoring ASPECTS simultaneously on non-contrast CT of patients with acute ischemic stroke.}},
  year     = {2021},
  issn     = {1361-8423},
  month    = {may},
  pages    = {101984},
  volume   = {70},
  abstract = {Detecting early infarct (EI) plays an essential role in patient selection for reperfusion therapy in the management of acute ischemic stroke (AIS). EI volume at acute or hyper-acute stage can be measured using advanced pre-treatment imaging, such as MRI and CT perfusion. In this study, a novel multi-task learning approach, EIS-Net, is proposed to segment EI and score Alberta Stroke Program Early CT Score (ASPECTS) simultaneously on baseline non-contrast CT (NCCT) scans of AIS patients. The EIS-Net comprises of a 3D triplet convolutional neural network (T-CNN) for EI segmentation and a multi-region classification network for ASPECTS scoring. T-CNN has triple encoders with original NCCT, mirrored NCCT, and atlas as inputs, as well as one decoder. A comparison disparity block (CDB) is designed to extract and enhance image contexts. In the decoder, a multi-level attention gate module (MAGM) is developed to recalibrate the features of the decoder for both segmentation and classification tasks. Evaluations using a high-quality dataset comprising of baseline NCCT and concomitant diffusion weighted MRI (DWI) as reference standard of 260 patients with AIS show that the proposed EIS-Net can accurately segment EI. The EIS-Net segmented EI volume strongly correlates with EI volume on DWI (r=0.919), and the mean difference between the two volumes is 8.5 mL. For ASPECTS scoring, the proposed EIS-Net achieves an intraclass correlation coefficient of 0.78 for total 10-point ASPECTS and a kappa of 0.75 for dichotomized ASPECTS (≤ 4 vs. >4). Both EI segmentation and ASPECTS scoring tasks achieve state-of-the-art performances.},
  doi      = {10.1016/j.media.2021.101984},
  pmid     = {33676101},
  url      = {http://www.ncbi.nlm.nih.gov/pubmed/33676101},
}

@Article{art/FengR_2021,
  author    = {Feng, Ruiwei and Zheng, Xiangshang and Gao, Tianxiang and Chen, Jintai and Wang, Wenzhe and Chen, Danny Z. and Wu, Jian},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{Interactive Few-Shot Learning: Limited Supervision, Better Medical Image Segmentation}},
  year      = {2021},
  issn      = {1558254X},
  number    = {10},
  pages     = {2575--2588},
  volume    = {40},
  abstract  = {Many known supervised deep learning methods for medical image segmentation suffer an expensive burden of data annotation for model training. Recently, few-shot segmentation methods were proposed to alleviate this burden, but such methods often showed poor adaptability to the target tasks. By prudently introducing interactive learning into the few-shot learning strategy, we develop a novel few-shot segmentation approach called Interactive Few-shot Learning (IFSL), which not only addresses the annotation burden of medical image segmentation models but also tackles the common issues of the known few-shot segmentation methods. First, we design a new few-shot segmentation structure, called Medical Prior-based Few-shot Learning Network (MPrNet), which uses only a few annotated samples (e.g., 10 samples) as support images to guide the segmentation of query images without any pre-training. Then, we propose an Interactive Learning-based Test Time Optimization Algorithm (IL-TTOA) to strengthen our MPrNet on the fly for the target task in an interactive fashion. To our best knowledge, our IFSL approach is the first to allow few-shot segmentation models to be optimized and strengthened on the target tasks in an interactive and controllable manner. Experiments on four few-shot segmentation tasks show that our IFSL approach outperforms the state-of-the-art methods by more than 20% in the DSC metric. Specifically, the interactive optimization algorithm (IL-TTOA) further contributes 10% DSC improvement for the few-shot segmentation models.},
  doi       = {10.1109/TMI.2021.3060551},
  keywords  = {Medical image segmentation,few-shot learning,interactive learning,limited supervision},
  pmid      = {33606628},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/WuH_202105,
  author    = {Wu, Huisi and Wang, Wei and Zhong, Jiafu and Lei, Baiying and Wen, Zhenkun and Qin, Jing},
  journal   = {Medical Image Analysis},
  title     = {{SCS-Net: A Scale and Context Sensitive Network for Retinal Vessel Segmentation}},
  year      = {2021},
  issn      = {13618423},
  month     = {may},
  pages     = {102025},
  volume    = {70},
  abstract  = {Accurately segmenting retinal vessel from retinal images is essential for the detection and diagnosis of many eye diseases. However, it remains a challenging task due to (1) the large variations of scale in the retinal vessels and (2) the complicated anatomical context of retinal vessels, including complex vasculature and morphology, the low contrast between some vessels and the background, and the existence of exudates and hemorrhage. It is difficult for a model to capture representative and distinguishing features for retinal vessels under such large scale and semantics variations. Limited training data also make this task even harder. In order to comprehensively tackle these challenges, we propose a novel scale and context sensitive network (a.k.a., SCS−Net) for retinal vessel segmentation. We first propose a scale-aware feature aggregation (SFA) module, aiming at dynamically adjusting the receptive fields to effectively extract multi-scale features. Then, an adaptive feature fusion (AFF) module is designed to guide efficient fusion between adjacent hierarchical features to capture more semantic information. Finally, a multi-level semantic supervision (MSS) module is employed to learn more distinctive semantic representation for refining the vessel maps. We conduct extensive experiments on the six mainstream retinal image databases (DRIVE, CHASEDB1, STARE, IOSTAR, HRF, and LES-AV). The experimental results demonstrate the effectiveness of the proposed SCS-Net, which is capable of achieving better segmentation performance than other state-of-the-art approaches, especially for the challenging cases with large scale variations and complex context environments.},
  doi       = {10.1016/j.media.2021.102025},
  keywords  = {Adaptive feature fusion,Multi-level semantic supervision,Retinal vessel segmentation,Scale-aware feature aggregation},
  pmid      = {33721692},
  publisher = {Elsevier},
  url       = {https://linkinghub.elsevier.com/retrieve/pii/S1361841521000712},
}

@Article{art/HussainM_2021,
  author    = {Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{Cascaded Localization Regression Neural Nets for Kidney Localization and Segmentation-free Volume Estimation}},
  year      = {2021},
  issn      = {1558254X},
  abstract  = {Kidney volume is an essential biomarker for a number of kidney disease diagnoses, for example, chronic kidney disease. Existing total kidney volume estimation methods often rely on an intermediate kidney segmentation step. On the other hand, automatic kidney localization in volumetric medical images is a critical step that often precedes subsequent data processing and analysis. Most current approaches perform kidney localization via an intermediate classification or regression step. This paper proposes an integrated deep learning approach for (i) kidney localization in computed tomography scans and (ii) segmentation-free renal volume estimation. Our localization method uses a selection-convolutional neural network that approximates the kidney inferior-superior span along the axial direction. Cross-sectional (2D) slices from the estimated span are subsequently used in a combined sagittal-axial Mask-RCNN that detects the organ bounding boxes on the axial and sagittal slices, the combination of which produces a final 3D organ bounding box. Furthermore, we use a fully convolutional network to estimate the kidney volume that skips the segmentation procedure. We also present a mathematical expression to approximate the &#x2018;volume error&#x2019; metric from the &#x2018;S&#x00F8;rensen&#x2013;Dice coefficient.&#x2019; We accessed 100 patients&#x2019; CT scans from the Vancouver General Hospital records and obtained 210 patients&#x2019; CT scans from the 2019 Kidney Tumor Segmentation Challenge database to validate our method. Our method produces a kidney boundary wall localization error of &#x007E;2.4mm and a mean volume estimation error of &#x007E;5%.},
  doi       = {10.1109/TMI.2021.3060465},
  keywords  = {CNN,Computed tomography,FCN,Image segmentation,Kidney,Location awareness,Mask-RCNN,S&#x00F8;rensen&#x2013;Dice,Three-dimensional displays,Two dimensional displays,Volume measurement,kidney localization,kidney volume},
  pmid      = {33606626},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/ZhouH_2021,
  author        = {Zhou, Haoyin and Jayender, Jagadeesan},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Real-time Nonrigid Mosaicking of Laparoscopy Images}},
  year          = {2021},
  issn          = {1558254X},
  pages         = {1--1},
  abstract      = {The ability to extend the field of view of laparoscopy images can help the surgeons to obtain a better understanding of the anatomical context. However, due to tissue deformation, complex camera motion and significant three-dimensional (3D) anatomical surface, image pixels may have non-rigid deformation and traditional mosaicking methods cannot work robustly for laparoscopy images in real-time. To solve this problem, a novel two-dimensional (2D) non-rigid simultaneous localization and mapping (SLAM) system is proposed in this paper, which is able to compensate for the deformation of pixels and perform image mosaicking in real-time. The key algorithm of this 2D non-rigid SLAM system is the expectation maximization and dual quaternion (EMDQ) algorithm, which can generate smooth and dense deformation field from sparse and noisy image feature matches in real-time. An uncertainty-based loop closing method has been proposed to reduce the accumulative errors. To achieve real-time performance, both CPU and GPU parallel computation technologies are used for dense mosaicking of all pixels. Experimental results on in vivo and synthetic data demonstrate the feasibility and accuracy of our non-rigid mosaicking method.},
  archiveprefix = {arXiv},
  arxivid       = {2103.07414},
  doi           = {10.1109/TMI.2021.3065030},
  eprint        = {2103.07414},
  keywords      = {2D non-rigid SLAM,Cameras,EMDQ,Laparoscopes,Real-time systems,Simultaneous localization and mapping,Strain,Three-dimensional displays,Two dimensional displays,image mosaicking,mismatch removal,uncertainty},
  url           = {http://arxiv.org/abs/2103.07414%0Ahttp://dx.doi.org/10.1109/TMI.2021.3065030},
}

@Article{art/PengY_2021,
  author   = {Peng, Yuanyuan and Zhu, Weifang and Chen, Zhongyue and Wang, Meng and Geng, Le and Yu, Kai and Zhou, Yi and Wang, Ting and Xiang, Daoman and Chen, Feng and Chen, Xinjian},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Automatic Staging for Retinopathy of Prematurity with Deep Feature Fusion and Ordinal Classification Strategy}},
  year     = {2021},
  issn     = {1558254X},
  pages    = {1--1},
  abstract = {Retinopathy of prematurity (ROP) is a retinal disease which frequently occurs in premature babies with low birth weight and is considered as one of the major preventable causes of childhood blindness. Although automatic and semi-automatic diagnoses of ROP based on fundus image have been researched, most of the previous studies focused on plus disease detection and ROP screening. There are few studies focusing on ROP staging, which is important for the severity evaluation of the disease. To be consistent with clinical 5-level ROP staging, a novel and effective deep neural network based 5-level ROP staging network is proposed, which consists of multi-stream based parallel feature extractor, concatenation based deep feature fuser and clinical practice based ordinal classifier. First, the three-stream parallel framework including ResNet18, DenseNet121 and EfficientNetB2 is proposed as the feature extractor, which can extract rich and diverse high-level features. Second, the features from three streams are deeply fused by concatenation and convolution to generate a more effective and comprehensive feature. Finally, in the classification stage, an ordinal classification strategy is adopted, which can effectively improve the ROP staging performance. The proposed ROP staging network was evaluated with per-image and per-examination strategies. For per-image ROP staging, the proposed method was evaluated on 635 retinal fundus images from 196 examinations, including 303 Normal, 26 Stage 1, 127 Stage 2, 106 Stage 3, 61 Stage 4 and 12 Stage 5, which achieves 0.9055 for weighted recall, 0.9092 for weighted precision, 0.9043 for weighted F1 score, 0.9827 for accuracy with 1 (ACC1) and 0.9786 for Kappa, respectively. While for per-examination ROP staging, 1173 examinations with a 4-fold cross validation strategy were used to evaluate the effectiveness of the proposed method, which prove the validity and advantage of the proposed method.},
  doi      = {10.1109/TMI.2021.3065753},
  keywords = {Automatic Staging,Blindness,Convolution,Data mining,Feature Fusion,Feature extraction,Fundus Images,Ordinal Classification,Retinal vessels,Retinopathy,Retinopathy of Prematurity,Standards},
  url      = {https://ieeexplore.ieee.org/document/9376907/},
}

@Article{art/KimB_2021,
  author        = {Kim, Bach Ngoc and Dolz, Jose and Jodoin, Pierre Marc and Desrosiers, Christian},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Privacy-Net: An Adversarial Approach for Identity-Obfuscated Segmentation of Medical Images}},
  year          = {2021},
  issn          = {1558254X},
  pages         = {1--1},
  abstract      = {This paper presents a client/server privacy-preserving network in the context of multicentric medical image analysis. Our approach is based on adversarial learning which encodes images to obfuscate the patient identity while preserving enough information for a target task. Our novel architecture is composed of three components: 1) an encoder network which removes identity-specific features from input medical images, 2) a discriminator network that attempts to identify the subject from the encoded images, 3) a medical image analysis network which analyzes the content of the encoded images (segmentation in our case). By simultaneously fooling the discriminator and optimizing the medical analysis network, the encoder learns to remove privacy-specific features while keeping those essentials for the target task. Our approach is illustrated on the problem of segmenting brain MRI from the large-scale Parkinson Progression Marker Initiative (PPMI) dataset. Using longitudinal data from PPMI, we show that the discriminator learns to heavily distort input images while allowing for highly accurate segmentation results. Our results also demonstrate that an encoder trained on the PPMI dataset can be used for segmenting other datasets, without the need for retraining. The code is made available at: https://github.com/bachkimn/Privacy-Net-An-Adversarial-Approach-forIdentity-Obfuscated-Segmentation-of-MedicalImages.},
  archiveprefix = {arXiv},
  arxivid       = {1909.04087},
  doi           = {10.1109/TMI.2021.3065727},
  eprint        = {1909.04087},
  keywords      = {Adversarial,Biomedical imaging,Deep Learning,Image analysis,Image segmentation,Medical Images,Privacy,Privacy-preserving,Segmentation,Servers,Task analysis,Training},
  url           = {https://ieeexplore.ieee.org/document/9376890/},
}

@Article{art/XiongZ_202101,
  author  = {Xiong, Zhaohan and Xia, Qing and Hu, Zhiqiang and Huang, Ning and Bian, Cheng and Zheng, Yefeng and Vesal, Sulaiman and Ravikumar, Nishant and Maier, Andreas and Yang, Xin and Heng, Pheng-Ann and Ni, Dong and Li, Caizi and Tong, Qianqian and Si, Weixin and Puybareau, Elodie and Khoudli, Younes and G{\'{e}}raud, Thierry and Chen, Chen and Bai, Wenjia and Rueckert, Daniel and Xu, Lingchao and Zhuang, Xiahai and Luo, Xinzhe and Jia, Shuman and Sermesant, Maxime and Liu, Yashu and Wang, Kuanquan and Borra, Davide and Masci, Alessandro and Corsi, Cristiana and de Vente, Coen and Veta, Mitko and Karim, Rashed and Preetha, Chandrakanth Jayachandran and Engelhardt, Sandy and Qiao, Menyun and Wang, Yuanyuan and Tao, Qian and Nu{\~{n}}ez-Garcia, Marta and Camara, Oscar and Savioli, Nicolo and Lamata, Pablo and Zhao, Jichao},
  journal = {Medical Image Analysis},
  title   = {{A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging}},
  year    = {2021},
  issn    = {13618415},
  month   = {jan},
  pages   = {101832},
  volume  = {67},
  doi     = {10.1016/j.media.2020.101832},
  url     = {https://linkinghub.elsevier.com/retrieve/pii/S1361841520301961},
}

@Article{art/SunY_2021,
  author    = {Sun, Yue and Gao, Kun and Wu, Zhengwang and Li, Guannan and Zong, Xiaopeng and Lei, Zhihao and Wei, Ying and Ma, Jun and Yang, Xiaoping and Feng, Xue and Zhao, Li and {Le Phan}, Trung and Shin, Jitae and Zhong, Tao and Zhang, Yu and Yu, Lequan and Li, Caizi and Basnet, Ramesh and {Omair Ahmad}, M. and Swamy, M. N.S. and Ma, Wenao and Dou, Qi and Bui, Toan Duc and Noguera, Camilo Bermudez and Landman, Bennett and Gotlib, Ian H. and Humphreys, Kathryn L. and Shultz, Sarah and Li, Longchuan and Niu, Sijie and Lin, Weili and Jewells, Valerie and Shen, Dinggang and Li, Gang and Wang, Li},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{Multi-Site Infant Brain Segmentation Algorithms: The iSeg-2019 Challenge}},
  year      = {2021},
  issn      = {1558254X},
  abstract  = {To better understand early brain development in health and disorder, it is critical to accurately segment infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF). Deep learning-based methods have achieved state-of-the-art performance; however, one of the major limitations is that the learning-based methods may suffer from the multi-site issue, that is, the models trained on a dataset from one site may not be applicable to the datasets acquired from other sites with different imaging protocols/scanners. To promote methodological development in the community, the iSeg-2019 challenge (http://iseg2019.web.unc.edu) provides a set of 6-month infant subjects from multiple sites with different protocols/scanners for the participating methods. Training/validation subjects are from UNC (MAP) and testing subjects are from UNC/UMN (BCP), Stanford University, and Emory University. By the time of writing, there are 30 automatic segmentation methods participated in the iSeg-2019. In this article, 8 top-ranked methods were reviewed by detailing their pipelines/implementations, presenting experimental results, and evaluating performance across different sites in terms of whole brain, regions of interest, and gyral landmark curves. We further pointed out their limitations and possible directions for addressing the multi-site issue. We find that multi-site consistency is still an open issue. We hope that the multi-site dataset in the iSeg-2019 and this review article will attract more researchers to address the challenging and critical multi-site issue in practice.},
  doi       = {10.1109/TMI.2021.3055428},
  keywords  = {Brain,Image segmentation,Infant brain segmentation,Magnetic resonance imaging,Manuals,Pediatrics,Testing,Training,deep learning,domain adaption,isointense phase,low tissue contrast,multi-site issue},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@Article{art/VianiN_202112,
  author    = {Viani, Natalia and Botelle, Riley and Kerwin, Jack and Yin, Lucia and Patel, Rashmi and Stewart, Robert and Velupillai, Sumithra},
  journal   = {Scientific Reports},
  title     = {{A natural language processing approach for identifying temporal disease onset information from mental healthcare text}},
  year      = {2021},
  issn      = {20452322},
  month     = {dec},
  number    = {1},
  pages     = {757},
  volume    = {11},
  abstract  = {Receiving timely and appropriate treatment is crucial for better health outcomes, and research on the contribution of specific variables is essential. In the mental health domain, an important research variable is the date of psychosis symptom onset, as longer delays in treatment are associated with worse intervention outcomes. The growing adoption of electronic health records (EHRs) within mental health services provides an invaluable opportunity to study this problem at scale retrospectively. However, disease onset information is often only available in open text fields, requiring natural language processing (NLP) techniques for automated analyses. Since this variable can be documented at different points during a patient's care, NLP methods that model clinical and temporal associations are needed. We address the identification of psychosis onset by: 1) manually annotating a corpus of mental health EHRs with disease onset mentions, 2) modelling the underlying NLP problem as a paragraph classification approach, and 3) combining multiple onset paragraphs at the patient level to generate a ranked list of likely disease onset dates. For 22/31 test patients (71%) the correct onset date was found among the top-3 NLP predictions. The proposed approach was also applied at scale, allowing an onset date to be estimated for 2483 patients.},
  doi       = {10.1038/s41598-020-80457-0},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Viani et al. - 2021 - A natural language processing approach for identifying temporal disease onset information from mental healthcare t.pdf:pdf},
  keywords  = {Data mining,Machine learning,Psychiatric disorders,Schizophrenia},
  pmid      = {33436814},
  publisher = {Nature Research},
  url       = {https://doi.org/10.1038/s41598-020-80457-0},
}

@Article{art/KimY_202012,
  author    = {Kim, Yoojoong and Lee, Jeong Hyeon and Choi, Sunho and Lee, Jeong Moon and Kim, Jong Ho and Seok, Junhee and Joo, Hyung Joon},
  journal   = {Scientific Reports},
  title     = {{Validation of deep learning natural language processing algorithm for keyword extraction from pathology reports in electronic health records}},
  year      = {2020},
  issn      = {20452322},
  month     = {dec},
  number    = {1},
  pages     = {20265},
  volume    = {10},
  abstract  = {Pathology reports contain the essential data for both clinical and research purposes. However, the extraction of meaningful, qualitative data from the original document is difficult due to the narrative and complex nature of such reports. Keyword extraction for pathology reports is necessary to summarize the informative text and reduce intensive time consumption. In this study, we employed a deep learning model for the natural language process to extract keywords from pathology reports and presented the supervised keyword extraction algorithm. We considered three types of pathological keywords, namely specimen, procedure, and pathology types. We compared the performance of the present algorithm with the conventional keyword extraction methods on the 3115 pathology reports that were manually labeled by professional pathologists. Additionally, we applied the present algorithm to 36,014 unlabeled pathology reports and analysed the extracted keywords with biomedical vocabulary sets. The results demonstrated the suitability of our model for practical application in extracting important data from pathology reports.},
  doi       = {10.1038/s41598-020-77258-w},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2020 - Validation of deep learning natural language processing algorithm for keyword extraction from pathology reports in e.pdf:pdf},
  keywords  = {Biomedical engineering,Data mining,Machine learning},
  pmid      = {33219276},
  publisher = {Nature Research},
  url       = {https://www.nature.com/articles/s41598-020-77258-w},
}

@Article{art/KimC_201902,
  author    = {Kim, Chulho and Zhu, Vivienne and Obeid, Jihad and Lenert, Leslie},
  journal   = {PLoS ONE},
  title     = {{Natural language processing and machine learning algorithm to identify brain MRI reports with acute ischemic stroke}},
  year      = {2019},
  issn      = {19326203},
  month     = {feb},
  number    = {2},
  pages     = {e0212778},
  volume    = {14},
  abstract  = {Background and purpose This project assessed performance of natural language processing (NLP) and machine learning (ML) algorithms for classification of brain MRI radiology reports into acute ischemic stroke (AIS) and non-AIS phenotypes. Materials and methods All brain MRI reports from a single academic institution over a two year period were randomly divided into 2 groups for ML: Training (70%) and testing (30%). Using "quanteda" NLP package, all text data were parsed into tokens to create the data frequency matrix. Ten-fold cross-validation was applied for bias correction of the training set. Labeling for AIS was performed manually, identifying clinical notes. We applied binary logistic regression, na{\"{i}}ve Bayesian classification, single decision tree, and support vector machine for the binary classifiers, and we assessed performance of the algorithms by F1-measure. We also assessed how n-grams or term frequency-inverse document frequency weighting affected the performance of the algorithms. Results Of all 3,204 brain MRI documents, 432 (14.3%) were labeled as AIS. AIS documents were longer in character length than those of non-AIS (median [interquartile range]; 551 [377- 681] vs. 309 [164-396]). Of all ML algorithms, single decision tree had the highest F1-measure (93.2) and accuracy (98.0%). Adding bigrams to the ML model improved F1-mesaure of na{\"{i}}ve Bayesian classification, but not in others, and term frequency-inverse document frequency weighting to data frequency matrix did not show any additional performance improvements. Conclusions Supervised ML based NLP algorithms are useful for automatic classification of brain MRI reports for identification of AIS patients. Single decision tree was the best classifier to identify brain MRI reports with AIS.},
  doi       = {10.1371/journal.pone.0212778},
  editor    = {Shawe-Taylor, John},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2019 - Natural language processing and machine learning algorithm to identify brain MRI reports with acute ischemic stroke.pdf:pdf},
  keywords  = {Artificial intelligence,Decision trees,Ischemic stroke,Machine learning algorithms,Magnetic resonance imaging,Natural language processing,Stroke,Support vector machines},
  pmid      = {30818342},
  publisher = {Public Library of Science},
  url       = {https://dx.plos.org/10.1371/journal.pone.0212778},
}

@Article{art/LiangH_201903,
  author    = {Liang, Huiying and Tsui, Brian Y. and Ni, Hao and Valentim, Carolina C.S. and Baxter, Sally L. and Liu, Guangjian and Cai, Wenjia and Kermany, Daniel S. and Sun, Xin and Chen, Jiancong and He, Liya and Zhu, Jie and Tian, Pin and Shao, Hua and Zheng, Lianghong and Hou, Rui and Hewett, Sierra and Li, Gen and Liang, Ping and Zang, Xuan and Zhang, Zhiqi and Pan, Liyan and Cai, Huimin and Ling, Rujuan and Li, Shuhua and Cui, Yongwang and Tang, Shusheng and Ye, Hong and Huang, Xiaoyan and He, Waner and Liang, Wenqing and Zhang, Qing and Jiang, Jianmin and Yu, Wei and Gao, Jianqun and Ou, Wanxing and Deng, Yingmin and Hou, Qiaozhen and Wang, Bei and Yao, Cuichan and Liang, Yan and Zhang, Shu and Duan, Yaou and Zhang, Runze and Gibson, Sarah and Zhang, Charlotte L. and Li, Oulan and Zhang, Edward D. and Karin, Gabriel and Nguyen, Nathan and Wu, Xiaokang and Wen, Cindy and Xu, Jie and Xu, Wenqin and Wang, Bochu and Wang, Winston and Li, Jing and Pizzato, Bianca and Bao, Caroline and Xiang, Daoman and He, Wanting and He, Suiqin and Zhou, Yugui and Haw, Weldon and Goldbaum, Michael and Tremoulet, Adriana and Hsu, Chun Nan and Carter, Hannah and Zhu, Long and Zhang, Kang and Xia, Huimin},
  journal   = {Nature Medicine},
  title     = {{Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence}},
  year      = {2019},
  issn      = {1546170X},
  month     = {mar},
  number    = {3},
  pages     = {433--438},
  volume    = {25},
  abstract  = {Artificial intelligence (AI)-based methods have emerged as powerful tools to transform medical care. Although machine learning classifiers (MLCs) have already demonstrated strong performance in image-based diagnoses, analysis of diverse and massive electronic health record (EHR) data remains challenging. Here, we show that MLCs can query EHRs in a manner similar to the hypothetico-deductive reasoning used by physicians and unearth associations that previous statistical methods have not found. Our model applies an automated natural language processing system using deep learning techniques to extract clinically relevant information from EHRs. In total, 101.6 million data points from 1,362,559 pediatric patient visits presenting to a major referral center were analyzed to train and validate the framework. Our model demonstrates high diagnostic accuracy across multiple organ systems and is comparable to experienced pediatricians in diagnosing common childhood diseases. Our study provides a proof of concept for implementing an AI-based system as a means to aid physicians in tackling large amounts of data, augmenting diagnostic evaluations, and to provide clinical decision support in cases of diagnostic uncertainty or complexity. Although this impact may be most evident in areas where healthcare providers are in relative shortage, the benefits of such an AI system are likely to be universal.},
  doi       = {10.1038/s41591-018-0335-9},
  file      = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang et al. - 2019 - Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence.pdf:pdf},
  keywords  = {Computational biology and bioinformatics,Data integration,Health care,Paediatrics},
  pmid      = {30742121},
  publisher = {Nature Publishing Group},
  url       = {https://doi.org/10.1038/s41591-018-0335-9},
}

@InCollection{art/CarseJ_2019,
  author    = {Carse, Jacob and McKenna, Stephen},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Active Learning for Patch-Based Digital Pathology Using Convolutional Neural Networks to Reduce Annotation Costs}},
  year      = {2019},
  isbn      = {9783030239367},
  pages     = {20--27},
  volume    = {11435 LNCS},
  abstract  = {Methods to reduce the need for costly data annotations become increasingly important as deep learning gains popularity in medical image analysis and digital pathology. Active learning is an appealing approach that can reduce the amount of annotated data needed to train machine learning models but traditional active learning strategies do not always work well with deep learning. In patch-based machine learning systems, active learning methods typically request annotations for small individual patches which can be tedious and costly for the annotator who needs to rely on visual context for the patches. We propose an active learning framework that selects regions for annotation that are built up of several patches, which should increase annotation throughput. The framework was evaluated with several query strategies on the task of nuclei classification. Convolutional neural networks were trained on small patches, each containing a single nucleus. Traditional query strategies performed worse than random sampling. A K-centre sampling strategy showed a modest gain. Further investigation is needed in order to achieve significant performance gains using deep active learning for this task.},
  doi       = {10.1007/978-3-030-23937-4_3},
  issn      = {16113349},
  keywords  = {Active learning,Deep learning,Image annotation,Nuclei classification},
  url       = {http://link.springer.com/10.1007/978-3-030-23937-4_3},
}

@InProceedings{art/ShaoW_201804,
  author    = {Shao, Wei and Sun, Liang and Zhang, Daoqiang},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  title     = {{Deep active learning for nucleus classification in pathology images}},
  year      = {2018},
  month     = {apr},
  pages     = {199--202},
  publisher = {IEEE},
  volume    = {2018-April},
  abstract  = {The systematic study of nuclei patterns in pathology images is very important for fully charactering the grade of cancerous tissues. Nowadays, with the great advance of deep neural networks (i.e., DNN), intense interest in adopting DNN to distinguish different types of pathology nuclei is widely spread. However, most of the existing methods need to annotate lots of nuclei images in the training stage, and this is not always an option for the labelling cost are high. To address this problem, we propose a novel approach called DAPC (i.e., deep active learning with pairwise constraints) to actively select the most valuable nuclei for annotation. Specifically, we firstly design a novel pairwise-constraint regularized deep convolutional neural network (i.e., CNN) that can simultaneously preserve the distribution of different subjects and optimize the objective criterion of conventional CNN. Then, through the properly designed CNN, we query the most informative nuclei in the unlabelled dataset for human annotation, and the parameters of the designed CNN is subsequently updated by incorporating the newly annotated samples to enhance the CNNs performance incrementally. We evaluate our method on a public available pathology colon dataset, the experimental results show that the proposed method could achieves to a weighted F1-score of 79.2% by only annotating 60% nuclei in the training set, which is better than the comparing methods.},
  doi       = {10.1109/ISBI.2018.8363554},
  isbn      = {9781538636367},
  issn      = {19458452},
  keywords  = {Active Learning,Cell Nucleus Classification,Deep Learning,Pair-wise Constraints},
  url       = {https://ieeexplore.ieee.org/document/8363554/},
}

@Article{art/LinT_201405,
  author        = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Microsoft COCO: Common objects in context}},
  year          = {2014},
  issn          = {16113349},
  month         = {may},
  number        = {PART 5},
  pages         = {740--755},
  volume        = {8693 LNCS},
  abstract      = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. {\textcopyright} 2014 Springer International Publishing.},
  archiveprefix = {arXiv},
  arxivid       = {1405.0312},
  doi           = {10.1007/978-3-319-10602-1_48},
  eprint        = {1405.0312},
  url           = {http://arxiv.org/abs/1405.0312},
}

@Article{art/DengL_201211,
  author   = {Deng, Li},
  journal  = {IEEE Signal Processing Magazine},
  title    = {{The MNIST database of handwritten digit images for machine learning research}},
  year     = {2012},
  issn     = {10535888},
  month    = {nov},
  number   = {6},
  pages    = {141--142},
  volume   = {29},
  abstract = {In this issue, Best of the Web presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research. {\textcopyright} 2012 IEEE.},
  doi      = {10.1109/MSP.2012.2211477},
  url      = {http://ieeexplore.ieee.org/document/6296535/},
}

@InProceedings{art/DengJ_201006,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {{ImageNet: A large-scale hierarchical image database}},
  year      = {2010},
  month     = {jun},
  pages     = {248--255},
  publisher = {IEEE},
  abstract  = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  doi       = {10.1109/cvpr.2009.5206848},
  isbn      = {978-1-4244-3992-8},
  url       = {https://ieeexplore.ieee.org/document/5206848/},
}

@Misc{art/ISIC_2020,
  author    = {{International Skin Imaging Collaboration}},
  title     = {{SIIM-ISIC 2020 Challenge Dataset}},
  year      = {2020},
  doi       = {10.34970/2020-DS01},
  publisher = {International Skin Imaging Collaboration},
  url       = {https://challenge2020.isic-archive.com/},
}

@Article{art/XiaW_202101,
  author        = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},
  title         = {{GAN Inversion: A Survey}},
  year          = {2021},
  month         = {jan},
  abstract      = {GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model, for the image to be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling the pretrained GAN models such as StyleGAN and BigGAN to be used for real image editing applications. Meanwhile, GAN inversion also provides insights on the interpretation of GAN's latent space and how the realistic images can be generated. In this paper, we provide an overview of GAN inversion with a focus on its recent algorithms and applications. We cover important techniques of GAN inversion and their applications to image restoration and image manipulation. We further elaborate on some trends and challenges for future directions.},
  archiveprefix = {arXiv},
  arxivid       = {2101.05278},
  eprint        = {2101.05278},
  url           = {http://arxiv.org/abs/2101.05278},
}

@Article{art/KhanS_202101,
  author        = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  title         = {{Transformers in Vision: A Survey}},
  year          = {2021},
  month         = {jan},
  abstract      = {Astounding results from transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. This has led to exciting progress on a number of tasks while requiring minimal inductive biases in the model design. This survey aims to provide a comprehensive overview of the transformer models in the computer vision discipline and assumes little to no prior background in the field. We start with an introduction to fundamental concepts behind the success of transformer models i.e., self-supervision and self-attention. Transformer architectures leverage self-attention mechanisms to encode long-range dependencies in the input domain which makes them highly expressive. Since they assume minimal prior knowledge about the structure of the problem, self-supervision using pretext tasks is applied to pre-train transformer models on large-scale (unlabelled) datasets. The learned representations are then fine-tuned on the downstream tasks, typically leading to excellent performance due to the generalization and expressivity of encoded features. We cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering and visual reasoning), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
  archiveprefix = {arXiv},
  arxivid       = {2101.01169},
  eprint        = {2101.01169},
  url           = {http://arxiv.org/abs/2101.01169},
}

@Article{art/DosovitskiyA_202010,
  author        = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  title         = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
  year          = {2020},
  month         = {oct},
  abstract      = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  arxivid       = {2010.11929},
  eprint        = {2010.11929},
  url           = {http://arxiv.org/abs/2010.11929},
}

@Article{art/Magee_2009,
  author   = {{Magee D., Treanor D., Crellin D., Shires M., Smith K., Mohee K.}, and Quirke P},
  journal  = {Optical Tissue Image analysis in Microscopy, Histopathology and Endoscopy (MICCAI Workshop)},
  title    = {{Colour Normalisation in Digital Histopathology Images}},
  year     = {2009},
  pages    = {100--111},
  abstract = {Abstract. Colour consistency in light microscopy based histology is an increasingly important problem with the advent of Gigapixel digital slide scanners and automatic image analysis. This paper presents an evaluation of two novel colour normalisation approaches against the previously utilised method of linear normalisation in l$\alpha$$\beta$ colourspace. These approaches map the colour distribution of an over/under stained image to that of a well stained target image. The first novel approach presented is a multi-modal extension to linear normalisation in l$\alpha$$\beta$ colourspace using an automatic image segmentation method and defining separate transforms for each class. The second approach normalises in a representation space obtained using stain specific colour deconvolution. Additionally, we present a method for estimation of the required colour deconvolution vectors directly from the image data. Our evaluation demonstrates the inherent variability in the original data, the known theoretical problems with linear normalisation in l$\alpha$$\beta$ colourspace, and that a multi-modal colour deconvolution based approach overcomes these problems. The segmentation based approach, while producing good results on the majority of images, is less successful than the colour deconvolution method for a significant minority of images as robust segmentation is required to avoid introducing artifacts.},
  doi      = {10.1.1.157.5405},
  url      = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.5405},
}

@Article{art/SilverD_201601,
  author   = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  journal  = {Nature},
  title    = {{Mastering the game of Go with deep neural networks and tree search}},
  year     = {2016},
  issn     = {14764687},
  month    = {jan},
  number   = {7587},
  pages    = {484--489},
  volume   = {529},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  doi      = {10.1038/nature16961},
  pmid     = {26819042},
  url      = {http://www.nature.com/articles/nature16961},
}

@Article{art/AertsH_201409,
  author   = {Aerts, Hugo J.W.L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T.H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
  journal  = {Nature Communications},
  title    = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
  year     = {2014},
  issn     = {20411723},
  month    = {sep},
  number   = {1},
  pages    = {4006},
  volume   = {5},
  abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost. {\textcopyright} 2014 Macmillan Publishers Limited. All rights reserved.},
  doi      = {10.1038/ncomms5006},
  pmid     = {24892406},
  url      = {http://www.nature.com/articles/ncomms5006},
}

@Article{art/SetioA_201712,
  author        = {Setio, Arnaud Arindra Adiyoso and Traverso, Alberto and de Bel, Thomas and Berens, Moira S.N. and van den Bogaard, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria Evelina and Geurts, Bram and van der Gugten, Robbert and Heng, Pheng Ann and Jansen, Bart and de Kaste, Michael M.J. and Kotov, Valentin and Lin, Jack Yu Hung and Manders, Jeroen T.M.C. and S{\'{o}}{\~{n}}ora-Mengana, Alexander and Garc{\'{i}}a-Naranjo, Juan Carlos and Papavasileiou, Evgenia and Prokop, Mathias and Saletta, Marco and Schaefer-Prokop, Cornelia M. and Scholten, Ernst T. and Scholten, Luuk and Snoeren, Miranda M. and Torres, Ernesto Lopez and Vandemeulebroucke, Jef and Walasek, Nicole and Zuidhof, Guido C.A. and van Ginneken, Bram and Jacobs, Colin},
  journal       = {Medical Image Analysis},
  title         = {{Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge}},
  year          = {2017},
  issn          = {13618423},
  month         = {dec},
  pages         = {1--13},
  volume        = {42},
  abstract      = {Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95% at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.},
  archiveprefix = {arXiv},
  arxivid       = {1612.08012},
  doi           = {10.1016/j.media.2017.06.015},
  eprint        = {1612.08012},
  keywords      = {Computed tomography,Computer-aided detection,Convolutional networks,Deep learning,Medical image challenges,Pulmonary nodules},
  pmid          = {28732268},
  url           = {http://arxiv.org/abs/1612.08012 http://dx.doi.org/10.1016/j.media.2017.06.015},
}

@InProceedings{art/SpanierA_2014,
  author    = {Spanier, Assaf B. and Joskowicz, Leo},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Rule-based ventral cavity multi-organ automatic segmentation in CT scans}},
  year      = {2014},
  pages     = {163--170},
  volume    = {8848},
  abstract  = {We describe a new method for the automatic segmentation of multiple organs of the ventral cavity in CT scans. The method is based on a set of rules that determine the order in which the organs are isolated and segmented. First, the air-containing organs are segmented: the trachea and the lungs. Then, the organs with high blood content: the spleen, the kidneys and the liver, are segmented. Each organ is individually segmented with a generic four-step pipeline procedure. Our method is unique in that it uses the same generic segmentation approach for all organs and in that it relies on the segmentation difficulty of organs to guide the segmentation process. Experimental results on 20 CT scans of the VISCERAL Anatomy2 Challenge training datasets yield an average Dice volume overlap similarity score of 90.95. For the 10 CT scans test datasets, the average Dice scores is 88.5.},
  doi       = {10.1007/978-3-319-13972-2_15},
  isbn      = {9783319139715},
  issn      = {16113349},
}

@Misc{art/StanfordS_2011,
  author    = {Stanford, Stanford Center for Reproducible Neuroscience},
  title     = {{OpenNEURO}},
  year      = {2011},
  abstract  = {A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data},
  booktitle = {World Wide Web},
}

@Article{art/LiewS_2017,
  author   = {Liew, Sook Lei and Anglin, Julia M. and Banks, Nick W. and Sondag, Matt and Ito, Kaori L. and Kim, Hosung and Chan, Jennifer and Ito, Joyce and Jung, Connie and Khoshab, Nima and Lefebvre, Stephanie and Nakamura, William and Saldana, David and Schmiesing, Allie and Tran, Cathy and Vo, Danny and Ard, Tyler and Heydari, Panthea and Kim, Bokkyu and Aziz-Zadeh, Lisa and Cramer, Steven C. and Liu, Jingchun and Soekadar, Surjo and Nordvik, Jan Egil and Westlye, Lars T. and Wang, Junping and Winstein, Carolee and Yu, Chunshui and Ai, Lei and Koo, Bonhwang and Craddock, R. Cameron and Milham, Michael and Lakich, Matthew and Pienta, Amy and Stroud, Alison},
  journal  = {bioRxiv},
  title    = {{A large, open source dataset of stroke anatomical brain images and manual lesion segmentations}},
  year     = {2017},
  issn     = {2052-4463},
  abstract = {Stroke is the leading cause of adult disability worldwide, with up to two-thirds of individuals experiencing long-term disabilities. Large-scale neuroimaging studies have shown promise in identifying robust biomarkers (e.g., measures of brain structure) of long-term stroke recovery following rehabilitation. However, analyzing large rehabilitation-related datasets is problematic due to barriers in accurate stroke lesion segmentation. Manually-traced lesions are currently the gold standard for lesion segmentation on T1-weighted MRIs, but are labor intensive and require anatomical expertise. While algorithms have been developed to automate this process, the results often lack accuracy. Newer algorithms that employ machine-learning techniques are promising, yet these require large training datasets to optimize performance. Here we present ATLAS (Anatomical Tracings of Lesions After Stroke), an open-source dataset of 304 T1-weighted MRIs with manually segmented lesions and metadata. This large, diverse dataset can be used to train and test lesion segmentation algorithms and provides a standardized dataset for comparing the performance of different segmentation methods. We hope ATLAS release 1.1 will be a useful resource to assess and improve the accuracy of current lesion segmentation methods.},
  doi      = {10.1101/179614},
}

@Misc{art/AlkhasliI_2019,
  author    = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M and Binkofski, Ferdinand},
  title     = {{Resting State - TMS}},
  year      = {2019},
  doi       = {10.18112/OPENNEURO.DS001832.V1.0.1},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001832/versions/1.0.1},
}

@InProceedings{art/SzegedyC_2015,
  author        = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Going deeper with convolutions}},
  year          = {2015},
  pages         = {1--9},
  volume        = {07-12-June},
  abstract      = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  arxivid       = {1409.4842},
  doi           = {10.1109/CVPR.2015.7298594},
  eprint        = {1409.4842},
  isbn          = {9781467369640},
  issn          = {10636919},
}

@Misc{art/AnP_2020,
  author    = {An, P and Xu, S and Harmon, S.A and Turkbey, E.B and Sanford, T.H. and Amalou, A. and Kassin, M. and Varble, N. and Blain, M. and Anderson, V. and Patella, F. and G., Carrafiello and Turkbey, B.T. and Wood, B.J.},
  title     = {{CT Images in Covid-19 [Data set]}},
  year      = {2020},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2020.GQRY-NC81},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/o5QvB},
}

@Article{art/SouzaR_201804,
  author   = {Souza, Roberto and Lucena, Oeslle and Garrafa, Julia and Gobbi, David and Saluzzi, Marina and Appenzeller, Simone and Rittner, Let{\'{i}}cia and Frayne, Richard and Lotufo, Roberto},
  journal  = {NeuroImage},
  title    = {{An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement}},
  year     = {2018},
  issn     = {10959572},
  month    = {apr},
  pages    = {482--494},
  volume   = {170},
  abstract = {This paper presents an open, multi-vendor, multi-field strength magnetic resonance (MR) T1-weighted volumetric brain imaging dataset, named Calgary-Campinas-359 (CC-359). The dataset is composed of images of older healthy adults (29–80 years) acquired on scanners from three vendors (Siemens, Philips and General Electric) at both 1.5 T and 3 T. CC-359 is comprised of 359 datasets, approximately 60 subjects per vendor and magnetic field strength. The dataset is approximately age and gender balanced, subject to the constraints of the available images. It provides consensus brain extraction masks for all volumes generated using supervised classification. Manual segmentation results for twelve randomly selected subjects performed by an expert are also provided. The CC-359 dataset allows investigation of 1) the influences of both vendor and magnetic field strength on quantitative analysis of brain MR; 2) parameter optimization for automatic segmentation methods; and potentially 3) machine learning classifiers with big data, specifically those based on deep learning methods, as these approaches require a large amount of data. To illustrate the utility of this dataset, we compared to the results of a supervised classifier, the results of eight publicly available skull stripping methods and one publicly available consensus algorithm. A linear mixed effects model analysis indicated that vendor (p−value<0.001) and magnetic field strength (p−value<0.001) have statistically significant impacts on skull stripping results.},
  doi      = {10.1016/j.neuroimage.2017.08.021},
  keywords = {Brain MR image analysis,Brain extraction,Brain segmentation,MP-RAGE,Public database,Skull stripping},
  pmid     = {28807870},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811917306687},
}

@Misc{art/DesaiS_2020,
  author    = {Desai, Shivang and Baghal, Ahmad and Wongsurawat, Thidathip and Jenjaroenpun, Piroon and Powell, Thomas and Al-Shukri, Shaymaa and Gates, Kim and Farmer, Phillip and Rutherford, Michael and Blake, Geri and Nolan, Tracy and Sexton, Kevin and Bennett, William and Smith, Kirk and Syed, Shorabuddin and Prior, Fred},
  title     = {{Chest imaging representing a COVID-19 positive rural U.S. population}},
  year      = {2020},
  abstract  = {As the COVID-19 pandemic unfolds, radiology imaging is playing an increasingly vital role in determining therapeutic options, patient management, and research directions. Publicly available data are essential to drive new research into disease etiology, early detection, and response to therapy. In response to the COVID-19 crisis, the National Cancer Institute (NCI) has extended the Cancer Imaging Archive (TCIA) to include COVID-19 related images. Rural populations are one population at risk for underrepresentation in such public repositories. We have published in TCIA a collection of radiographic and CT imaging studies for patients who tested positive for COVID-19 in the state of Arkansas. A set of clinical data describes each patient including demographics, comorbidities, selected lab data and key radiology findings. These data are cross-linked to SARS-COV-2 cDNA sequence data extracted from clinical isolates from the same population, uploaded to the GenBank repository. We believe this collection will help to address population imbalance in COVID-19 data by providing samples from this normally underrepresented population.},
  booktitle = {Scientific Data},
  doi       = {10.1038/s41597-020-00741-6},
  issn      = {20524463},
  number    = {1},
  pmid      = {33235265},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/C5IvB},
  volume    = {7},
}

@InProceedings{art/WeiD_2020,
  author    = {Wei, Donglai and Lin, Zudi and Franco-Barranco, Daniel and Wendt, Nils and Liu, Xingyu and Yin, Wenjie and Huang, Xin and Gupta, Aarush and Jang, Won Dong and Wang, Xueying and Arganda-Carreras, Ignacio and Lichtman, Jeff W. and Pfister, Hanspeter},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{MitoEM Dataset: Large-Scale 3D Mitochondria Instance Segmentation from EM Images}},
  year      = {2020},
  pages     = {66--76},
  volume    = {12265 LNCS},
  abstract  = {Electron microscopy (EM) allows the identification of intracellular organelles such as mitochondria, providing insights for clinical and scientific studies. However, public mitochondria segmentation datasets only contain hundreds of instances with simple shapes. It is unclear if existing methods achieving human-level accuracy on these small datasets are robust in practice. To this end, we introduce the MitoEM dataset, a 3D mitochondria instance segmentation dataset with two (30 $\mu$ m)3 volumes from human and rat cortices respectively, 3,600× larger than previous benchmarks. With around 40K instances, we find a great diversity of mitochondria in terms of shape and density. For evaluation, we tailor the implementation of the average precision (AP) metric for 3D data with a 45× speedup. On MitoEM, we find existing instance segmentation methods often fail to correctly segment mitochondria with complex shapes or close contacts with other instances. Thus, our MitoEM dataset poses new challenges to the field. We release our code and data: https://donglaiw.github.io/page/mitoEM/index.html.},
  doi       = {10.1007/978-3-030-59722-1_7},
  isbn      = {9783030597214},
  issn      = {16113349},
  keywords  = {3D instance segmentation,EM dataset,Mitochondria},
}

@Article{art/ChongY_202005,
  author   = {Chong, Yoong Min and Sam, I-Ching and Ponnampalavanar, Sasheela and {Syed Omar}, Sharifah Faridah and Kamarulzaman, Adeeba and Munusamy, Vijayan and Wong, Chee Kuan and Jamaluddin, Fadhil Hadi and Gan, Han Ming and Chong, Jennifer and Teh, Cindy Shuan Ju and Chan, Yoke Fun},
  journal  = {Microbiology Resource Announcements},
  title    = {{Complete Genome Sequences of SARS-CoV-2 Strains Detected in Malaysia}},
  year     = {2020},
  issn     = {2576-098X},
  month    = {may},
  number   = {20},
  volume   = {9},
  abstract = {We sequenced four severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomes from Malaysia during the second wave of infection and found unique mutations which suggest local evolution. Circulating Malaysian strains represent introductions from different countries, particularly during the first wave of infection. Genome sequencing is important for understanding local epidemiology.We sequenced four severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomes from Malaysia during the second wave of infection and found unique mutations which suggest local evolution. Circulating Malaysian strains represent introductions from different countries, particularly during the first wave of infection. Genome sequencing is important for understanding local epidemiology.},
  doi      = {10.1128/mra.00383-20},
  editor   = {Roux, Simon},
  url      = {https://mra.asm.org/content/9/20/e00383-20},
}

@Misc{art/LiP.WangS.LiT.LuJ.HuangFuY.&WangD_2020,
  author    = {{Li, P., Wang, S., Li, T., Lu, J., HuangFu, Y., & Wang}, D.},
  title     = {{A Large-Scale CT and PET/CT Dataset for Lung Cancer Diagnosis [Data set]}},
  year      = {2020},
  booktitle = {The Cancer Imaging Archive.},
  doi       = {10.7937/TCIA.2020.NNC2-0461},
  publisher = {The Cancer Imaging Archive},
  url       = {https://doi.org/10.7937/TCIA.2020.NNC2-0461},
}

@Article{art/LeuschnerJ_201910,
  author        = {Leuschner, Johannes and Schmidt, Maximilian and Baguer, Daniel Otero and Maa{\ss}, Peter},
  journal       = {arXiv},
  title         = {{The LoDoPaB-CT dataset: A benchmark dataset for low-dose ct reconstruction methods}},
  year          = {2019},
  month         = {oct},
  abstract      = {Deep Learning approaches for solving Inverse Problems in imaging have become very effective and are demonstrated to be quite competitive in the field. Comparing these approaches is a challenging task since they highly rely on the data and the setup that is used for training. We provide a public dataset of computed tomography images and simulated low-dose measurements suitable for training this kind of methods. With the LoDoPaB-CT Dataset we aim to create a benchmark that allows for a fair comparison. It contains over 40 000 scan slices from around 800 patients selected from the LIDC/IDRI Database. In this paper we describe how we processed the original slices and how we simulated the measurements. We also include first baseline results.},
  archiveprefix = {arXiv},
  arxivid       = {1910.01113},
  eprint        = {1910.01113},
  url           = {http://arxiv.org/abs/1910.01113},
}

@Article{art/QuellecG_2020,
  author        = {Quellec, Gwenol{\'{e}} and Lamard, Mathieu and Conze, Pierre Henri and Massin, Pascale and Cochener, B{\'{e}}atrice},
  journal       = {Medical Image Analysis},
  title         = {{Automatic detection of rare pathologies in fundus photographs using few-shot learning}},
  year          = {2020},
  issn          = {13618423},
  volume        = {61},
  abstract      = {In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.},
  archiveprefix = {arXiv},
  arxivid       = {1907.09449},
  doi           = {10.1016/j.media.2020.101660},
  eprint        = {1907.09449},
  keywords      = {Deep learning,Diabetic retinopathy screening,Few-shot learning,Rare conditions},
  pmid          = {32028213},
}

@Article{art/AliS_202012,
  author   = {Ali, Sharib and Zhou, Felix and Braden, Barbara and Bailey, Adam and Yang, Suhui and Cheng, Guanju and Zhang, Pengyi and Li, Xiaoqiong and Kayser, Maxime and Soberanis-Mukul, Roger D. and Albarqouni, Shadi and Wang, Xiaokang and Wang, Chunqing and Watanabe, Seiryo and Oksuz, Ilkay and Ning, Qingtian and Yang, Shufan and Khan, Mohammad Azam and Gao, Xiaohong W. and Realdon, Stefano and Loshchenov, Maxim and Schnabel, Julia A. and East, James E. and Wagnieres, Georges and Loschenov, Victor B. and Grisan, Enrico and Daul, Christian and Blondel, Walter and Rittscher, Jens},
  journal  = {Scientific Reports},
  title    = {{An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy}},
  year     = {2020},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {2748},
  volume   = {10},
  abstract = {We present a comprehensive analysis of the submissions to the first edition of the Endoscopy Artefact Detection challenge (EAD). Using crowd-sourcing, this initiative is a step towards understanding the limitations of existing state-of-the-art computer vision methods applied to endoscopy and promoting the development of new approaches suitable for clinical translation. Endoscopy is a routine imaging technique for the detection, diagnosis and treatment of diseases in hollow-organs; the esophagus, stomach, colon, uterus and the bladder. However the nature of these organs prevent imaged tissues to be free of imaging artefacts such as bubbles, pixel saturation, organ specularity and debris, all of which pose substantial challenges for any quantitative analysis. Consequently, the potential for improved clinical outcomes through quantitative assessment of abnormal mucosal surface observed in endoscopy videos is presently not realized accurately. The EAD challenge promotes awareness of and addresses this key bottleneck problem by investigating methods that can accurately classify, localize and segment artefacts in endoscopy frames as critical prerequisite tasks. Using a diverse curated multi-institutional, multi-modality, multi-organ dataset of video frames, the accuracy and performance of 23 algorithms were objectively ranked for artefact detection and segmentation. The ability of methods to generalize to unseen datasets was also evaluated. The best performing methods (top 15%) propose deep learning strategies to reconcile variabilities in artefact appearance with respect to size, modality, occurrence and organ type. However, no single method outperformed across all tasks. Detailed analyses reveal the shortcomings of current training strategies and highlight the need for developing new optimal metrics to accurately quantify the clinical applicability of methods.},
  doi      = {10.1038/s41598-020-59413-5},
  pmid     = {32066744},
  url      = {http://www.nature.com/articles/s41598-020-59413-5},
}

@Article{art/HarmonS_202012,
  author   = {Harmon, Stephanie A. and Sanford, Thomas H. and Xu, Sheng and Turkbey, Evrim B. and Roth, Holger and Xu, Ziyue and Yang, Dong and Myronenko, Andriy and Anderson, Victoria and Amalou, Amel and Blain, Maxime and Kassin, Michael and Long, Dilara and Varble, Nicole and Walker, Stephanie M. and Bagci, Ulas and Ierardi, Anna Maria and Stellato, Elvira and Plensich, Guido Giovanni and Franceschelli, Giuseppe and Girlando, Cristiano and Irmici, Giovanni and Labella, Dominic and Hammoud, Dima and Malayeri, Ashkan and Jones, Elizabeth and Summers, Ronald M. and Choyke, Peter L. and Xu, Daguang and Flores, Mona and Tamura, Kaku and Obinata, Hirofumi and Mori, Hitoshi and Patella, Francesca and Cariati, Maurizio and Carrafiello, Gianpaolo and An, Peng and Wood, Bradford J. and Turkbey, Baris},
  journal  = {Nature Communications},
  title    = {{Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets}},
  year     = {2020},
  issn     = {20411723},
  month    = {dec},
  number   = {1},
  pages    = {4080},
  volume   = {11},
  abstract = {Chest CT is emerging as a valuable diagnostic tool for clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to aid in rapid evaluation of CT scans for differentiation of COVID-19 findings from other clinical entities. Here we show that a series of deep learning algorithms, trained in a diverse multinational cohort of 1280 patients to localize parietal pleura/lung parenchyma followed by classification of COVID-19 pneumonia, can achieve up to 90.8% accuracy, with 84% sensitivity and 93% specificity, as evaluated in an independent test set (not included in training and validation) of 1337 patients. Normal controls included chest CTs from oncology, emergency, and pneumonia-related indications. The false positive rate in 140 patients with laboratory confirmed other (non COVID-19) pneumonias was 10%. AI-based algorithms can readily identify CT scans with COVID-19 associated pneumonia, as well as distinguish non-COVID related pneumonias with high specificity in diverse patient populations.},
  doi      = {10.1038/s41467-020-17971-2},
  pmid     = {32796848},
  url      = {http://www.nature.com/articles/s41467-020-17971-2},
}

@Article{art/TartaglioneE_202004,
  author        = {Tartaglione, Enzo and Barbano, Carlo Alberto and Berzovini, Claudio and Calandri, Marco and Grangetto, Marco},
  journal       = {International Journal of Environmental Research and Public Health},
  title         = {{Unveiling COVID-19 from chest x-ray with deep learning: A hurdles race with small data}},
  year          = {2020},
  issn          = {16604601},
  month         = {apr},
  number        = {18},
  pages         = {1--17},
  volume        = {17},
  abstract      = {The possibility to use widespread and simple chest X-ray (CXR) imaging for early screening of COVID-19 patients is attracting much interest from both the clinical and the AI community. In this study we provide insights and also raise warnings on what is reasonable to expect by applying deep learning to COVID classification of CXR images. We provide a methodological guide and critical reading of an extensive set of statistical results that can be obtained using currently available datasets. In particular, we take the challenge posed by current small size COVID data and show how significant can be the bias introduced by transfer-learning using larger public non-COVID CXR datasets. We also contribute by providing results on a medium size COVID CXR dataset, just collected by one of the major emergency hospitals in Northern Italy during the peak of the COVID pandemic. These novel data allow us to contribute to validate the generalization capacity of preliminary results circulating in the scientific community. Our conclusions shed some light into the possibility to effectively discriminate COVID using CXR.},
  archiveprefix = {arXiv},
  arxivid       = {2004.05405},
  doi           = {10.3390/ijerph17186933},
  eprint        = {2004.05405},
  keywords      = {COVID-19,Chest X-ray,Classification,Deep learning},
  pmid          = {32971995},
  url           = {http://arxiv.org/abs/2004.05405 http://dx.doi.org/10.3390/ijerph17186933},
}

@Article{art/MaguoloG_202004,
  author        = {Maguolo, Gianluca and Nanni, Loris},
  journal       = {arXiv},
  title         = {{A Critic Evaluation of Methods for COVID-19 Automatic Detection from X-Ray Images}},
  year          = {2020},
  issn          = {23318422},
  month         = {apr},
  abstract      = {In this paper, we compare and evaluate different testing protocols used for automatic COVID-19 diagnosis from X-Ray images in the recent literature. We show that similar results can be obtained using X-Ray images that do not contain most of the lungs. We are able to remove the lungs from the images by turning to black the center of the X-Ray scan and training our classifiers only on the outer part of the images. Hence, we deduce that several testing protocols for the recognition are not fair and that the neural networks are learning patterns in the dataset that are not correlated to the presence of COVID-19. Finally, we show that creating a fair testing protocol is a challenging task, and we provide a method to measure how fair a specific testing protocol is. In the future research we suggest to check the fairness of a testing protocol using our tools and we encourage researchers to look for better techniques than the ones that we propose.},
  archiveprefix = {arXiv},
  arxivid       = {2004.12823},
  eprint        = {2004.12823},
  keywords      = {Convolutional Neural Networks,Covid-19,Covid-19 Diagnosis,X-Ray Images},
  url           = {http://arxiv.org/abs/2004.12823},
}

@Article{art/AliS_2021,
  author        = {Ali, Sharib and Dmitrieva, Mariia and Ghatwary, Noha and Bano, Sophia and Polat, Gorkem and Temizel, Alptekin and Krenzer, Adrian and Hekalo, Amar and Guo, Yun Bo and Matuszewski, Bogdan and Gridach, Mourad and Voiculescu, Irina and Yoganand, Vishnusai and Chavan, Arnav and Raj, Aryan and Nguyen, Nhan T. and Tran, Dat Q. and Huynh, Le Duy and Boutry, Nicolas and Rezvy, Shahadate and Chen, Haijian and Choi, Yoon Ho and Subramanian, Anand and Balasubramanian, Velmurugan and Gao, Xiaohong W. and Hu, Hongyu and Liao, Yusheng and Stoyanov, Danail and Daul, Christian and Realdon, Stefano and Cannizzaro, Renato and Lamarque, Dominique and Tran-Nguyen, Terry and Bailey, Adam and Braden, Barbara and East, James E. and Rittscher, Jens},
  journal       = {Medical Image Analysis},
  title         = {{Deep learning for detection and segmentation of artefact and disease instances in gastrointestinal endoscopy}},
  year          = {2021},
  issn          = {13618423},
  volume        = {70},
  abstract      = {The Endoscopy Computer Vision Challenge (EndoCV) is a crowd-sourcing initiative to address eminent problems in developing reliable computer aided detection and diagnosis endoscopy systems and suggest a pathway for clinical translation of technologies. Whilst endoscopy is a widely used diagnostic and treatment tool for hollow-organs, there are several core challenges often faced by endoscopists, mainly: 1) presence of multi-class artefacts that hinder their visual interpretation, and 2) difficulty in identifying subtle precancerous precursors and cancer abnormalities. Artefacts often affect the robustness of deep learning methods applied to the gastrointestinal tract organs as they can be confused with tissue of interest. EndoCV2020 challenges are designed to address research questions in these remits. In this paper, we present a summary of methods developed by the top 17 teams and provide an objective comparison of state-of-the-art methods and methods designed by the participants for two sub-challenges: i) artefact detection and segmentation (EAD2020), and ii) disease detection and segmentation (EDD2020). Multi-center, multi-organ, multi-class, and multi-modal clinical endoscopy datasets were compiled for both EAD2020 and EDD2020 sub-challenges. The out-of-sample generalization ability of detection algorithms was also evaluated. Whilst most teams focused on accuracy improvements, only a few methods hold credibility for clinical usability. The best performing teams provided solutions to tackle class imbalance, and variabilities in size, origin, modality and occurrences by exploring data augmentation, data fusion, and optimal class thresholding techniques.},
  archiveprefix = {arXiv},
  arxivid       = {2010.06034},
  doi           = {10.1016/j.media.2021.102002},
  eprint        = {2010.06034},
  keywords      = {Artefact,Challenge,Deep learning,Detection,Disease,Endoscopy,Gastroenterology,Segmentation},
  pmid          = {33657508},
  url           = {http://arxiv.org/abs/2010.06034},
}

@Article{art/BornJ_202004,
  author        = {Born, Jannis and Br{\"{a}}ndle, Gabriel and Cossio, Manuel and Disdier, Marion and Goulet, Julie and Roulin, J{\'{e}}r{\'{e}}mie and Wiedemann, Nina},
  journal       = {arXiv},
  title         = {{POCOVID-net: Automatic detection of COVID-19 from a new lung ultrasound imaging dataset (POCUS)}},
  year          = {2020},
  issn          = {23318422},
  month         = {apr},
  abstract      = {With the rapid development of COVID-19 into a global pandemic, there is an ever more urgent need for cheap, fast and reliable tools that can assist physicians in diagnosing COVID-19. Medical imaging such as CT can take a key role in complementing conventional diagnostic tools from molecular biology, and, using deep learning techniques, several automatic systems were demonstrated promising performances using CT or X-ray data. Here, we advocate a more prominent role of point-of-care ultrasound imaging to guide COVID-19 detection. Ultrasound is non-invasive and ubiquitous in medical facilities around the globe. Our contribution is threefold. First, we gather a lung ultrasound (POCUS) dataset consisting of (currently) 1103 images (654 COVID-19, 277 bacterial pneumonia and 172 healthy controls), sampled from 64 videos. While this dataset was assembled from various online sources and is by no means exhaustive, it was processed specifically to feed deep learning models and is intended to serve as a starting point for an open-access initiative. Second, we train a deep convolutional neural network (POCOVID-Net) on this 3-class dataset and achieve an accuracy of 89% and, by a majority vote, a video accuracy of 92% . For detecting COVID-19 in particular, the model performs with a sensitivity of 0.96, a specificity of 0.79 and F1-score of 0.92 in a 5-fold cross validation. Third, we provide an open-access web service (POCOVIDScreen) that is available at: https://pocovidscreen.org. The website deploys the predictive model, allowing to perform predictions on ultrasound lung images. In addition, it grants medical staff the option to (bulk) upload their own screenings in order to contribute to the growing public database of pathological lung ultrasound images.},
  archiveprefix = {arXiv},
  arxivid       = {2004.12084},
  eprint        = {2004.12084},
  url           = {http://arxiv.org/abs/2004.12084},
}

@Article{art/BornJ_202009,
  author        = {Born, Jannis and Wiedemann, Nina and Br{\"{a}}ndle, Gabriel and Buhre, Charlotte and Rieck, Bastian and Borgwardt, Karsten},
  journal       = {arXiv},
  title         = {{Accelerating COVID-19 differential diagnosis with explainable ultrasound image analysis}},
  year          = {2020},
  issn          = {23318422},
  month         = {sep},
  abstract      = {Controlling the COVID-19 pandemic largely hinges upon the existence of fast, safe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or X-Ray, has many practical advantages and can serve as a globally-applicable first-line examination technique. We provide the largest publicly available lung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three classes (COVID-19, bacterial pneumonia, and healthy controls); curated and approved by medical experts. On this dataset, we perform an in-depth study of the value of deep learning methods for differential diagnosis of COVID-19. We propose a frame-based convolutional neural network that correctly classifies COVID-19 US videos with a sensitivity of 0.98 ± 0.04 and a specificity of 0.91 ± 0.08 (frame-based sensitivity 0.93±0.05, specificity 0.87±0.07). We further employ class activation maps for the spatio-temporal localization of pulmonary biomarkers, which we subsequently validate for human-in-the-loop scenarios in a blindfolded study with medical experts. Aiming for scalability and robustness, we perform ablation studies comparing mobile-friendly, frame- and video-based architectures and show reliability of the best model by aleatoric and epistemic uncertainty estimates. We hope to pave the road for a community effort toward an accessible, efficient and interpretable screening method and we have started to work on a clinical validation of the proposed method. Data and code are publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {2009.06116},
  doi           = {10.3390/app11020672},
  eprint        = {2009.06116},
  url           = {http://arxiv.org/abs/2009.06116},
}

@Article{art/ChowdhuryM_2020,
  author        = {Chowdhury, Muhammad E.H. and Rahman, Tawsifur and Khandakar, Amith and Mazhar, Rashid and Kadir, Muhammad Abdul and Mahbub, Zaid Bin and Islam, Khandakar Reajul and Khan, Muhammad Salman and Iqbal, Atif and Emadi, Nasser Al and Reaz, Mamun Bin Ibne and Islam, Mohammad Tariqul},
  journal       = {IEEE Access},
  title         = {{Can AI Help in Screening Viral and COVID-19 Pneumonia?}},
  year          = {2020},
  issn          = {21693536},
  pages         = {132665--132676},
  volume        = {8},
  abstract      = {Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.},
  archiveprefix = {arXiv},
  arxivid       = {2003.13145},
  doi           = {10.1109/ACCESS.2020.3010287},
  eprint        = {2003.13145},
  keywords      = {Artificial intelligence,COVID-19 pneumonia,computer-aided diagnostic tool,machine learning,transfer learning,viral pneumonia},
  url           = {https://ieeexplore.ieee.org/document/9144185/},
}

@Misc{art/BudaM_2020,
  author    = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and Swiecicki, Albert and Lo, Joseph Y and Yang, Jichen and Mazurowski, Maciej},
  title     = {{Breast Cancer Screening – Digital Breast Tomosynthesis (BCS-DBT)}},
  year      = {2020},
  doi       = {10.7937/E4WT-CD02},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/DAbbAw},
}

@Article{art/WangL_202012,
  author        = {Wang, Linda and Lin, Zhong Qiu and Wong, Alexander},
  journal       = {Scientific Reports},
  title         = {{COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images}},
  year          = {2020},
  issn          = {20452322},
  month         = {dec},
  number        = {1},
  pages         = {19549},
  volume        = {10},
  abstract      = {The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.},
  archiveprefix = {arXiv},
  arxivid       = {2009.05383},
  doi           = {10.1038/s41598-020-76550-z},
  eprint        = {2009.05383},
  pmid          = {33177550},
  url           = {http://www.nature.com/articles/s41598-020-76550-z},
}

@Article{art/BudaM_2020a,
  author        = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and {\'{S}}wi{\c{e}}cicki, Albert and Lo, Joseph Y. and Mazurowski, Maciej A.},
  title         = {{Detection of masses and architectural distortions in digital breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep learning model}},
  year          = {2020},
  abstract      = {Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65% at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development.},
  archiveprefix = {arXiv},
  arxivid       = {2011.07995},
  eprint        = {2011.07995},
  url           = {http://arxiv.org/abs/2011.07995},
}

@Misc{art/GireeshaH_2015,
  author    = {Gireesha, HM and S, Nanda},
  title     = {{Thyroid Nodule Segmentation And Classification In Ultrasound Images}},
  year      = {2015},
  abstract  = {Abstract—This paper proposes, a novel computer based approach for benign or malignant assessment of thyroid nodules in ultrasound images. Ultrasound imaging is one of the frequently used diagnosis tool to detect and classify abnormalities of the thyroid gland. The proposed approach comprises of four important stages: pre-processing, segmentation, feature extraction and classification. Rayleigh trimmed anisotropic diffusion filter is used for pre-processing. A watershed algorithm is used to segment the nodule region. An artificial neural network (ANN) and support vector machine (SVM) classifiers are employed for the classification task, utilizing feature vectors derived from gray level co-occurrence (GLCM) features. The classification results are evaluated with the use of accuracy, sensitivity and specificity. It is derived that SVM classifier provides better result than ANN for discriminating benign and malignant nodules, obtaining accuracy 92.5%, sensitivity 96.66% and specificity 80%.},
  booktitle = {International Journal of Engineering Research and Technology},
  doi       = {10.5281/ZENODO.3715942},
  keywords  = {"ijert",anisotropic diffusion,cad,computer aided diagnosis,thyroid ultrasound,watershed segmentation},
  number    = {5},
  pages     = {2252--2256},
  publisher = {Zenodo},
  url       = {file:///C:/Users/Anshi/Downloads/IJERTV3IS052114.pdf},
  volume    = {3},
}

@Article{art/JinL_202012,
  author   = {Jin, Liang and Yang, Jiancheng and Kuang, Kaiming and Ni, Bingbing and Gao, Yiyi and Sun, Yingli and Gao, Pan and Ma, Weiling and Tan, Mingyu and Kang, Hui and Chen, Jiajun and Li, Ming},
  journal  = {EBioMedicine},
  title    = {{Deep-learning-assisted detection and segmentation of rib fractures from CT scans: Development and validation of FracNet}},
  year     = {2020},
  issn     = {23523964},
  month    = {dec},
  pages    = {103106},
  volume   = {62},
  abstract = {Background: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9% with 5.27 false positives per scan and a segmentation Dice of 71.5%on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication.},
  doi      = {10.1016/j.ebiom.2020.103106},
  keywords = {Deep learning,Detection and segmentation,Rib fracture},
  pmid     = {33186809},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2352396420304825},
}

@Misc{art/DalcaA_2020,
  author    = {Dalca, Adrian and {Yipeng Hu} and Vercauteren, Tom and Heinrich, Mattias and Hansen, Lasse and Modat, Marc and Vos, Bob De and {Yiming Xiao} and Rivaz, Hassan and Chabanas, Matthieu and Reinertsen, Ingerid and Landman, Bennett and Cardoso, Jorge and Ginneken, Bram Van and {Alessa Hering} and Murphy, Keelin},
  title     = {{Learn2Reg - The Challenge}},
  year      = {2020},
  doi       = {10.5281/ZENODO.3715651},
  keywords  = {Abdomen,Biomedical Challenges,Brain,Deformable,MICCAI,MICCAI Challenges,Multimodal,Realtime,Registration,Thorax},
  publisher = {Zenodo},
  url       = {https://zenodo.org/record/3715651},
}

@Article{art/LoefflerM_2020,
  author   = {L{\"{o}}ffler, Maximilian T. and Sekuboyina, Anjany and Jacob, Alina and Grau, Anna-Lena and Scharr, Andreas and {El Husseini}, Malek and Kallweit, Mareike and Zimmer, Claus and Baum, Thomas and Kirschke, Jan S.},
  journal  = {Radiology: Artificial Intelligence},
  title    = {{A Vertebral Segmentation Dataset with Fracture Grading}},
  year     = {2020},
  issn     = {2638-6100},
  number   = {4},
  pages    = {e190138},
  volume   = {2},
  abstract = {Published under a CC BY 4.0 license. Supplemental material is available for this article.},
  doi      = {10.1148/ryai.2020190138},
}

@Article{art/VermaR_2020,
  author = {Verma, Ruchika and Kumar, Neeraj and Patil, Abhijeet and Kurian, Nikhil Cherian and Rane, Swapnil and Sethi, Amit},
  title  = {{Multi-organ Nuclei Segmentation and Classification Challenge 2020}},
  year   = {2020},
  number = {February},
  pages  = {1--3},
  doi    = {10.13140/RG.2.2.12290.02244},
}

@Article{art/SekuboyinaA_2020,
  author   = {Sekuboyina, Anjany and Rempfler, Markus and Valentinitsch, Alexander and Menze, Bjoern H. and Kirschke, Jan S.},
  journal  = {Radiology: Artificial Intelligence},
  title    = {{Labeling Vertebrae with Two-dimensional Reformations of Multidetector CT Images: An Adversarial Approach for Incorporating Prior Knowledge of Spine Anatomy}},
  year     = {2020},
  issn     = {2638-6100},
  number   = {2},
  pages    = {e190074},
  volume   = {2},
  abstract = {S pine CT is a commonly performed imaging procedure. In this study, we focused on labeling the vertebrae , which is the task of both locating and identifying the cervical, thoracic, lumbar, and sacral vertebrae in a regular spine CT scan. Labeling the vertebrae has immediate diagnostic consequences. Vertebral landmarks help identify scoliosis, pathologic lordosis, and kyphosis, for example. From a modeling perspective, labeling simplifies the downstream tasks of intervertebral disk segmenta-tion and vertebral segmentation. Previous approaches to labeling fell into one of two broad categories: the traditional model-based approaches and the relatively recent learning-based approaches. Model-based approaches such as those of Schmidt et al (1), Klinder et al (2), and Ma and Lu (3) used a priori information on the spine structure, such as statistical shape models or atlases. Due to their extensive reliance on a priori information, the generalizability of these approaches was limited. From a machine learning perspective, approaches have existed, ranging from regression forest models working on context features in Glocker et al (4), Glocker et al (5), and Suzani et al (6); a combination of convolutional neural networks and random forest models in Chen et al (7); and three-dimensional (3D) fully convolutional networks in Yang et al (8), followed by recurrent neural networks in Yang et al (9) and Liao et al (10). Most of these approaches work on full 3D multidetector CT scans. Recently , an approach achieved a higher labeling performance by working on two-dimensional (2D) maximum intensity projections (MIPs) using an architecture termed Btrfly Net, which was first proposed by Sekuboyina et al (11). In this study, we improved on the Btrfly architecture and extended it with a spine localization module, thus making the combination more generalizable. Concurrently , inspired by the generative adversarial learning domain , we investigated an a priori learning module that enforced the spine's anatomic a priori knowledge (ie, prior) onto the Btrfly network. Earlier approaches were This copy is for personal use only. To order printed copies, contact reprints@rsna.org Purpose: To use and test a labeling algorithm that operates on two-dimensional reformations, rather than three-dimensional data to locate and identify vertebrae. Materials and Methods: The authors improved the Btrfly Net, a fully convolutional network architecture described by Sekuboyina et al, which works on sagittal and coronal maximum intensity projections (MIPs) and augmented it with two additional components: spine localization and adversarial a priori learning. Furthermore, two variants of adversarial training schemes that incorporated the anatomic a priori knowledge into the Btrfly Net were explored. The superiority of the proposed approach for labeling vertebrae on three data-sets was investigated: a public benchmarking dataset of 302 CT scans and two in-house datasets with a total of 238 CT scans. The Wilcoxon signed rank test was employed to compute the statistical significance of the improvement in performance observed with various architectural components in the authors' approach. Results: On the public dataset, the authors' approach using the described Btrfly Net with energy-based prior encoding (Btrfly pe-eb) network performed as well as current state-of-the-art methods, achieving a statistically significant (P , .001) vertebrae identification rate of 88.5% 6 0.2 (standard deviation) and localization distances of less than 7 mm. On the in-house datasets that had a higher interscan data variability, an identification rate of 85.1% 6 1.2 was obtained. Conclusion: An identification performance comparable to existing three-dimensional approaches was achieved when labeling vertebrae on two-dimensional MIPs. The performance was further improved using the proposed adversarial training regimen that effectively enforced local spine a priori knowledge during training. Spine localization increased the generalizability of our approach by homogeniz-ing the content in the MIPs.},
  doi      = {10.1148/ryai.2020190074},
}

@Article{art/WangL_202004,
  author        = {Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Eide, Darrin and Funk, Kathryn and Kinney, Rodney and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex D. and Wang, Kuansan and Wilhelm, Chris and Xie, Boya and Raymond, Douglas and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian},
  journal       = {arXiv},
  title         = {{CORD-19: The COVID-19 open research dataset}},
  year          = {2020},
  issn          = {23318422},
  month         = {apr},
  abstract      = {The COVID-19 Open Research Dataset (CORD-19) is a growing1 resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 75K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and preview tools and upcoming shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.},
  archiveprefix = {arXiv},
  arxivid       = {2004.10706},
  eprint        = {2004.10706},
  pmid          = {32510522},
  url           = {http://arxiv.org/abs/2004.10706},
}

@Article{art/ZhaoJ_202003,
  author        = {Zhao, Jinyu and He, Xuehai and Yang, Xingyi and Zhang, Yichen and Zhang, Shanghang and Xie, Pengtao},
  journal       = {arXiv},
  title         = {{COVID-CT-Dataset: A CT image dataset about COVID-19}},
  year          = {2020},
  issn          = {23318422},
  month         = {mar},
  abstract      = {During the outbreak time of COVID-19, computed tomography (CT) is a useful manner for diagnosing COVID-19 patients. To mitigate the lack of publicly available COVID-19 CT images for developing CT-based diagnosis deep learning models of COVID-19, we build an open-sourced dataset COVID-CT, which contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist and via experimental studies. Using this dataset, we develop a joint classification and segmentation method that achieves an F1 of 0.85, an AUC of 0.95, and an accuracy of 0.83. The data and code are available at https://github.com/UCSD-AI4H/COVID-CT.},
  archiveprefix = {arXiv},
  arxivid       = {2003.13865},
  eprint        = {2003.13865},
  url           = {http://arxiv.org/abs/2003.13865},
}

@Article{art/SekuboyinaA_202001,
  author        = {Sekuboyina, Anjany and Husseini, Malek E. and Bayat, Amirhossein and L{\"{o}}ffler, Maximilian and Liebl, Hans and Li, Hongwei and Tetteh, Giles and Kuka{\v{c}}ka, Jan and Payer, Christian and {\v{S}}tern, Darko and Urschler, Martin and Chen, Maodong and Cheng, Dalong and Lessmann, Nikolas and Hu, Yujin and Wang, Tianfu and Yang, Dong and Xu, Daguang and Ambellan, Felix and Amiranashvili, Tamaz and Ehlke, Moritz and Lamecker, Hans and Lehnert, Sebastian and Lirio, Marilia and de Olaguer, Nicol{\'{a}}s P{\'{e}}rez and Ramm, Heiko and Sahu, Manish and Tack, Alexander and Zachow, Stefan and Jiang, Tao and Ma, Xinjun and Angerman, Christoph and Wang, Xin and Brown, Kevin and Wolf, Matthias and Kirszenberg, Alexandre and Puybareau, {\'{E}}lodie and Chen, Di and Bai, Yiwei and Rapazzo, Brandon H. and Yeah, Timyoas and Zhang, Amber and Xu, Shangliang and Hou, Feng and He, Zhiqiang and Zeng, Chan and Xiangshang, Zheng and Liming, Xu and Netherton, Tucker J. and Mumme, Raymond P. and Court, Laurence E. and Huang, Zixun and He, Chenhang and Wang, Li-Wen and Ling, Sai Ho and Huynh, L{\^{e}} Duy and Boutry, Nicolas and Jakubicek, Roman and Chmelik, Jiri and Mulay, Supriti and Sivaprakasam, Mohanasankar and Paetzold, Johannes C. and Shit, Suprosanna and Ezhov, Ivan and Wiestler, Benedikt and Glocker, Ben and Valentinitsch, Alexander and Rempfler, Markus and Menze, Bj{\"{o}}rn H. and Kirschke, Jan S.},
  journal       = {arXiv},
  title         = {{VerSe: A Vertebrae Labelling and Segmentation Benchmark for Multi-detector CT Images}},
  year          = {2020},
  month         = {jan},
  abstract      = {Vertebral labelling and segmentation are two fundamental tasks in an automated spine processing pipeline. Reliable and accurate processing of spine images is expected to benefit clinical decision-support systems for diagnosis, surgery planning, and population-based analysis on spine and bone health. However, designing automated algorithms for spine processing is challenging predominantly due to considerable variations in anatomy and acquisition protocols and due to a severe shortage of publicly available data. Addressing these limitations, the Large Scale Vertebrae Segmentation Challenge (VerSe) was organised in conjunction with the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) in 2019 and 2020, with a call for algorithms towards labelling and segmentation of vertebrae. Two datasets containing a total of 374 multi-detector CT scans from 355 patients were prepared and 4505 vertebrae have individually been annotated at voxel-level by a human-machine hybrid algorithm (https://osf.io/nqjyw/, https://osf.io/t98fz/). A total of 25 algorithms were benchmarked on these datasets. In this work, we present the the results of this evaluation and further investigate the performance-variation at vertebra-level, scan-level, and at different fields-of-view. We also evaluate the generalisability of the approaches to an implicit domain shift in data by evaluating the top performing algorithms of one challenge iteration on data from the other iteration. The principal takeaway from VerSe: the performance of an algorithm in labelling and segmenting a spine scan hinges on its ability to correctly identify vertebrae in cases of rare anatomical variations. The content and code concerning VerSe can be accessed at: https://github.com/anjany/verse.},
  archiveprefix = {arXiv},
  arxivid       = {2001.09193},
  eprint        = {2001.09193},
  keywords      = {Computed tomography,Spine segmentation,Vertebrae detection and localization,Vertebrae labelling,Vertebrae segmentation},
  url           = {http://arxiv.org/abs/2001.09193},
}

@Article{art/CassidyB_202004,
  author        = {Cassidy, Bill and Reeves, Neil D. and Joseph, Pappachan and Gillespie, David and O'Shea, Claire and Rajbhandari, Satyan and Maiya, Arun G. and Frank, Eibe and Boulton, Andrew and Armstrong, David and Najafi, Bijan and Wu, Justina and Yap, Moi Hoon},
  journal       = {arXiv},
  title         = {{DFUC 2020: Analysis towards diabetic foot ulcer detection}},
  year          = {2020},
  issn          = {23318422},
  month         = {apr},
  abstract      = {Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the detection of diabetic foot ulcers, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms that can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research.},
  archiveprefix = {arXiv},
  arxivid       = {2004.11853},
  eprint        = {2004.11853},
  url           = {http://arxiv.org/abs/2004.11853},
}

@Article{art/HeX_202003,
  author        = {He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
  journal       = {arXiv},
  title         = {{PATHVQA: 30000+ questions for medical visual question answering}},
  year          = {2020},
  issn          = {23318422},
  month         = {mar},
  abstract      = {Is it possible to develop an “AI Pathologist" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.},
  archiveprefix = {arXiv},
  arxivid       = {2003.10286},
  eprint        = {2003.10286},
  keywords      = {Dataset,Healthcare,Pathology,Visual question answering},
  url           = {http://arxiv.org/abs/2003.10286},
}

@Misc{art/MunozGilG_2020,
  author        = {Mu{\~{n}}oz-Gil, Gorka and Volpe, Giovanni and Garcia-March, Miguel Angel and Metzler, Ralf and Lewenstein, Maciej and Manzo, Carlo},
  title         = {{AnDi: The anomalous diffusion challenge}},
  year          = {2020},
  abstract      = {The deviation from pure Brownian motion, generally referred to as anomalous diffusion, has received large attention in the scientific literature to describe many physical scenarios. Several methods, based on classical statistics and machine learning approaches, have been developed to characterize anomalous diffusion from experimental data, which are usually acquired as particle trajectories. With the aim to assess and compare the available methods to characterize anomalous diffusion, we have organized the Anomalous Diffusion (AnDi) Challenge (http://www.andi-challenge.org/). Specifically, the AnDi Challenge will address three different aspects of anomalous diffusion characterization, namely: (i) Inference of the anomalous diffusion exponent. (ii) Identification of the underlying diffusion model. (iii) Segmentation of trajectories. Each problem includes sub-tasks for different number of dimensions (1D, 2D and 3D). In order to compare the various methods, we have developed a dedicated open-source framework for the simulation of the anomalous diffusion trajectories that are used for the training and test datasets. The challenge was launched on March 1, 2020, and consists of three phases. Currently, the participation to the first phase is open. Submissions will be automatically evaluated and the performance of the top-scoring methods will be thoroughly analyzed and compared in an upcoming article.},
  archiveprefix = {arXiv},
  arxivid       = {2003.12036},
  booktitle     = {arXiv},
  doi           = {10.1117/12.2567914},
  eprint        = {2003.12036},
  issn          = {23318422},
  keywords      = {ANDI challenge,anomalous diffusion,physics simulations},
  publisher     = {Zenodo},
  url           = {https://zenodo.org/record/3707702},
}

@Article{art/BawaV_202006,
  author        = {Bawa, Vivek Singh and Singh, Gurkirt and Kaping'a, Francis and Skarga-Bandurova, Inna and Leporini, Alice and Landolfo, Carmela and Stabile, Armando and Setti, Francesco and Muradore, Riccardo and Oleari, Elettra and Cuzzolin, Fabio},
  journal       = {arXiv},
  title         = {{ESAD: Endoscopic surgeon action detection dataset}},
  year          = {2020},
  month         = {jun},
  abstract      = {In this work, we take aim towards increasing the effectiveness of surgical assistant robots. We intended to make assistant robots safer by making them aware about the actions of surgeon, so it can take appropriate assisting actions. In other words, we aim to solve the problem of surgeon action detection in endoscopic videos. To this, we introduce a challenging dataset for surgeon action detection in real world endoscopic videos. Action classes are picked based on the feedback of surgeons and annotated by medical professional. Given a video frame, we draw bounding box around surgical tool which is performing action and label it with action label. Finally, we present a frame-level action detection baseline model based on recent advances in object detection. Results on our new dataset show that our presented dataset provides enough interesting challenges for future method and it can serve as strong benchmark corresponding research in surgeon action detection in endoscopic videos.},
  archiveprefix = {arXiv},
  arxivid       = {2006.07164},
  eprint        = {2006.07164},
  keywords      = {Action detection,Endoscopic video,Prostatectomy,Surgeon action detection,Surgical robotics},
  url           = {http://arxiv.org/abs/2006.07164},
}

@Article{art/Campello_2020,
  author  = {Campello and V{\'{i}}ctor, M.},
  journal = {IEEE Transactions on Medical Imaging},
  title   = {{Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation: The M&Ms Challenge}},
  year    = {2020},
}

@Article{art/HanG_2015,
  author   = {Han, Guanghui and Liu, Xiabi and Han, Feifei and Santika, I. Nyoman Tenaya and Zhao, Yanfeng and Zhao, Xinming and Zhou, Chunwu},
  journal  = {IEEE Transactions on Biomedical Engineering},
  title    = {{The LISS - A public database of common imaging signs of lung diseases for computer-aided detection and diagnosis research and medical education}},
  year     = {2015},
  issn     = {15582531},
  number   = {2},
  pages    = {648--656},
  volume   = {62},
  abstract = {Lung computed tomography (CT) imaging signs play important roles in the diagnosis of lung diseases. In this paper, we review the significance of CT imaging signs in disease diagnosis and determine the inclusion criterion of CT scans and CT imaging signs of our database. We develop the software of abnormal regions annotation and design the storage scheme of CT images and annotation data. Then, we present a publicly available database of lung CT imaging signs, called LISS for short, which contains 271 CT scans and 677 abnormal regions in them. The 677 abnormal regions are divided into nine categories of common CT imaging signs of lung disease (CISLs). The ground truth of these CISLs regions and the corresponding categories are provided. Furthermore, to make the database publicly available, all private data in CT scans are eliminated or replaced with provisioned values. The main characteristic of our LISS database is that it is developed from a new perspective of CT imaging signs of lung diseases instead of commonly considered lung nodules. Thus, it is promising to apply to computer-aided detection and diagnosis research and medical education.},
  doi      = {10.1109/TBME.2014.2363131},
  keywords = {CT imaging signs,Computer-aided diagnosis (CAD),lung lesions,medical database,medical education},
  pmid     = {25330480},
}

@Article{art/CastilloE_201001,
  author   = {Castillo, Edward and Castillo, Richard and Martinez, Josue and Shenoy, Maithili and Guerrero, Thomas},
  journal  = {Physics in Medicine and Biology},
  title    = {{Four-dimensional deformable image registration using trajectory modeling}},
  year     = {2010},
  issn     = {00319155},
  month    = {jan},
  number   = {1},
  pages    = {305--327},
  volume   = {55},
  abstract = {A four-dimensional deformable image registration (4D DIR) algorithm, referred to as 4D local trajectory modeling (4DLTM), is presented and applied to thoracic 4D computed tomography (4DCT) image sets. The theoretical framework on which this algorithm is built exploits the incremental continuity present in 4DCT component images to calculate a dense set of parameterized voxel trajectories through space as functions of time. The spatial accuracy of the 4DLTM algorithm is compared with an alternative registration approach in which component phase to phase (CPP) DIR is utilized to determine the full displacement between maximum inhale and exhale images. A publically available DIR reference database (http://www.dir-lab.com) is utilized for the spatial accuracy assessment. The database consists of ten 4DCT image sets and corresponding manually identified landmark points between the maximum phases. A subset of points are propagated through the expiratory 4DCT component images. Cubic polynomials were found to provide sufficient flexibility and spatial accuracy for describing the point trajectories through the expiratory phases. The resulting average spatial error between the maximum phases was 1.25 mm for the 4DLTM and 1.44 mm for the CPP. The 4DLTM method captures the long-range motion between 4DCT extremes with high spatial accuracy. {\textcopyright} 2010 Institute of Physics and Engineering in Medicine.},
  doi      = {10.1088/0031-9155/55/1/018},
  pmid     = {20009196},
  url      = {https://iopscience.iop.org/article/10.1088/0031-9155/55/1/018},
}

@InCollection{art/RoyA_2018,
  author        = {Roy, Abhijit Guha and Navab, Nassir and Wachinger, Christian},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Concurrent spatial and channel ‘squeeze & excitation' in fully convolutional networks}},
  year          = {2018},
  isbn          = {9783030009274},
  pages         = {421--429},
  volume        = {11070 LNCS},
  abstract      = {Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze & excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze & excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans and organ segmentation on whole body contrast enhanced CT scans.},
  archiveprefix = {arXiv},
  arxivid       = {1803.02579},
  doi           = {10.1007/978-3-030-00928-1_48},
  eprint        = {1803.02579},
  issn          = {16113349},
  url           = {http://link.springer.com/10.1007/978-3-030-00928-1_48},
}

@Article{art/BurlutskiyN_201807,
  author        = {Burlutskiy, Nikolay and Gu, Feng and Wilen, Lena Kajland and Backman, Max and Micke, Patrick},
  journal       = {arXiv},
  title         = {{A deep learning framework for automatic diagnosis in lung cancer}},
  year          = {2018},
  issn          = {23318422},
  month         = {jul},
  volume        = {abs/1807.1},
  abstract      = {We developed a deep learning framework that helps to automatically identify and segment lung cancer areas in patients' tissue specimens. The study was based on a cohort of lung cancer patients operated at the Uppsala University Hospital. The tissues were reviewed by lung pathologists and then the cores were compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin stained slides from 712 patients were scanned and then manually annotated. Then these scans and annotations were used to train segmentation models of the developed framework. The performance of the developed deep learning framework was evaluated on fully annotated TMA cores from 178 patients reaching pixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available Stanford TMA cores were used to demonstrate high performance of the framework qualitatively.},
  archiveprefix = {arXiv},
  arxivid       = {1807.10466},
  eprint        = {1807.10466},
  url           = {http://arxiv.org/abs/1807.10466},
}

@InProceedings{art/LafargeM_201803,
  author    = {Lafarge, Maxime W. and Moeskops, Pim and Veta, Mitko and Pluim, Josien P. W. and Eppenhof, Koen A.. J.},
  booktitle = {Medical Imaging 2018: Image Processing},
  title     = {{Deformable image registration using convolutional neural networks}},
  year      = {2018},
  editor    = {Angelini, Elsa D. and Landman, Bennett A.},
  month     = {mar},
  pages     = {27},
  publisher = {SPIE},
  abstract  = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Deformable image registration can be time-consuming and often needs extensive parameterization to perform well on a specific application. We present a step towards a registration framework based on a three-dimensional convolutional neural network. The network directly learns transformations between pairs of three-dimensional images. The outputs of the network are three maps for the x, y, and z components of a thin plate spline transformation grid. The network is trained on synthetic random transformations, which are applied to a small set of representative images for the desired application. Training therefore does not require manually annotated ground truth deformation information. The methodology is demonstrated on public data sets of inspiration-expiration lung CT image pairs, which come with annotated corresponding landmarks for evaluation of the registration accuracy. Advantages of this methodology are its fast registration times and its minimal parameterization.},
  doi       = {10.1117/12.2292443},
  isbn      = {9781510616370},
  issn      = {16057422},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10574/2292443/Deformable-image-registration-using-convolutional-neural-networks/10.1117/12.2292443.full},
}

@Article{art/NibaliA_201710,
  author   = {Nibali, Aiden and He, Zhen and Wollersheim, Dennis},
  journal  = {International Journal of Computer Assisted Radiology and Surgery},
  title    = {{Pulmonary nodule classification with deep residual networks}},
  year     = {2017},
  issn     = {18616429},
  month    = {oct},
  number   = {10},
  pages    = {1799--1808},
  volume   = {12},
  abstract = {Purpose : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules. Methods: We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification. Results: Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy. Conclusions: The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains.},
  doi      = {10.1007/s11548-017-1605-6},
  keywords = {CT images,Convolutional neural network,Lung nodule},
  pmid     = {28501942},
  url      = {http://link.springer.com/10.1007/s11548-017-1605-6},
}

@InProceedings{art/ZhuW_2018,
  author        = {Zhu, Wentao and Vang, Yeeleng S. and Huang, Yufang and Xie, Xiaohui},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{DeepEM: Deep 3D ConvNets with EM for weakly supervised pulmonary nodule detection}},
  year          = {2018},
  pages         = {812--820},
  volume        = {11071 LNCS},
  abstract      = {Recently deep learning has been witnessing widespread adoption in various medical image applications. However, training complex deep neural nets requires large-scale datasets labeled with ground truth, which are often unavailable in many medical image domains. For instance, to train a deep neural net to detect pulmonary nodules in lung computed tomography (CT) images, current practice is to manually label nodule locations and sizes in many CT images to construct a sufficiently large training dataset, which is costly and difficult to scale. On the other hand, electronic medical records (EMR) contain plenty of partial information on the content of each medical image. In this work, we explore how to tap this vast, but currently unexplored data source to improve pulmonary nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework augmented with expectation-maximization (EM), to mine weakly supervised labels in EMRs for pulmonary nodule detection. Experimental results show that DeepEM can lead to 1.5% and 3.9% average improvement in free-response receiver operating characteristic (FROC) scores on LUNA16 and Tianchi datasets, respectively, demonstrating the utility of incomplete information in EMRs for improving deep learning algorithms (https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git ).},
  archiveprefix = {arXiv},
  arxivid       = {1805.05373},
  doi           = {10.1007/978-3-030-00934-2_90},
  eprint        = {1805.05373},
  isbn          = {9783030009335},
  issn          = {16113349},
  keywords      = {Deep 3D convolutional nets,DeepEM (deep 3D ConvNets with EM),Pulmonary nodule detection,Weakly supervised detection},
}

@InProceedings{art/DeyR_201804,
  author        = {Dey, Raunak and Lu, Zhongjie and Hong, Yi},
  booktitle     = {Proceedings - International Symposium on Biomedical Imaging},
  title         = {{Diagnostic classification of lung nodules using 3D neural networks}},
  year          = {2018},
  month         = {apr},
  pages         = {774--778},
  publisher     = {IEEE},
  volume        = {2018-April},
  abstract      = {Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making financial and care plans. In this paper, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification.},
  archiveprefix = {arXiv},
  arxivid       = {1803.07192},
  doi           = {10.1109/ISBI.2018.8363687},
  eprint        = {1803.07192},
  isbn          = {9781538636367},
  issn          = {19458452},
  keywords      = {Deep neural networks,LIDC-IDRI,Lung nodule classification,Multi-output networks},
  url           = {http://arxiv.org/abs/1803.07192 http://dx.doi.org/10.1109/ISBI.2018.8363687 https://ieeexplore.ieee.org/document/8363687/},
}

@Article{art/GaoM_201801,
  author   = {Gao, Mingchen and Bagci, Ulas and Lu, Le and Wu, Aaron and Buty, Mario and Shin, Hoo Chang and Roth, Holger and Papadakis, Georgios Z. and Depeursinge, Adrien and Summers, Ronald M. and Xu, Ziyue and Mollura, Daniel J.},
  journal  = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization},
  title    = {{Holistic classification of CT attenuation patterns for interstitial lung diseases via deep convolutional neural networks}},
  year     = {2018},
  issn     = {21681171},
  month    = {jan},
  number   = {1},
  pages    = {1--6},
  volume   = {6},
  abstract = {Interstitial lung diseases (ILD) involve several abnormal imaging patterns observed in computed tomography (CT) images. Accurate classification of these patterns plays a significant role in precise clinical decision making of the extent and nature of the diseases. Therefore, it is important for developing automated pulmonary computer-aided detection systems. Conventionally, this task relies on experts' manual identification of regions of interest (ROIs) as a prerequisite to diagnose potential diseases. This protocol is time consuming and inhibits fully automatic assessment. In this paper, we present a new method to classify ILD imaging patterns on CT images. The main difference is that the proposed algorithm uses the entire image as a holistic input. By circumventing the prerequisite of manual input ROIs, our problem set-up is significantly more difficult than previous work but can better address the clinical workflow. Qualitative and quantitative results using a publicly available ILD database demonstrate state-of-the-art classification accuracy under the patch-based classification and shows the potential of predicting the ILD type using holistic image.},
  doi      = {10.1080/21681163.2015.1124249},
  keywords = {Interstitial lung disease,convolutional neural network,holistic medical image classification},
  url      = {https://www.tandfonline.com/doi/full/10.1080/21681163.2015.1124249},
}

@Article{art/WangX_202009,
  author   = {Wang, Xi and Chen, Hao and Gan, Caixia and Lin, Huangjing and Dou, Qi and Tsougenis, Efstratios and Huang, Qitao and Cai, Muyan and Heng, Pheng Ann},
  journal  = {IEEE Transactions on Cybernetics},
  title    = {{Weakly Supervised Deep Learning for Whole Slide Lung Cancer Image Analysis}},
  year     = {2020},
  issn     = {21682275},
  month    = {sep},
  number   = {9},
  pages    = {3950--3962},
  volume   = {50},
  abstract = {Histopathology image analysis serves as the gold standard for cancer diagnosis. Efficient and precise diagnosis is quite critical for the subsequent therapeutic treatment of patients. So far, computer-aided diagnosis has not been widely applied in pathological field yet as currently well-addressed tasks are only the tip of the iceberg. Whole slide image (WSI) classification is a quite challenging problem. First, the scarcity of annotations heavily impedes the pace of developing effective approaches. Pixelwise delineated annotations on WSIs are time consuming and tedious, which poses difficulties in building a large-scale training dataset. In addition, a variety of heterogeneous patterns of tumor existing in high magnification field are actually the major obstacle. Furthermore, a gigapixel scale WSI cannot be directly analyzed due to the immeasurable computational cost. How to design the weakly supervised learning methods to maximize the use of available WSI-level labels that can be readily obtained in clinical practice is quite appealing. To overcome these challenges, we present a weakly supervised approach in this article for fast and effective classification on the whole slide lung cancer images. Our method first takes advantage of a patch-based fully convolutional network (FCN) to retrieve discriminative blocks and provides representative deep features with high efficiency. Then, different context-aware block selection and feature aggregation strategies are explored to generate globally holistic WSI descriptor which is ultimately fed into a random forest (RF) classifier for the image-level prediction. To the best of our knowledge, this is the first study to exploit the potential of image-level labels along with some coarse annotations for weakly supervised learning. A large-scale lung cancer WSI dataset is constructed in this article for evaluation, which validates the effectiveness and feasibility of the proposed method. Extensive experiments demonstrate the superior performance of our method that surpasses the state-of-the-art approaches by a significant margin with an accuracy of 97.3%. In addition, our method also achieves the best performance on the public lung cancer WSIs dataset from The Cancer Genome Atlas (TCGA). We highlight that a small number of coarse annotations can contribute to further accuracy improvement. We believe that weakly supervised learning methods have great potential to assist pathologists in histology image diagnosis in the near future.},
  doi      = {10.1109/TCYB.2019.2935141},
  keywords = {Deep learning,histology image analysis,weakly supervised learning,whole slide images (WSIs)},
  pmid     = {31484154},
  url      = {https://ieeexplore.ieee.org/document/8822590/},
}

@Article{art/DepeursingeA_2012,
  author   = {Depeursinge, Adrien and Vargas, Alejandro and Platon, Alexandra and Geissbuhler, Antoine and Poletti, Pierre Alexandre and M{\"{u}}ller, Henning},
  journal  = {Computerized Medical Imaging and Graphics},
  title    = {{Building a reference multimedia database for interstitial lung diseases}},
  year     = {2012},
  issn     = {08956111},
  number   = {3},
  pages    = {227--238},
  volume   = {36},
  abstract = {This paper describes the methodology used to create a multimedia collection of cases with interstitial lung diseases (ILDs) at the University Hospitals of Geneva. The dataset contains high-resolution computed tomography (HRCT) image series with three-dimensional annotated regions of pathological lung tissue along with clinical parameters from patients with pathologically proven diagnoses of ILDs. The motivations for this work is to palliate the lack of publicly available collections of ILD cases to serve as a basis for the development and evaluation of image-based computerized diagnostic aid. After 38 months of data collection, the library contains 128 patients affected with one of the 13 histological diagnoses of ILDs, 108 image series with more than 41. l of annotated lung tissue patterns as well as a comprehensive set of 99 clinical parameters related to ILDs. The database is available for research on request and after signature of a license agreement. {\textcopyright} 2011 Elsevier Ltd.},
  doi      = {10.1016/j.compmedimag.2011.07.003},
  keywords = {Case-based retrieval,Computer-aided diagnosis,Content-based image retrieval,High-resolution computed tomography,Interstitial lung diseases,Multimedia database},
  pmid     = {21803548},
}

@InCollection{art/ShenW_2016,
  author    = {Shen, Wei and Zhou, Mu and Yang, Feng and Dong, Di and Yang, Caiyun and Zang, Yali and Tian, Jie},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer International Publishing},
  title     = {{Learning from experts: Developing transferable deep features for patient-level lung cancer prediction}},
  year      = {2016},
  address   = {Cham},
  editor    = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
  isbn      = {9783319467221},
  pages     = {124--131},
  volume    = {9901 LNCS},
  abstract  = {Due to recent progress in Convolutional Neural Networks (CNNs),developing image-based CNN models for predictive diagnosis is gaining enormous interest. However,to date,insufficient imaging samples with truly pathological-proven labels impede the evaluation of CNN models at scale. In this paper,we formulate a domain-adaptation framework that learns transferable deep features for patient-level lung cancer malignancy prediction. The presented work learns CNN-based features from a large discovery set (2272 lung nodules) with malignancy likelihood labels involving multiple radiologists' assessments,and then tests the transferable predictability of these CNN-based features on a diagnosis-definite set (115 cases) with true pathologically-proven lung cancer labels. We evaluate our approach on the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset,where both human expert labeling information on cancer malignancy likelihood and a set of pathologically-proven malignancy labels were provided. Experimental results demonstrate the superior predictive performance of the transferable deep features on predicting true patient-level lung cancer malignancy (Acc=70.69 %,AUC=0.66),which outperforms a nodule level CNN model (Acc=65.38 %,AUC=0.63) and is even comparable to that of using the radiologists' knowledge (Acc=72.41 %,AUC=0.76). The proposed model can largely reduce the demand for pathologically proven data,holding promise to empower cancer diagnosis by leveraging multi-source CT imaging datasets.},
  doi       = {10.1007/978-3-319-46723-8_15},
  issn      = {16113349},
  url       = {http://link.springer.com/10.1007/978-3-319-46723-8_15},
}

@InCollection{art/DuanJ_201807,
  author        = {Duan, Jinming and Schlemper, Jo and Bai, Wenjia and Dawes, Timothy J.W. and Bello, Ghalib and Doumou, Georgia and {De Marvao}, Antonio and O'Regan, Declan P. and Rueckert, Daniel},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Deep Nested Level Sets: Fully Automated Segmentation of Cardiac MR Images in Patients with Pulmonary Hypertension}},
  year          = {2018},
  isbn          = {9783030009366},
  month         = {jul},
  pages         = {595--603},
  volume        = {11073 LNCS},
  abstract      = {In this paper we introduce a novel and accurate optimisation method for segmentation of cardiac MR (CMR) images in patients with pulmonary hypertension (PH). The proposed method explicitly takes into account the image features learned from a deep neural network. To this end, we estimate simultaneous probability maps over region and edge locations in CMR images using a fully convolutional network. Due to the distinct morphology of the heart in patients with PH, these probability maps can then be incorporated in a single nested level set optimisation framework to achieve multi-region segmentation with high efficiency. The proposed method uses an automatic way for level set initialisation and thus the whole optimisation is fully automated. We demonstrate that the proposed deep nested level set (DNLS) method outperforms existing state-of-the-art methods for CMR segmentation in PH patients.},
  archiveprefix = {arXiv},
  arxivid       = {1807.10760},
  doi           = {10.1007/978-3-030-00937-3_68},
  eprint        = {1807.10760},
  issn          = {16113349},
  url           = {http://arxiv.org/abs/1807.10760 http://link.springer.com/10.1007/978-3-030-00937-3_68},
}

@Article{art/AltafF_201902,
  author        = {Altaf, Fouzia and Islam, Syed M.S. and Akhtar, Naveed and Janjua, Naeem Khalid},
  journal       = {IEEE Access},
  title         = {{Going deep in medical image analysis: Concepts, methods, challenges, and future directions}},
  year          = {2019},
  issn          = {21693536},
  month         = {feb},
  pages         = {99540--99572},
  volume        = {7},
  abstract      = {Medical image analysis is currently experiencing a paradigm shift due to deep learning. This technology has recently attracted so much interest of the Medical Imaging Community that it led to a specialized conference in ''Medical Imaging with Deep Learning'' in the year 2018. This paper surveys the recent developments in this direction and provides a critical review of the related major aspects. We organize the reviewed literature according to the underlying pattern recognition tasks and further sub-categorize it following a taxonomy based on human anatomy. This paper does not assume prior knowledge of deep learning and makes a significant contribution in explaining the core deep learning concepts to the non-experts in the Medical Community. This paper provides a unique computer vision/machine learning perspective taken on the advances of deep learning in medical imaging. This enables us to single out ''lack of appropriately annotated large-scale data sets" as the core challenge (among other challenges) in this research direction. We draw on the insights from the sister research fields of computer vision, pattern recognition, and machine learning, where the techniques of dealing with such challenges have already matured, to provide promising directions for the Medical Imaging Community to fully harness deep learning in the future.},
  archiveprefix = {arXiv},
  arxivid       = {1902.05655},
  doi           = {10.1109/ACCESS.2019.2929365},
  eprint        = {1902.05655},
  keywords      = {Artificial neural networks,Data sets,Deep learning,Medical imaging,Survey,Tutorial},
  url           = {http://arxiv.org/abs/1902.05655 https://ieeexplore.ieee.org/document/8764525/},
}

@Article{art/MasoodA_201803,
  author   = {Masood, Anum and Sheng, Bin and Li, Ping and Hou, Xuhong and Wei, Xiaoer and Qin, Jing and Feng, Dagan},
  journal  = {Journal of Biomedical Informatics},
  title    = {{Computer-Assisted Decision Support System in Pulmonary Cancer detection and stage classification on CT images}},
  year     = {2018},
  issn     = {15320464},
  month    = {mar},
  pages    = {117--128},
  volume   = {79},
  abstract = {Pulmonary cancer is considered as one of the major causes of death worldwide. For the detection of lung cancer, computer-assisted diagnosis (CADx) systems have been designed. Internet-of-Things (IoT) has enabled ubiquitous internet access to biomedical datasets and techniques; in result, the progress in CADx is significant. Unlike the conventional CADx, deep learning techniques have the basic advantage of an automatic exploitation feature as they have the ability to learn mid and high level image representations. We proposed a Computer-Assisted Decision Support System in Pulmonary Cancer by using the novel deep learning based model and metastasis information obtained from MBAN (Medical Body Area Network). The proposed model, DFCNet, is based on the deep fully convolutional neural network (FCNN) which is used for classification of each detected pulmonary nodule into four lung cancer stages. The performance of proposed work is evaluated on different datasets with varying scan conditions. Comparison of proposed classifier is done with the existing CNN techniques. Overall accuracy of CNN and DFCNet was 77.6% and 84.58%, respectively. Experimental results illustrate the effectiveness of proposed method for the detection and classification of lung cancer nodules. These results demonstrate the potential for the proposed technique in helping the radiologists in improving nodule detection accuracy with efficiency.},
  doi      = {10.1016/j.jbi.2018.01.005},
  keywords = {Convolutional neural networks (CNN),Deep learning,Lung cancer stages,MBAN (Medical Body Area Network),Nodule detection,mIoT (medical Internet of Things)},
  pmid     = {29366586},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1532046418300078},
}

@InProceedings{art/LiZ_201806,
  author    = {Li, Zhe and Wang, Chong and Han, Mei and Xue, Yuan and Wei, Wei and Li, Li Jia and Fei-Fei, Li},
  booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title     = {{Thoracic Disease Identification and Localization with Limited Supervision}},
  year      = {2018},
  month     = {jun},
  pages     = {8290--8299},
  publisher = {IEEE},
  abstract  = {Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks.},
  doi       = {10.1109/CVPR.2018.00865},
  isbn      = {9781538664209},
  issn      = {10636919},
  url       = {https://ieeexplore.ieee.org/document/8578963/},
}

@InProceedings{art/KimJ_2018,
  author    = {Kim, Jihang},
  booktitle = {Medical Imaging with Deep Learning (MIDL 2018)},
  title     = {{Lung nodule segmentation with convolutional neural network trained by simple diameter information}},
  year      = {2018},
  number    = {Midl},
  pages     = {3--5},
  abstract  = {Lung nodule segmentation can help radiologists' analysis of nodule risk. Recent deep learning based approaches have shown promising results in the segmentation task. However, a 3D segmentation map necessary for training the algorithms requires an expensive effort from expert radiologists. We propose a new method to train the deep neural network, only utilizing diameter information for each nodule. We validate our model with the LUNA16 dataset, showing competitive results compared to the previous state-of-the-art methods in various evaluation metrics. Our experiments also provide plausible qualitative results comparable to the ground truth segmentation.},
}

@Article{art/GonzalezG_201801,
  author   = {Gonzalez, German and Ash, Samuel Y. and Vegas-S{\'{a}}nchez-Ferrero, Gonzalo and Onieva, Jorge Onieva and Rahaghi, Farbod N. and Ross, James C. and D{\'{a}}z, Alejandro and Est{\'{e}}par, Raul San Jos{\'{e}} and Washko, George R.},
  journal  = {American Journal of Respiratory and Critical Care Medicine},
  title    = {{Disease staging and prognosis in smokers using deep learning in chest computed tomography}},
  year     = {2018},
  issn     = {15354970},
  month    = {jan},
  number   = {2},
  pages    = {193--203},
  volume   = {197},
  abstract = {Rationale: Deep learning is a powerful tool that may allow for improved outcome prediction. Objectives: To determine if deep learning, specifically convolutional neural network (CNN) analysis, could detect and stage chronic obstructive pulmonary disease (COPD) and predict acute respiratory disease (ARD) events and mortality in smokers. Methods: A CNN was trained using computed tomography scans from 7, 983 COPDGene participants and evaluated using 1,000 nonoverlapping COPDGene participants and 1, 672 ECLIPSE participants. Logistic regression (C statistic and the Hosmer-Lemeshow test) was used to assess COPD diagnosis and ARD prediction. Cox regression (C index and the Greenwood-Nam-D'Agnostino test) was used to assess mortality. Measurements and Main Results: In COPDGene, the C statistic for the detection of COPD was 0.856. A total of 51.1% of participants in COPDGene were accurately staged and 74.95% were within one stage. In ECLIPSE, 29.4% were accurately staged and 74.6% were within one stage. In COPDGene and ECLIPSE, the C statistics for ARD events were 0.64 and 0.55, respectively, and the Hosmer-Lemeshow P values were 0.502 and 0.380, respectively, suggesting no evidence of poor calibration. In COPDGene and ECLIPSE, CNN predicted mortality with fair discrimination (C indices, 0.72 and 0.60, respectively), and without evidence of poor calibration (Greenwood-Nam-D'Agnostino P values, 0.307 and 0.331, respectively). Conclusions: A deep-learning approach that uses only computed tomography imaging data can identify those smokers who have COPD and predict who are most likely to have ARD events and those with the highest mortality. At a population level CNN analysis may be a powerful tool for risk assessment.},
  doi      = {10.1164/rccm.201705-0860OC},
  keywords = {Artificial intelligence (computer vision systems),Chronic obstructive pulmonary disease,Neural networks,X-ray computed tomography},
  pmid     = {28892454},
  url      = {http://www.atsjournals.org/doi/10.1164/rccm.201705-0860OC},
}

@InCollection{art/BiffiC_2018,
  author        = {Biffi, Carlo and Oktay, Ozan and Tarroni, Giacomo and Bai, Wenjia and {De Marvao}, Antonio and Doumou, Georgia and Rajchl, Martin and Bedair, Reem and Prasad, Sanjay and Cook, Stuart and O'Regan, Declan and Rueckert, Daniel},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Learning interpretable anatomical features through deep generative models: Application to cardiac remodeling}},
  year          = {2018},
  isbn          = {9783030009335},
  pages         = {464--471},
  volume        = {11071 LNCS},
  abstract      = {Alterations in the geometry and function of the heart define well-established causes of cardiovascular disease. However, current approaches to the diagnosis of cardiovascular diseases often rely on subjective human assessment as well as manual analysis of medical images. Both factors limit the sensitivity in quantifying complex structural and functional phenotypes. Deep learning approaches have recently achieved success for tasks such as classification or segmentation of medical images, but lack interpretability in the feature extraction and decision processes, limiting their value in clinical diagnosis. In this work, we propose a 3D convolutional generative model for automatic classification of images from patients with cardiac diseases associated with structural remodeling. The model leverages interpretable task-specific anatomic patterns learned from 3D segmentations. It further allows to visualise and quantify the learned pathology-specific remodeling patterns in the original input space of the images. This approach yields high accuracy in the categorization of healthy and hypertrophic cardiomyopathy subjects when tested on unseen MR images from our own multi-centre dataset (100%) as well on the ACDC MICCAI 2017 dataset (90%). We believe that the proposed deep learning approach is a promising step towards the development of interpretable classifiers for the medical imaging domain, which may help clinicians to improve diagnostic accuracy and enhance patient risk-stratification.},
  archiveprefix = {arXiv},
  arxivid       = {1807.06843},
  doi           = {10.1007/978-3-030-00934-2_52},
  eprint        = {1807.06843},
  issn          = {16113349},
  url           = {http://link.springer.com/10.1007/978-3-030-00934-2_52},
}

@Article{art/HeG_201903,
  author        = {He, Guocai},
  journal       = {arXiv},
  title         = {{Lung CT Imaging Sign Classification through Deep Learning on Small Data}},
  year          = {2019},
  issn          = {23318422},
  month         = {mar},
  volume        = {abs/1903.0},
  abstract      = {The annotated medical images are usually expensive to be collected. This paper proposes a deep learning method on small data to classify Common Imaging Signs of Lung diseases (CISL) in computed tomography (CT) images. We explore both the real data and the data generated by Generative Adversarial Network (GAN) to improve the reliability and the generalization of learning. First, we use GAN to generate a large number of CISLs from small annotated data, which are difficult to be distinguished from real counterparts. These generated samples are used to pre-train a Convolutional Neural Network (CNN) for classifying CISLs. Second, we fine-tune the CNN classification model with real data. Experiments were conducted on the LISS database of CISLs. We successfully convinced radiologists that our generated CISLs samples were real for 56.7% of our experiments. The pre-trained CNN model achieves 88.4% of mean accuracy of binary classification, and after fine-tuning, the mean accuracy is significantly increased to 95.0%. For multi-classification of all types of CISLs and normal tissues, through the two stages of training, the mean accuracy, sensitivity and specificity are up to about 91.83%, 92.73% and 99.0%, respectively. To our knowledge, this is the best result achieved on the LISS database, which demonstrates that the proposed method is effective and promising for fulfilling deep learning on small data.},
  archiveprefix = {arXiv},
  arxivid       = {1903.00183},
  eprint        = {1903.00183},
  keywords      = {Convolutional Neural Networks (CNNs),Deep learning,Generative Adversarial Network (GAN),Lung CT imaging signs classification,Small data},
  url           = {http://arxiv.org/abs/1903.00183},
}

@Article{art/OktayO_201804,
  author        = {Oktay, Ozan and Schlemper, Jo and {Le Folgoc}, Loic and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  journal       = {arXiv},
  title         = {{Attention U-Net: Learning where to look for the pancreas}},
  year          = {2018},
  issn          = {23318422},
  month         = {apr},
  volume        = {abs/1804.0},
  abstract      = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The source code for the proposed architecture is publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {1804.03999},
  eprint        = {1804.03999},
  url           = {http://arxiv.org/abs/1804.03999},
}

@Article{art/SongY_201304,
  author   = {Song, Yang and Cai, Weidong and Zhou, Yun and Feng, David Dagan},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Feature-based image patch approximation for lung tissue classification}},
  year     = {2013},
  issn     = {02780062},
  month    = {apr},
  number   = {4},
  pages    = {797--808},
  volume   = {32},
  abstract = {In this paper, we propose a new classification method for five categories of lung tissues in high-resolution computed tomography (HRCT) images, with feature-based image patch approximation. We design two new feature descriptors for higher feature descriptiveness, namely the rotation-invariant Gabor-local binary patterns (RGLBP) texture descriptor and multi-coordinate histogram of oriented gradients (MCHOG) gradient descriptor. Together with intensity features, each image patch is then labeled based on its feature approximation from reference image patches. And a new patch-adaptive sparse approximation (PASA) method is designed with the following main components: minimum discrepancy criteria for sparse-based classification, patch-specific adaptation for discriminative approximation, and feature-space weighting for distance computation. The patch-wise labelings are then accumulated as probabilistic estimations for region-level classification. The proposed method is evaluated on a publicly available ILD database, showing encouraging performance improvements over the state-of-the-arts. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2013.2241448},
  keywords = {Adaptive,gradient,reference,texture},
  pmid     = {23340591},
  url      = {http://ieeexplore.ieee.org/document/6415277/},
}

@Article{art/LoshchilovI_201608,
  author        = {Loshchilov, Ilya and Hutter, Frank},
  journal       = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  title         = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
  year          = {2016},
  month         = {aug},
  volume        = {abs/1608.0},
  abstract      = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arXiv},
  arxivid       = {1608.03983},
  eprint        = {1608.03983},
  url           = {http://arxiv.org/abs/1608.03983},
}

@InCollection{art/HusseinS_201704,
  author        = {Hussein, Sarfaraz and Cao, Kunlin and Song, Qi and Bagci, Ulas},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Risk stratification of lung nodules using 3D CNN-based multi-task learning}},
  year          = {2017},
  isbn          = {9783319590493},
  month         = {apr},
  pages         = {249--260},
  volume        = {10265 LNCS},
  abstract      = {Risk stratification of lung nodules is a task of primary importance in lung cancer diagnosis. Any improvement in robust and accurate nodule characterization can assist in identifying cancer stage, prognosis, and improving treatment planning. In this study, we propose a 3D Convolutional Neural Network (CNN) based nodule characterization strategy. With a completely 3D approach, we utilize the volumetric information from a CT scan which would be otherwise lost in the conventional 2D CNN based approaches. In order to address the need for a large amount of training data for CNN, we resort to transfer learning to obtain highly discriminative features.Moreover, we also acquire the task dependent feature representation for six high-level nodule attributes and fuse this complementary information via a Multi-task learning (MTL) framework. Finally, we propose to incorporate potential disagreement among radiologists while scoring different nodule attributes in a graph regularized sparsemulti-task learning. We evaluated our proposed approach on one of the largest publicly available lung nodule datasets comprising 1018 scans and obtained state-of-the-art results in regressing the malignancy scores.},
  archiveprefix = {arXiv},
  arxivid       = {1704.08797},
  doi           = {10.1007/978-3-319-59050-9_20},
  eprint        = {1704.08797},
  issn          = {16113349},
  keywords      = {3D convolutional neural network,Computed tomography (CT),Computer-aided diagnosis (CAD),Deep learning,Lung nodule characterization,Multi-task learning,Transfer learning},
  url           = {http://arxiv.org/abs/1704.08797 http://dx.doi.org/10.1007/978-3-319-59050-9_20 http://link.springer.com/10.1007/978-3-319-59050-9_20},
}

@Article{art/SongY_201506,
  author   = {Song, Yang and Cai, Weidong and Huang, Heng and Zhou, Yun and Feng, David Dagan and Wang, Yue and Fulham, Michael J. and Chen, Mei},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Large margin local estimate with applications to medical image classification}},
  year     = {2015},
  issn     = {1558254X},
  month    = {jun},
  number   = {6},
  pages    = {1362--1377},
  volume   = {34},
  abstract = {Medical images usually exhibit large intra-class variation and inter-class ambiguity in the feature space, which could affect classification accuracy. To tackle this issue, we propose a new Large Margin Local Estimate (LMLE) classification model with sub-categorization based sparse representation. We first sub-categorize the reference sets of different classes into multiple clusters, to reduce feature variation within each subcategory compared to the entire reference set. Local estimates are generated for the test image using sparse representation with reference subcategories as the dictionaries. The similarity between the test image and each class is then computed by fusing the distances with the local estimates in a learning-based large margin aggregation construct to alleviate the problem of inter-class ambiguity. The derived similarities are finally used to determine the class label. We demonstrate that our LMLE model is generally applicable to different imaging modalities, and applied it to three tasks: interstitial lung disease (ILD) classification on high-resolution computed tomography (HRCT) images, phenotype binary classification and continuous regression on brain magnetic resonance (MR) imaging. Our experimental results show statistically significant performance improvements over existing popular classifiers.},
  doi      = {10.1109/TMI.2015.2393954},
  keywords = {Large margin fusion,medical image classification,sparse representation,sub-categorization},
  pmid     = {25616009},
  url      = {http://ieeexplore.ieee.org/document/7014242/},
}

@Article{art/LaLondeR_201804,
  author        = {LaLonde, Rodney and Bagci, Ulas},
  journal       = {arXiv},
  title         = {{Capsules for object segmentation}},
  year          = {2018},
  issn          = {23318422},
  month         = {apr},
  volume        = {abs/1804.0},
  abstract      = {Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al. [2017], referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutionaldeconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 × 512) as opposed to baseline capsules (typically less than 32 × 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4% while still providing a better segmentation accuracy.},
  archiveprefix = {arXiv},
  arxivid       = {1804.04241},
  eprint        = {1804.04241},
  url           = {http://arxiv.org/abs/1804.04241},
}

@Article{art/SuJ_201910,
  author        = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal       = {IEEE Transactions on Evolutionary Computation},
  title         = {{One Pixel Attack for Fooling Deep Neural Networks}},
  year          = {2019},
  issn          = {19410026},
  month         = {oct},
  number        = {5},
  pages         = {828--841},
  volume        = {23},
  abstract      = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
  archiveprefix = {arXiv},
  arxivid       = {1710.08864},
  doi           = {10.1109/TEVC.2019.2890858},
  eprint        = {1710.08864},
  keywords      = {Convolutional neural network,differential evolution (DE),image recognition,information security},
  url           = {http://arxiv.org/abs/1710.08864 http://dx.doi.org/10.1109/TEVC.2019.2890858},
}

@Misc{art/SmithKC_2015,
  author    = {{Smith K}, Clark K and Smith, K and Clark, K and Bennett, W and Nolan, T and Kirby, J and Wolfsberger, M and Moulton, J and Vendt, B and Freymann, J},
  title     = {{Data From CT_COLONOGRAPHY}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.NWTESAY1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/DQE2},
}

@Article{art/ChangC_201111,
  author   = {Chang, Chuan Yu and Chung, Pau Choo and Hong, Yong Cheng and Tseng, Chin Hsiao},
  journal  = {IEEE Computational Intelligence Magazine},
  title    = {{A neural network for thyroid segmentation and volume estimation in CT images}},
  year     = {2011},
  issn     = {1556603X},
  month    = {nov},
  number   = {4},
  pages    = {43--55},
  volume   = {6},
  abstract = {Thyroid region segmentation and volume estimation is a prerequisite step to diagnosing the pathology of the thyroid gland. In this study, a progressive learning vector quantization neural network (PLVQNN) combined with a preprocessing procedure is proposed for automatic thyroid segmentation and volume estimation using computerized tomography (CT) images. The preprocessing procedure is used to extract the region of interest (ROI) of thyroid glands and exclude non-thyroid glands based on thyroid anatomy. The PLVQNN contains several learning vector quantization neural networks (LVQNNs), each responsible for segmenting one slice of a thyroid CT image. The training of the PLVQNN is conducted starting from the LVQNN of most reliable (middle) slices, where the thyroid has the largest region. The training then propagates upwards and downwards to adjacent LVQNNs using the results of the middle slice as the initialization values and constraints. Experimental results show that the proposed method can effectively segment thyroid glands and estimate thyroid volume. {\textcopyright} 2011 IEEE.},
  doi      = {10.1109/MCI.2011.942756},
  url      = {http://ieeexplore.ieee.org/document/6052365/},
}

@Article{art/GuiJ_202001,
  author        = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  journal       = {arXiv},
  title         = {{A review on generative adversarial networks: Algorithms, theory, and applications}},
  year          = {2020},
  issn          = {23318422},
  month         = {jan},
  abstract      = {Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.},
  archiveprefix = {arXiv},
  arxivid       = {2001.06937},
  eprint        = {2001.06937},
  keywords      = {Algorithm,Applications,Deep Learning,Generative Adversarial Networks,Theory},
  url           = {http://arxiv.org/abs/2001.06937},
}

@InProceedings{art/ZhouY_2017,
  author    = {Zhou, Yanning and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng Ann},
  booktitle = {arXiv},
  title     = {{SFCN-OPI: Detection and fine-grained classification of nuclei using sibling FCN with objectness prior interaction}},
  year      = {2017},
  abstract  = {Cell nuclei detection and fine-grained classification have been fundamental yet challenging problems in histopathology image analysis. Due to the nuclei tiny size, significant inter/intra-class variances, as well as the inferior image quality, previous automated methods would easily suffer from limited accuracy and robustness. In the meanwhile, existing approaches usually deal with these two tasks independently, which would neglect the close relatedness of them. In this paper, we present a novel method of sibling fully convolutional network with prior objectness interaction (called SFCN-OPI) to tackle the two tasks simultaneously and interactively using a unified end-to-end framework. Specifically, the sibling FCN branches share features in earlier layers while holding respective higher layers for specific tasks. More importantly, the detection branch outputs the objectness prior which dynamically interacts with the fine-grained classification sibling branch during the training and testing processes. With this mechanism, the fine-grained classification successfully focuses on regions with high confidence of nuclei existence and outputs the conditional probability, which in turn benefits the detection through back propagation. Extensive experiments on colon cancer histology images have validated the effectiveness of our proposed SFCN-OPI and our method has outperformed the state-of-the-art methods by a large margin.},
}

@Article{art/ZhangH_201912,
  author        = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
  year          = {2019},
  issn          = {19393539},
  month         = {dec},
  number        = {8},
  pages         = {1947--1962},
  volume        = {41},
  abstract      = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
  archiveprefix = {arXiv},
  arxivid       = {1710.10916},
  doi           = {10.1109/TPAMI.2018.2856256},
  eprint        = {1710.10916},
  keywords      = {Generative models,generative adversarial networks (GANs),multi-distribution approximation,multi-stage GANs,photo-realistic image generation,text-to-image synthesis},
  pmid          = {30010548},
  url           = {http://arxiv.org/abs/1612.03242},
}

@Article{art/GoodfellowI_201406,
  author        = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title         = {{Generative Adversarial Networks}},
  year          = {2014},
  month         = {jun},
  abstract      = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  arxivid       = {1406.2661},
  eprint        = {1406.2661},
  url           = {http://arxiv.org/abs/1406.2661},
}

@Article{art/_202003,
  journal  = {Alzheimer's & Dementia},
  title    = {{2020 Alzheimer's disease facts and figures}},
  year     = {2020},
  issn     = {1552-5260},
  month    = {mar},
  number   = {3},
  pages    = {391--460},
  volume   = {16},
  abstract = {This article describes the public health impact of Alzheimer's disease (AD), including incidence and prevalence, mortality and morbidity, use and costs of care, and the overall impact on caregivers and society. The Special Report discusses the future challenges of meeting care demands for the growing number of people living with Alzheimer's dementia in the United States with a particular emphasis on primary care. By mid-century, the number of Americans age 65 and older with Alzheimer's dementia may grow to 13.8 million. This represents a steep increase from the estimated 5.8 million Americans age 65 and older who have Alzheimer's dementia today. Official death certificates recorded 122,019 deaths from AD in 2018, the latest year for which data are available, making Alzheimer's the sixth leading cause of death in the United States and the fifth leading cause of death among Americans age 65 and older. Between 2000 and 2018, deaths resulting from stroke, HIV and heart disease decreased, whereas reported deaths from Alzheimer's increased 146.2%. In 2019, more than 16 million family members and other unpaid caregivers provided an estimated 18.6 billion hours of care to people with Alzheimer's or other dementias. This care is valued at nearly $244 billion, but its costs extend to family caregivers' increased risk for emotional distress and negative mental and physical health outcomes. Average per-person Medicare payments for services to beneficiaries age 65 and older with AD or other dementias are more than three times as great as payments for beneficiaries without these conditions, and Medicaid payments are more than 23 times as great. Total payments in 2020 for health care, long-term care and hospice services for people age 65 and older with dementia are estimated to be $305 billion. As the population of Americans living with Alzheimer's dementia increases, the burden of caring for that population also increases. These challenges are exacerbated by a shortage of dementia care specialists, which places an increasing burden on primary care physicians (PCPs) to provide care for people living with dementia. Many PCPs feel underprepared and inadequately trained to handle dementia care responsibilities effectively. This report includes recommendations for maximizing quality care in the face of the shortage of specialists and training challenges in primary care.},
  doi      = {10.1002/alz.12068},
  keywords = {Alzheimer's dementia,Alzheimer's disease,Alzheimer's disease continuum,Biomarkers,Caregivers,Dementia,Dementia care training,Family caregiver,Geriatrician,Health care costs,Health care expenditures,Health care professional,Incidence,Long-term care costs,Medicaid spending,Medicare spending,Morbidity,Mortality,Prevalence,Primary care physician,Risk factors,Spouse caregiver},
  pmid     = {32157811},
  url      = {https://onlinelibrary.wiley.com/doi/10.1002/alz.12068},
}

@Article{art/MoenE_201912,
  author   = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and {Van Valen}, David},
  journal  = {Nature Methods},
  title    = {{Deep learning for cellular image analysis}},
  year     = {2019},
  issn     = {15487105},
  month    = {dec},
  number   = {12},
  pages    = {1233--1246},
  volume   = {16},
  abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
  doi      = {10.1038/s41592-019-0403-1},
  pmid     = {31133758},
  url      = {http://www.nature.com/articles/s41592-019-0403-1},
}

@Article{art/RezaeiM_202004,
  author        = {Rezaei, Mahdi and Shahidi, Mahsa},
  journal       = {arXiv},
  title         = {{Zero-Shot Learning and Its Applications From Autonomous Vehicles to Covid-19 Diagnosis: A Review}},
  year          = {2020},
  issn          = {26665212},
  month         = {apr},
  abstract      = {The challenge of learning a new concept without receiving any examples beforehand is called zero-shot learning (ZSL). One of the major issues in deep learning based methodologies is the requirement of feeding a vast amount of annotated and labelled images by a human to train the network model. ZSL is known for having minimal human intervention by relying only on previously known concepts and auxiliary information. It is an ever-growing research area since it has human-like characteristics in learning new concepts, which makes it applicable in many real-world scenarios, from autonomous vehicles to surveillance systems to medical imaging and COVID-19 CT scan-based diagnosis. In this paper, we present the definition of the problem, we review over fundamentals, and the challenging steps of Zero-shot learning, including recent categories of solutions, motivations behind each approach, and their advantages over other categories. Inspired from different settings and extensions, we have a broaden solution called one/few-shot learning. We then review thorough datasets, the variety of splits, and the evaluation protocols proposed so far. Finally, we discuss the recent applications and possible future directions of ZSL. We aim to convey a useful intuition through this paper towards the goal of handling computer vision learning tasks more similar to the way humans learn.},
  archiveprefix = {arXiv},
  arxivid       = {2004.14143},
  doi           = {10.1016/j.ibmed.2020.100005},
  eprint        = {2004.14143},
  keywords      = {Autonomous Vehicles,COVID-19 Pandemic,Deep Learning,Machine Learning,Semantic Embedding,Supervised Annotation,Zero-shot learning},
  url           = {http://arxiv.org/abs/2004.14143},
}

@Article{art/MondalA_201810,
  author        = {Mondal, Arnab Kumar and Dolz, Jose and Desrosiers, Christian},
  journal       = {arXiv},
  title         = {{Few-shot 3D multi-modal medical image segmentation using generative adversarial learning}},
  year          = {2018},
  issn          = {23318422},
  month         = {oct},
  abstract      = {We address the problem of segmenting 3D multimodal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-Art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot GAN-Unet3D.},
  archiveprefix = {arXiv},
  arxivid       = {1810.12241},
  eprint        = {1810.12241},
  keywords      = {Deep learning,Few-shot learning,GANs,Multi-modal brain MRI,Semisupervised segmentation},
  url           = {http://arxiv.org/abs/1810.12241},
}

@Article{art/DettmersT_2016,
  author        = {Dettmers, Tim},
  journal       = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  title         = {{8-Bit Approximations for Parallelism in Deep Learning}},
  year          = {2016},
  abstract      = {The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.},
  archiveprefix = {arXiv},
  arxivid       = {1511.04561},
  eprint        = {1511.04561},
}

@Article{art/VanhouckeV_2011,
  author   = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mz},
  journal  = {Proc. Deep Learning and {\ldots}},
  title    = {{Improving the speed of neural networks on CPUs}},
  year     = {2011},
  issn     = {9781450329569},
  pages    = {1--8},
  abstract = {Recent advances in deep learning have made the use of large, deep neural net- works with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improve- ment over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.},
  url      = {http://research.google.com/pubs/archive/37631.pdf},
}

@Article{art/TaiC_2016,
  author        = {Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and Weinan, E.},
  journal       = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  title         = {{Convolutional neural networks with low-rank regularization}},
  year          = {2016},
  abstract      = {Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves 91.31% accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.},
  archiveprefix = {arXiv},
  arxivid       = {1511.06067},
  eprint        = {1511.06067},
}

@Article{art/DentonE_2014,
  author        = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {{Exploiting linear structure within convolutional networks for efficient evaluation}},
  year          = {2014},
  issn          = {10495258},
  number        = {January},
  pages         = {1269--1277},
  volume        = {2},
  abstract      = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 ×, while keeping the accuracy within 1% of the original model.},
  archiveprefix = {arXiv},
  arxivid       = {1404.0736},
  eprint        = {1404.0736},
}

@Article{art/LebedevV_2016,
  author        = {Lebedev, Vadim and Lempitsky, Victor},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Fast ConvNets Using Group-Wise Brain Damage}},
  year          = {2016},
  issn          = {10636919},
  pages         = {2554--2564},
  volume        = {2016-Decem},
  abstract      = {We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers in ConvNets. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion. After such pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. We investigate different ways to add group-wise prunning to the learning process, and show that severalfold speedups of convolutional layers can be attained using group-sparsity regularizers. Our approach can adjust the shapes of the receptive fields in the convolutional layers, and even prune excessive feature maps from ConvNets, all in data-driven way.},
  archiveprefix = {arXiv},
  arxivid       = {1506.02515},
  doi           = {10.1109/CVPR.2016.280},
  eprint        = {1506.02515},
  isbn          = {9781467388504},
}

@Article{art/ChoiY_201712,
  author        = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
  journal       = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  title         = {{Towards the limit of network quantization}},
  year          = {2017},
  month         = {dec},
  abstract      = {Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.},
  archiveprefix = {arXiv},
  arxivid       = {1612.01543},
  eprint        = {1612.01543},
  url           = {http://arxiv.org/abs/1612.01543},
}

@InProceedings{art/CzarneckiW_2017,
  author        = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{Sobolev training for neural networks}},
  year          = {2017},
  pages         = {4279--4288},
  volume        = {2017-Decem},
  abstract      = {At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
  archiveprefix = {arXiv},
  arxivid       = {1706.04859},
  eprint        = {1706.04859},
  issn          = {10495258},
}

@Article{art/LiZ_2018,
  author        = {Li, Zhizhong and Hoiem, Derek},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Learning without Forgetting}},
  year          = {2018},
  issn          = {19393539},
  number        = {12},
  pages         = {2935--2947},
  volume        = {40},
  abstract      = {When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
  archiveprefix = {arXiv},
  arxivid       = {1606.09282},
  doi           = {10.1109/TPAMI.2017.2773081},
  eprint        = {1606.09282},
  keywords      = {Convolutional neural networks,deep learning,multi-task learning,transfer learning,visual recognition},
  pmid          = {29990101},
}

@Article{art/MoeskopsP_201707,
  author        = {Moeskops, Pim and Veta, Mitko and Lafarge, Maxime W. and Eppenhof, Koen A.J. and Pluim, Josien P.W.},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Adversarial training and dilated convolutions for brain MRI segmentation}},
  year          = {2017},
  issn          = {16113349},
  month         = {jul},
  pages         = {56--64},
  volume        = {10553 LNCS},
  abstract      = {Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images. In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss. The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.},
  archiveprefix = {arXiv},
  arxivid       = {1707.03195},
  doi           = {10.1007/978-3-319-67558-9_7},
  eprint        = {1707.03195},
  isbn          = {9783319675572},
  keywords      = {Adversarial networks,Brain MRI,Convolutional neural networks,Deep learning,Dilated convolution,Medical image segmentation},
  url           = {http://arxiv.org/abs/1707.03195},
}

@Article{art/LucP_201611,
  author        = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
  title         = {{Semantic Segmentation using Adversarial Networks}},
  year          = {2016},
  month         = {nov},
  abstract      = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
  archiveprefix = {arXiv},
  arxivid       = {1611.08408},
  eprint        = {1611.08408},
  url           = {http://arxiv.org/abs/1611.08408},
}

@Article{art/ZhuW_201612,
  author        = {Zhu, Wentao and Xiang, Xiang and Tran, Trac D. and Xie, Xiaohui},
  title         = {{Adversarial Deep Structural Networks for Mammographic Mass Segmentation}},
  year          = {2016},
  month         = {dec},
  abstract      = {Mass segmentation is an important task in mammogram analysis, providing effective morphological features and regions of interest (ROI) for mass detection and classification. Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation. The network employs a fully convolutional network (FCN) to model potential function, followed by a CRF to perform structural learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with position priori for the task. Due to the small size of mammogram datasets, we use adversarial training to control over-fitting. Four models with different convolutional kernels are further fused to improve the segmentation results. Experimental results on two public datasets, INbreast and DDSM-BCRP, show that our end-to-end network combined with adversarial training achieves the-state-of-the-art results.},
  archiveprefix = {arXiv},
  arxivid       = {1612.05970},
  doi           = {10.1101/095786},
  eprint        = {1612.05970},
  url           = {http://arxiv.org/abs/1612.05970},
}

@Article{art/WangZ_201906,
  author        = {Wang, Zhengwei and She, Qi and Ward, Tom{\'{a}}s E.},
  journal       = {arXiv},
  title         = {{Generative adversarial networks: A survey and taxonomy}},
  year          = {2019},
  issn          = {23318422},
  month         = {jun},
  abstract      = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably the revolutionary techniques are in the area of computer vision such as plausible image generation, image to image translation, facial attribute manipulation and similar domains. Despite the significant success achieved in the computer vision field, applying GANs to real-world problems still poses significant challenges, three of which we focus on here: (1) High quality image generation; (2) Diverse image generation; and (3) Stable training. Through an in-depth review of GAN-related research in the literature, we provide an account of the architecture-variants and loss-variants, which have been proposed to handle these three challenges from two perspectives. We propose loss-variants and architecture-variants for classifying the most popular GANs, and discuss the potential improvements with focusing on these two aspects. While several reviews for GANs have been presented to date, none have focused on the review of GAN-variants based on their handling the challenges mentioned above. In this paper, we review and critically discuss 7 architecture-variant GANs and 9 loss-variant GANs for remedying those three challenges. The objective of this review is to provide an insight on the footprint that current GANs research focuses on the performance improvement. Code related to GAN-variants studied in this work is summarized on https:// github.com/ sheqi/GAN Review.},
  archiveprefix = {arXiv},
  arxivid       = {1906.01529},
  eprint        = {1906.01529},
  keywords      = {Architecture-variants,Computer vision,Generative adversarial networks,Loss-variants,Stable training},
  url           = {http://arxiv.org/abs/1906.01529},
}

@Article{art/DesikanR_200607,
  author   = {Desikan, Rahul S. and S{\'{e}}gonne, Florent and Fischl, Bruce and Quinn, Brian T. and Dickerson, Bradford C. and Blacker, Deborah and Buckner, Randy L. and Dale, Anders M. and Maguire, R. Paul and Hyman, Bradley T. and Albert, Marilyn S. and Killiany, Ronald J.},
  journal  = {NeuroImage},
  title    = {{An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest}},
  year     = {2006},
  issn     = {10538119},
  month    = {jul},
  number   = {3},
  pages    = {968--980},
  volume   = {31},
  abstract = {In this study, we have assessed the validity and reliability of an automated labeling system that we have developed for subdividing the human cerebral cortex on magnetic resonance images into gyral based regions of interest (ROIs). Using a dataset of 40 MRI scans we manually identified 34 cortical ROIs in each of the individual hemispheres. This information was then encoded in the form of an atlas that was utilized to automatically label ROIs. To examine the validity, as well as the intra- and inter-rater reliability of the automated system, we used both intraclass correlation coefficients (ICC), and a new method known as mean distance maps, to assess the degree of mismatch between the manual and the automated sets of ROIs. When compared with the manual ROIs, the automated ROIs were highly accurate, with an average ICC of 0.835 across all of the ROIs, and a mean distance error of less than 1 mm. Intra- and inter-rater comparisons yielded little to no difference between the sets of ROIs. These findings suggest that the automated method we have developed for subdividing the human cerebral cortex into standard gyral-based neuroanatomical regions is both anatomically valid and reliable. This method may be useful for both morphometric and functional studies of the cerebral cortex as well as for clinical investigations aimed at tracking the evolution of disease-induced changes over time, including clinical trials in which MRI-based measures are used to examine response to treatment. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
  doi      = {10.1016/j.neuroimage.2006.01.021},
  pmid     = {16530430},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811906000437},
}

@Article{art/ClarkK_201312,
  author   = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
  journal  = {Journal of Digital Imaging},
  title    = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
  year     = {2013},
  issn     = {08971889},
  month    = {dec},
  number   = {6},
  pages    = {1045--1057},
  volume   = {26},
  abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
  doi      = {10.1007/s10278-013-9622-7},
  keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
  pmid     = {23884657},
  url      = {https://doi.org/10.1007/s10278-013-9622-7 http://link.springer.com/10.1007/s10278-013-9622-7},
}

@Misc{art/MackinD_2017,
  author    = {Mackin, Dennis and Ray, Xenia and Zhang, Lifei and Fried, David and Yang, Jinzhong and Taylor, Brian and Rodriguez-Rivera, Edgardo and Dodge, Cristina and Jones, Aaron and Court, Laurence},
  title     = {{Data From Credence Cartridge Radiomics Phantom CT Scans}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.ZUZRML5B},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/jIxyAQ},
}

@Misc{art/ShafiqulHassanM_2018,
  author    = {M, Shafiq ul Hassan and G, Zhang and K, Latifi and G, Ullah and R, Gillies and E, Moros},
  title     = {{Credence Cartridge Radiomics Phantom CT Scans with Controlled Scanning Approach (CC-Radiomics-Phantom-2)}},
  year      = {2018},
  doi       = {10.7937/TCIA.2019.4L24TZ5G},
  publisher = {The Cancer Imaging Archive},
  url       = {http://doi.org/10.7937/TCIA.2019.4l24tz5g},
}

@Article{art/GerR_201812,
  author   = {Ger, Rachel B. and Zhou, Shouhao and Chi, Pai Chun Melinda and Lee, Hannah J. and Layman, Rick R. and Jones, A. Kyle and Goff, David L. and Fuller, Clifton D. and Howell, Rebecca M. and Li, Heng and Stafford, R. Jason and Court, Laurence E. and Mackin, Dennis S.},
  journal  = {Scientific Reports},
  title    = {{Comprehensive Investigation on Controlling for CT Imaging Variabilities in Radiomics Studies}},
  year     = {2018},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {13047},
  volume   = {8},
  abstract = {Radiomics has shown promise in improving models for predicting patient outcomes. However, to maximize the information gain of the radiomics features, especially in larger patient cohorts, the variability in radiomics features owing to differences between scanners and scanning protocols must be accounted for. To this aim, the imaging variability of radiomics feature values was evaluated on 100 computed tomography scanners at 35 clinics by imaging a radiomics phantom using a controlled protocol and the commonly used chest and head protocols of the local clinic. We used a linear mixed-effects model to determine the degree to which the manufacturer and individual scanners contribute to the overall variability. Using a controlled protocol reduced the overall variability by 57% and 52% compared to the local chest and head protocols respectively. The controlled protocol also reduced the relative contribution of the manufacturer to the total variability. For almost all variabilities (manufacturer, scanner, and residual with different preprocesssing), the controlled protocol scans had a significantly smaller variability than the local protocol scans did. For most radiomics features, the imaging variability was small relative to the inter-patient feature variability in non–small cell lung cancer and head and neck squamous cell carcinoma patient cohorts. From this study, we conclude that using controlled scans can reduce the variability in radiomics features, and our results demonstrate the importance of using controlled protocols in prospective radiomics studies.},
  doi      = {10.1038/s41598-018-31509-z},
  pmid     = {30158540},
  url      = {http://www.nature.com/articles/s41598-018-31509-z},
}

@Article{art/GerR_201809,
  author   = {Ger, Rachel B. and Yang, Jinzhong and Ding, Yao and Jacobsen, Megan C. and Cardenas, Carlos E. and Fuller, Clifton D. and Howell, Rebecca M. and Li, Heng and Stafford, R. Jason and Zhou, Shouhao and Court, Laurence E.},
  journal  = {Medical Physics},
  title    = {{Synthetic head and neck and phantom images for determining deformable image registration accuracy in magnetic resonance imaging}},
  year     = {2018},
  issn     = {00942405},
  month    = {sep},
  number   = {9},
  pages    = {4315--4321},
  volume   = {45},
  abstract = {Purpose: Magnetic resonance imaging (MRI) provides noninvasive evaluation of patient's anatomy without using ionizing radiation. Due to this and the high soft-tissue contrast, MRI use has increased and has potential for use in longitudinal studies where changes in patients' anatomy or tumors at different time points are compared. Deformable image registration can be useful for these studies. Here, we describe two datasets that can be used to evaluate the registration accuracy of systems for MR images, as it cannot be assumed to be the same as that measured on CT images. Acquisition and validation methods: Two sets of images were created to test registration accuracy. (a) A porcine phantom was created by placing ten 0.35-mm gold markers into porcine meat. The porcine phantom was immobilized in a plastic container with movable dividers. T1-weighted, T2-weighted, and CT images were acquired with the porcine phantom compressed in four different ways. The markers were not visible on the MR images, due to the selected voxel size, so they did not interfere with the measured registration accuracy, while the markers were visible on the CT images and were used to identify the known deformation between positions. (b) Synthetic images were created using 28 head and neck squamous cell carcinoma patients who had MR scans pre-, mid-, and postradiotherapy treatment. An inter- and intrapatient variation model was created using these patient scans. Four synthetic pretreatment images were created using the interpatient model, and four synthetic post-treatment images were created for each of the pretreatment images using the intrapatient model. Data format and usage notes: The T1-weighted, T2-weighted, and CT scans of the porcine phantom in the four positions are provided. Four T1-weighted synthetic pretreatment images each with four synthetic post-treatment images, and four T2-weighted synthetic pretreatment images each with four synthetic post-treatment images are provided. Additionally, the applied deformation vector fields to generate the synthetic post-treatment images are provided. The data are available on TCIA under the collection MRI-DIR. Potential applications: The proposed database provides two sets of images (one acquired and one computer generated) for use in evaluating deformable image registration accuracy. T1- and T2-weighted images are available for each technique as the different image contrast in the two types of images may impact the registration accuracy.},
  doi      = {10.1002/mp.13090},
  keywords = {MRI,deformable image registration,deformation,registration},
  pmid     = {30007075},
  url      = {http://doi.wiley.com/10.1002/mp.13090},
}

@Article{art/NadkarniN_201901,
  author   = {Nadkarni, Nachiket A. and Bougacha, Salma and Garin, Cl{\'{e}}ment and Dhenain, Marc and Picq, Jean Luc},
  journal  = {NeuroImage},
  title    = {{A 3D population-based brain atlas of the mouse lemur primate with examples of applications in aging studies and comparative anatomy}},
  year     = {2019},
  issn     = {10959572},
  month    = {jan},
  pages    = {85--95},
  volume   = {185},
  abstract = {The gray mouse lemur (Microcebus murinus) is a small prosimian of growing interest for studies of primate biology and evolution, and notably as a model organism of brain aging. As brain atlases are essential tools for brain investigation, the objective of the current work was to create the first 3D digital atlas of the mouse lemur brain. For this, a template image was constructed from in vivo magnetic resonance imaging (MRI) data of 34 animals. This template was then manually segmented into 40 cortical, 74 subcortical and 6 cerebro-spinal fluid (CSF) regions. Additionally, we generated probability maps of gray matter, white matter and CSF. The template, manual segmentation and probability maps, as well as imaging tools used to create and manipulate the template, can all be freely downloaded. The atlas was first used to automatically assess regional age-associated cerebral atrophy in a cohort of mouse lemurs previously studied by voxel based morphometry (VBM). Results based on the atlas were in good agreement with the VBM ones, showing age-associated atrophy in the same brain regions such as the insular, parietal or occipital cortices as well as the thalamus or hypothalamus. The atlas was also used as a tool for comparative neuroanatomy. To begin with, we compared measurements of brain regions in our MRI data with histology-based measures from a reference article largely used in previous comparative neuroanatomy studies. We found large discrepancies between our MRI-based data and those of the reference histology-based article. Next, regional brain volumes were compared amongst the mouse lemur and several other mammalian species where high quality volumetric MRI brain atlases were available, including rodents (mouse, rat) and primates (marmoset, macaque, and human). Unlike those based on histological atlases, measures from MRI atlases indicated similar cortical to cerebral volume indices in all primates, including in mouse lemurs, and lower values in mice. On the other hand, white matter to cerebral volume index increased from rodents to small primates (mouse lemurs and marmosets) to macaque, reaching their highest values in humans.},
  doi      = {10.1016/j.neuroimage.2018.10.010},
  keywords = {Atlas,Cerebral atrophy,Comparative anatomy,MRI,Mouse lemur,Template},
  pmid     = {30326295},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918319694},
}

@Misc{art/NadkarniN_2019,
  author    = {Nadkarni, N A and Bougacha, S and Garin, C and Picq, J L and Dhenain, M},
  title     = {{MouseLemurAtlas_MRIraw}},
  year      = {2019},
  doi       = {10.18112/OPENNEURO.DS001945.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001945/versions/1.0.0},
}

@Misc{art/TakataN_2020,
  author    = {Takata, Norio and Sato, Nobuhiko and Komaki, Yuji and Okano, Hideyuki and Tanaka, Kenji F},
  title     = {{Mouse_awake_rest}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002551.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002551/versions/1.0.0},
}

@Misc{art/GerR_2018,
  author    = {Ger, Rachel and Yang, Jinzhong and Ding, Yao and Jacobsen, Megan and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, R Jason and Zhou, Shouhao and Court, Laurence},
  title     = {{Data from Synthetic and Phantom MR Images for Determining Deformable Image Registration Accuracy (MRI-DIR)}},
  year      = {2018},
  doi       = {10.7937/K9/TCIA.2018.3F08IEJT},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/-gA4Ag},
}

@Misc{art/GerR_2019,
  author    = {Ger, Rachel and Zhou, Shouhao and Chi, Pai-Chun and Lee, Hannah and Layman, Rick R and Jones, Kyle and Goff, David and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, Jason and Court, Laurence and Mackin, Dennis},
  title     = {{Data from CT Phantom Scans for Head, Chest, and Controlled Protocols on 100 Scanners (CC-Radiomics-Phantom-3)}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.J71I4FAH},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/RADDAg},
}

@Misc{art/MuziP_2015,
  author    = {Muzi, Peter and Wanner, Michelle and Kinahan, Paul},
  title     = {{Data From RIDER_PHANTOM_PET-CT}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.8WG2KN4W},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bIRXAQ},
}

@Article{art/CodellaN_201902,
  author        = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and {Emre Celebi}, M. and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
  journal       = {arXiv},
  title         = {{Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)}},
  year          = {2019},
  month         = {feb},
  abstract      = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10% of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
  archiveprefix = {arXiv},
  arxivid       = {1902.03368},
  eprint        = {1902.03368},
  keywords      = {Deep learning,Dermoscopy,Melanoma,Skin cancer},
  url           = {http://arxiv.org/abs/1902.03368},
}

@Article{art/RajpurkarP_201712,
  author        = {Rajpurkar, Pranav and Irvin, Jeremy and Bagul, Aarti and Ding, Daisy and Duan, Tony and Mehta, Hershel and Yang, Brandon and Zhu, Kaylie and Laird, Dillon and Ball, Robyn L. and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
  journal       = {arXiv},
  title         = {{MURA: Large dataset for abnormality detection in musculoskeletal radiographs}},
  year          = {2017},
  issn          = {23318422},
  month         = {dec},
  abstract      = {We introduce MURA, a large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. To evaluate models robustly and to get an estimate of radiologist performance, we collect additional labels from six boardcertified Stanford radiologists on the test set, consisting of 207 musculoskeletal studies. On this test set, the majority vote of a group of three radiologists serves as gold standard. We train a 169-layer DenseNet baseline model to detect and localize abnormalities. Our model achieves an AUROC of 0.929, with an operating point of 0.815 sensitivity and 0.887 specificity. We compare our model and radiologists on the Cohen's kappa statistic, which expresses the agreement of our model and of each radiologist with the gold standard. Model performance is comparable to the best radiologist performance in detecting abnormalities on finger and wrist studies. However, model performance is lower than best radiologist performance in detecting abnormalities on elbow, forearm, hand, humerus, and shoulder studies. We believe that the task is a good challenge for future research. To encourage advances, we have made our dataset freely available at http://stanfordmlgroup.github.io/competitions/mura.},
  archiveprefix = {arXiv},
  arxivid       = {1712.06957},
  eprint        = {1712.06957},
  url           = {http://arxiv.org/abs/1712.06957},
}

@Article{art/BienN_201811,
  author   = {Bien, Nicholas and Rajpurkar, Pranav and Ball, Robyn L. and Irvin, Jeremy and Park, Allison and Jones, Erik and Bereket, Michael and Patel, Bhavik N. and Yeom, Kristen W. and Shpanskaya, Katie and Halabi, Safwan and Zucker, Evan and Fanton, Gary and Amanatullah, Derek F. and Beaulieu, Christopher F. and Riley, Geoffrey M. and Stewart, Russell J. and Blankenberg, Francis G. and Larson, David B. and Jones, Ricky H. and Langlotz, Curtis P. and Ng, Andrew Y. and Lungren, Matthew P.},
  journal  = {PLoS Medicine},
  title    = {{Deep-learning-assisted diagnosis for knee magnetic resonance imaging: Development and retrospective validation of MRNet}},
  year     = {2018},
  issn     = {15491676},
  month    = {nov},
  number   = {11},
  pages    = {e1002699},
  volume   = {15},
  abstract = {Background: Magnetic resonance imaging (MRI) of the knee is the preferred method for diagnosing knee injuries. However, interpretation of knee MRI is time-intensive and subject to diagnostic error and variability. An automated system for interpreting knee MRI could prioritize high-risk patients and assist clinicians in making diagnoses. Deep learning methods, in being able to automatically learn layers of features, are well suited for modeling the complex relationships between medical images and their interpretations. In this study we developed a deep learning model for detecting general abnormalities and specific diagnoses (anterior cruciate ligament [ACL] tears and meniscal tears) on knee MRI exams. We then measured the effect of providing the model's predictions to clinical experts during interpretation. Methods and findings: Our dataset consisted of 1,370 knee MRI exams performed at Stanford University Medical Center between January 1, 2001, and December 31, 2012 (mean age 38.0 years; 569 [41.5%] female patients). The majority vote of 3 musculoskeletal radiologists established reference standard labels on an internal validation set of 120 exams. We developed MRNet, a convolutional neural network for classifying MRI series and combined predictions from 3 series per exam using logistic regression. In detecting abnormalities, ACL tears, and meniscal tears, this model achieved area under the receiver operating characteristic curve (AUC) values of 0.937 (95% CI 0.895, 0.980), 0.965 (95% CI 0.938, 0.993), and 0.847 (95% CI 0.780, 0.914), respectively, on the internal validation set. We also obtained a public dataset of 917 exams with sagittal T1-weighted series and labels for ACL injury from Clinical Hospital Centre Rijeka, Croatia. On the external validation set of 183 exams, the MRNet trained on Stanford sagittal T2-weighted series achieved an AUC of 0.824 (95% CI 0.757, 0.892) in the detection of ACL injuries with no additional training, while an MRNet trained on the rest of the external data achieved an AUC of 0.911 (95% CI 0.864, 0.958). We additionally measured the specificity, sensitivity, and accuracy of 9 clinical experts (7 board-certified general radiologists and 2 orthopedic surgeons) on the internal validation set both with and without model assistance. Using a 2-sided Pearson's chi-squared test with adjustment for multiple comparisons, we found no significant differences between the performance of the model and that of unassisted general radiologists in detecting abnormalities. General radiologists achieved significantly higher sensitivity in detecting ACL tears (p-value = 0.002; q-value = 0.019) and significantly higher specificity in detecting meniscal tears (p-value = 0.003; q-value = 0.019). Using a 1-tailed t test on the change in performance metrics, we found that providing model predictions significantly increased clinical experts' specificity in identifying ACL tears (p-value < 0.001; q-value = 0.006). The primary limitations of our study include lack of surgical ground truth and the small size of the panel of clinical experts. Conclusions: Our deep learning model can rapidly generate accurate clinical pathology classifications of knee MRI exams from both internal and external datasets. Moreover, our results support the assertion that deep learning models can improve the performance of clinical experts during medical imaging interpretation. Further research is needed to validate the model prospectively and to determine its utility in the clinical setting.},
  doi      = {10.1371/journal.pmed.1002699},
  editor   = {Saria, Suchi},
  pmid     = {30481176},
  url      = {https://dx.plos.org/10.1371/journal.pmed.1002699},
}

@Article{art/TschandlP_201812,
  author   = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
  journal  = {Scientific Data},
  title    = {{Data descriptor: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions}},
  year     = {2018},
  issn     = {20524463},
  month    = {dec},
  number   = {1},
  pages    = {180161},
  volume   = {5},
  abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50% of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
  doi      = {10.1038/sdata.2018.161},
  pmid     = {30106392},
  url      = {http://www.nature.com/articles/sdata2018161},
}

@Article{art/CodellaN_201810,
  author        = {Codella, Noel C.F. and Gutman, David and Celebi, M. Emre and Helba, Brian and Marchetti, Michael A. and Dusza, Stephen W. and Kalloo, Aadi and Liopyris, Konstantinos and Mishra, Nabin and Kittler, Harald and Halpern, Allan},
  journal       = {Proceedings - International Symposium on Biomedical Imaging},
  title         = {{Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)}},
  year          = {2018},
  issn          = {19458452},
  month         = {oct},
  pages         = {168--172},
  volume        = {2018-April},
  abstract      = {This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classification. Participation involved 593 registrations, 81 pre-submissions, 46 finalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this field to date. While the official challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.},
  archiveprefix = {arXiv},
  arxivid       = {1710.05006},
  doi           = {10.1109/ISBI.2018.8363547},
  eprint        = {1710.05006},
  isbn          = {9781538636367},
  keywords      = {Challenge,Dataset,Deep learning,Dermatology,Dermoscopy,Melanoma,Skin cancer},
  url           = {http://arxiv.org/abs/1710.05006},
}

@InProceedings{art/SongY_201504,
  author    = {Song, Yang and Cai, Weidong and Zhang, Fan and Huang, Heng and Zhou, Yun and {Dagan Feng}, David},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  title     = {{Bone texture characterization with fisher encoding of local descriptors}},
  year      = {2015},
  month     = {apr},
  pages     = {5--8},
  publisher = {IEEE},
  volume    = {2015-July},
  abstract  = {Bone texture characterization is important for differentiating osteoporotic and healthy subjects. Automated classification is however very challenging due to the high degree of visual similarity between the two types of images. In this paper, we propose to describe the bone textures by extracting dense sets of local descriptors and encoding them with the improved Fisher vector (IFV). Compared to the standard bag-of-visual-words (BoW) model, Fisher encoding is more discriminative by representing the distribution of local descriptors in addition to the occurrence frequencies. Our method is evaluated on the ISBI 2014 challenge dataset of bone texture characterization, and we demonstrate excellent classification performance compared to the challenge entries and large improvement over the BoW model.},
  doi       = {10.1109/ISBI.2015.7163803},
  isbn      = {9781479923748},
  issn      = {19458452},
  keywords  = {Bone texture,Fisher vector,classification,feature encoding},
  url       = {http://ieeexplore.ieee.org/document/7163803/},
}

@InProceedings{art/MishraR_2018,
  author    = {Mishra, Rashika and Daescu, Ovidiu and Leavey, Patrick and Rakheja, Dinesh and Sengupta, Anita},
  booktitle = {Journal of Computational Biology},
  title     = {{Convolutional neural network for histopathological analysis of osteosarcoma}},
  year      = {2018},
  number    = {3},
  pages     = {313--325},
  volume    = {25},
  abstract  = {Pathologists often deal with high complexity and sometimes disagreement over osteosarcoma tumor classification due to cellular heterogeneity in the dataset. Segmentation and classification of histology tissue in H&E stained tumor image datasets is a challenging task because of intra-class variations, inter-class similarity, crowded context, and noisy data. In recent years, deep learning approaches have led to encouraging results in breast cancer and prostate cancer analysis. In this article, we propose convolutional neural network (CNN) as a tool to improve efficiency and accuracy of osteosarcoma tumor classification into tumor classes (viable tumor, necrosis) versus nontumor. The proposed CNN architecture contains eight learned layers: three sets of stacked two convolutional layers interspersed with max pooling layers for feature extraction and two fully connected layers with data augmentation strategies to boost performance. The use of a neural network results in higher accuracy of average 92% for the classification. We compare the proposed architecture with three existing and proven CNN architectures for image classification: AlexNet, LeNet, and VGGNet. We also provide a pipeline to calculate percentage necrosis in a given whole slide image. We conclude that the use of neural networks can assure both high accuracy and efficiency in osteosarcoma classification.},
  doi       = {10.1089/cmb.2017.0153},
  issn      = {10665277},
  keywords  = {convolutional neural network,histology image analysis,osteosarcoma},
  pmid      = {29083930},
}

@InProceedings{art/ArunachalamH_2017,
  author    = {Arunachalam, Harish Babu and Mishra, Rashika and Armaselu, Bogdan and Daescu, Ovidiu and Martinez, Maria and Leavey, Patrick and Rakheja, Dinesh and Cederberg, Kevin and Sengupta, Anita and Ni'Suilleabhain, Molly},
  booktitle = {Pacific Symposium on Biocomputing},
  title     = {{Computer aided image segmentation and classification for viable and non-viable tumor identification in osteosarcoma}},
  year      = {2017},
  number    = {212679},
  pages     = {195--206},
  volume    = {0},
  abstract  = {Osteosarcoma is one of the most common types of bone cancer in children. To gauge the extent of cancer treatment response in the patient after surgical resection, the H&E stained image slides are manually evaluated by pathologists to estimate the percentage of necrosis, a time consuming process prone to observer bias and inaccuracy. Digital image analysis is a potential method to automate this process, thus saving time and providing a more accurate evaluation. The slides are scanned in Aperio Scanscope, converted to digital Whole Slide Images (WSIs) and stored in SVS format. These are high resolution images, of the order of 10 9 pixels, allowing up to 40X magnification factor. This paper proposes an image segmentation and analysis technique for segmenting tumor and non-tumor regions in histopathological WSIs of osteosarcoma datasets. Our approach is a combination of pixel-based and object-based methods which utilize tumor properties such as nuclei cluster, density, and circularity to classify tumor regions as viable and non-viable. A K-Means clustering technique is used for tumor isolation using color normalization, followed by multi-threshold Otsu segmentation technique to further classify tumor region as viable and non-viable. Then a Flood-fill algorithm is applied to cluster similar pixels into cellular objects and compute cluster data for further analysis of regions under study. To the best of our knowledge this is the first comprehensive solution that is able to produce such a classification for Osteosarcoma cancer. The results are very conclusive in identifying viable and non-viable tumor regions. In our experiments, the accuracy of the discussed approach is 100% in viable tumor and coagulative necrosis identification while it is around 90% for fibrosis and acellular/hypocellular tumor osteoid, for all the sampled datasets used. We expect the developed software to lead to a significant increase in accuracy and decrease in inter-observer variability in assessment of necrosis by the pathologists and a reduction in the time spent by the pathologists in such assessments.},
  doi       = {10.1142/9789813207813_0020},
  issn      = {23356936},
  keywords  = {Image segmentation,Osteosarcoma,Otsu thresholding,SVS image analysis},
  pmid      = {27896975},
}

@Article{art/LeaveyP_2017,
  author   = {P., Leavey and H.B., Arunachalam and B., Armaselu and A., Sengupta and D., Rakheja and S., Skapek and K., Cederberg and J.-P., Bach and S., Glick and M., Ni'Suilleabhain and R., Mishra and M., Martinez and R., Ziraldo and D., Leonard},
  journal  = {Pediatric blood & cancer},
  title    = {{American Society of Pediatric Hematology/Oncology (ASPHO) Palais des congr{\'{e}}s de Montr{\'{e}}al Montr{\'{e}}al, Canada April 26-29, 2017}},
  year     = {2017},
  issn     = {15455017},
  volume   = {64},
  abstract = {Background: In light of failed efforts to intensify treatment based on tumor necrosis after 10- weeks of chemotherapy, a novel biomarker to predict or measure response is critically needed to develop new therapies for patients with osteosarcoma. Objectives:We hypothesize that such a biomarker can be discovered by combining the emerging virtual microscopy technology of whole-slide digital imaging with computer-based image pattern recognition algorithms. Initial development of the biomarker would be based on histological features of tumor response. Design/Method: We retrieved data from resection specimens of 50 osteosarcoma patients treated at Children'sMedical Center Dallas between 1995-2015.We implemented a multi-step process to develop an automated image analysis protocol for histological features that involved digitalization of all slides, development of an image visualization and navigation tool, and an image segmentation and processing module for extracting major features within an image. Results: Tumor preparation: all resected tumors were prepared for histological evaluation with standard procedures that are briefly summarized. After surgical resection, each patient tumor was bisected in the plane predicted to provide the largest surface area for histology slide preparation. Using a pre-determined grid, each area within the grid was harvested to produce one slide (20-50 slides/patient). Using an Aperio scanner, we digitalized all histology slides for each patient, within which digitalized slides retain histological details from 2x to 20x magnification (each 20x image = 1 tile). Since each digitalized whole slide has approximately 4000 tiles, each patient whole tumor is represented by approximately 200,000 tiles.Main image processing algorithm development: Two-hundred image tiles were specifically identified from 3 patients' cases, to represent the main tumor features. An image segmentation pipeline was then designed and implemented to differentiate tumor from non-tumor and viable tumor from nonviable tumor based on color, shape, and cellular density characteristics. Implementing Color segmentation for chromatically distinct regions, and threshold segmentation using Multi-Otsu with clustering and contouring of segmented images for shape and density analysis, we completed the initial image analysis protocol. Using a flood-fill algorithm applied to pixels, cell counts, and cell density, we identified viable tumor and coagulative necrosis with 100% accuracy, fibrosis with 93% accuracy, and tumor osteoid with 89% accuracy. We then identified 2,500 tiles based on a stratified random sample approach to train and validate a set of machine learning algorithms. These efforts are ongoing. Conclusion: We have completed the first phase necessary to automate the interpretation of histological features of tumor response in osteosarcoma.},
  doi      = {10.1002/pbc.26591},
  keywords = {*biological marker,*osteosarcoma,*pattern recognition,*tumor necrosis,cell count,cell density,clinical article,digital imaging,female,fibrosis,histology,human,human tissue,image analysis,image processing,image segmentation,machine learning,male,microscopy,pipeline,random sample,surgery},
  pmid     = {28371237},
}

@InProceedings{art/MishraR_2017,
  author    = {Mishra, Rashika and Daescu, Ovidiu and Leavey, Patrick and Rakheja, Dinesh and Sengupta, Anita},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Histopathological diagnosis for viable and non-viable tumor prediction for osteosarcoma using convolutional neural network}},
  year      = {2017},
  pages     = {12--23},
  volume    = {10330 LNBI},
  abstract  = {Pathologists often deal with high complexity and sometimes disagreement over Osteosarcoma tumor classification due to cellular heterogeneity in the dataset. Segmentation and classification of histology tissue in H&E stained tumor image datasets is challenging due to intra-class variations and inter-class similarity, crowded context, and noisy data. In recent years, deep learning approaches have led to encouraging results in breast cancer and prostate cancer analysis. In this paper, we propose a Convolutional neural network (CNN) as a tool to improve efficiency and accuracy of Osteosarcoma tumor classification into tumor classes (viable tumor, necrosis) vs non-tumor. The proposed CNN architecture contains five learned layers: three convolutional layers interspersed with max pooling layers for feature extraction and two fully-connected layers with data augmentation strategies to boost performance. We conclude that the use of neural network can assure high accuracy and efficiency in Osteosarcoma classification.},
  doi       = {10.1007/978-3-319-59575-7_2},
  isbn      = {9783319595740},
  issn      = {16113349},
  keywords  = {Convolutional neural network,Histology image analysis,Osteosarcoma},
}

@Misc{art/YorkeA_2019,
  author    = {Yorke, Afua A. and McDonald, Gary C. and {Solis Jr.}, David and Guerrero., Thomas},
  title     = {{Pelvic Reference Data}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.woskq5oo},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/tQGJAw},
}

@Misc{art/LeaveyP.S_2019,
  author    = {{Leavey P.}, Sengupta A Rakheja D Daescu O Arunachalam H B & Mishra R},
  title     = {{Osteosarcoma data from UT Southwestern/UT Dallas for Viable and Necrotic Tumor Assessment}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.BVHJHDAS},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/xwElAw},
}

@Article{art/MatekC_201911,
  author   = {Matek, Christian and Schwarz, Simone and Spiekermann, Karsten and Marr, Carsten},
  journal  = {Nature Machine Intelligence},
  title    = {{Human-level recognition of blast cells in acute myeloid leukaemia with convolutional neural networks}},
  year     = {2019},
  issn     = {2522-5839},
  month    = {nov},
  number   = {11},
  pages    = {538--544},
  volume   = {1},
  abstract = {Reliable recognition of malignant white blood cells is a key step in the diagnosis of hematologic malignancies such as Acute Myeloid Leukemia. Microscopic morphological examination of blood cells is usually performed by trained human examiners, making the process tedious, time-consuming and hard to standardise. We compile an annotated image dataset of over 18,000 white blood cells, use it to train a convolutional neural network for leukocyte classification, and evaluate the network's performance. The network classifies the most important cell types with high accuracy. It also allows us to decide two clinically relevant questions with human-level performance, namely (i) if a given cell has blast character, and (ii) if it belongs to the cell types normally present in non-pathological blood smears. Our approach holds the potential to be used as a classification aid for examining much larger numbers of cells in a smear than can usually be done by a human expert. This will allow clinicians to recognize malignant cell populations with lower prevalence at an earlier stage of the disease.},
  doi      = {10.1038/s42256-019-0101-9},
  url      = {http://www.nature.com/articles/s42256-019-0101-9},
}

@Misc{art/MouryaS_2019,
  author    = {Mourya, Simmi and Kant, Sonaal and Kumar, Pulkit and Gupta, Anubha and Gupta, Rita},
  title     = {{ALL Challenge dataset of ISBI 2019}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.DC64I46R},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/zwYlAw},
}

@Book{art/PanY_2019,
  author    = {Pan, Yongsheng and Liu, Mingxia and Xia, Yong and Shen, Dinggang},
  editor    = {Gupta, Anubha and Gupta, Ritu},
  publisher = {Springer Singapore},
  title     = {{Neighborhood-Correction Algorithm for Cells}},
  year      = {2019},
  address   = {Singapore},
  isbn      = {9789811507984},
  series    = {Lecture Notes in Bioengineering},
  doi       = {10.1007/978-981-15-0798-4},
  keywords  = {Microscopic image classification,Fisher vector,Res,b,b-lymphoblast cells,development institute of northwestern,fisher vector,leukemia,microscopic image classification,pan,polytechnical university,research,residual network,xia,y},
  url       = {http://dx.doi.org/10.1007/978-981-15-0798-4_8},
}

@Article{art/GehlotS_202004,
  author        = {Gehlot, Shiv and Gupta, Anubha and Gupta, Ritu},
  journal       = {Medical Image Analysis},
  title         = {{SDCT-AuxNet$\theta$: DCT augmented stain deconvolutional CNN with auxiliary classifier for cancer diagnosis}},
  year          = {2020},
  issn          = {13618423},
  month         = {apr},
  pages         = {101661},
  volume        = {61},
  abstract      = {Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell cancer across the globe. With the popularity of convolutional neural networks (CNNs), computer-aided diagnosis of cancer has attracted considerable attention. Such tools are easily deployable and are cost-effective. Hence, these can enable extensive coverage of cancer diagnostic facilities. However, the development of such a tool for ALL cancer was challenging so far due to the non-availability of a large training dataset. The visual similarity between the malignant and normal cells adds to the complexity of the problem. This paper discusses the recent release of a large dataset and presents a novel deep learning architecture for the classification of cell images of ALL cancer. The proposed architecture, namely, SDCT-AuxNet$\theta$ is a 2-module framework that utilizes a compact CNN as the main classifier in one module and a Kernel SVM as the auxiliary classifier in the other one. While CNN classifier uses features through bilinear-pooling, spectral-averaged features are used by the auxiliary classifier. Further, this CNN is trained on the stain deconvolved quantity images in the optical density domain instead of the conventional RGB images. A novel test strategy is proposed that exploits both the classifiers for decision making using the confidence scores of their predicted class labels. Elaborate experiments have been carried out on our recently released public dataset of 15114 images of ALL cancer and healthy cells to establish the validity of the proposed methodology that is also robust to subject-level variability. A weighted F1 score of 94.8% is obtained that is best so far on this challenging dataset.},
  archiveprefix = {arXiv},
  arxivid       = {2006.00304},
  doi           = {10.1016/j.media.2020.101661},
  eprint        = {2006.00304},
  keywords      = {ALL diagnosis,Acute lymphoblastic leukemia,Cell classification,Convolutional neural network,Deep learning},
  pmid          = {32066066},
  url           = {https://linkinghub.elsevier.com/retrieve/pii/S136184152030027X},
}

@Article{art/GoswamiS_202003,
  author        = {Goswami, Shubham and Mehta, Suril and Sahrawat, Dhruva and Gupta, Anubha and Gupta, Ritu},
  journal       = {arXiv},
  title         = {{Heterogeneity loss to handle intersubject and intrasubject variability in cancer}},
  year          = {2020},
  issn          = {23318422},
  month         = {mar},
  abstract      = {Developing nations lack adequate number of hospitals with modern equipment and skilled doctors. Hence, a significant proportion of these nations' population, particularly in rural areas, is not able to avail specialized and timely healthcare facilities. In recent years, deep learning (DL) models, a class of artificial intelligence (AI) methods, have shown impressive results in medical domain. These AI methods can provide immense support to developing nations as affordable healthcare solutions. This work is focused on one such application of blood cancer diagnosis. However, there are some challenges to DL models in cancer research because of the unavailability of a large data for adequate training and the difficulty of capturing heterogeneity in data at different levels ranging from acquisition characteristics, session, to subject-level (within subjects and across subjects). These challenges render DL models prone to overfitting and hence, models lack generalization on prospective subjects' data. In this work, we address these problems in the application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep learning. We propose heterogeneity loss that captures subject-level heterogeneity, thereby, forcing the neural network to learn subject-independent features. We also propose an unorthodox ensemble strategy that helps us in providing improved classification over models trained on 7-folds giving a weighted-F1 score of 95.26% on unseen (test) subjects' data that are, so far, the best results on the C-NMC 2019 dataset for B-ALL classification.},
  archiveprefix = {arXiv},
  arxivid       = {2003.03295},
  eprint        = {2003.03295},
  url           = {http://arxiv.org/abs/2003.03295},
}

@Misc{art/MatekC_2019,
  author    = {Matek, Christian and Schwarz, Simone and Marr, Carsten and Spiekermann, Karsten},
  title     = {{A Single-cell Morphological Dataset of Leukocytes from AML Patients and Non-malignant Controls [Data set]}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/tcia.2019.36f5o9ld},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/fgWkAw},
}

@Article{art/BorovecJ_2020,
  author   = {Borovec, Jiri and Kybic, Jan and Arganda-Carreras, Ignacio and Sorokin, Dmitry V. and Bueno, Gloria and Khvostikov, Alexander V. and Bakas, Spyridon and Chang, Eric I.Chao and Heldmann, Stefan and Kartasalo, Kimmo and Latonen, Leena and Lotz, Johannes and Noga, Michelle and Pati, Sarthak and Punithakumar, Kumaradevan and Ruusuvuori, Pekka and Skalski, Andrzej and Tahmasebi, Nazanin and Valkonen, Masi and Venet, Ludovic and Wang, Yizhe and Weiss, Nick and Wodzinski, Marek and Xiang, Yu and Xu, Yan and Yan, Yan and Yushkevich, Paul and Zhao, Shengyu and Munoz-Barrutia, Arrate},
  journal  = {IEEE transactions on medical imaging},
  title    = {{ANHIR: Automatic Non-Rigid Histological Image Registration Challenge}},
  year     = {2020},
  issn     = {1558254X},
  number   = {10},
  pages    = {3042--3052},
  volume   = {39},
  abstract = {Automatic Non-rigid Histological Image Registration (ANHIR) challenge was organized to compare the performance of image registration algorithms on several kinds of microscopy histology images in a fair and independent manner. We have assembled 8 datasets, containing 355 images with 18 different stains, resulting in 481 image pairs to be registered. Registration accuracy was evaluated using manually placed landmarks. In total, 256 teams registered for the challenge, 10 submitted the results, and 6 participated in the workshop. Here, we present the results of 7 well-performing methods from the challenge together with 6 well-known existing methods. The best methods used coarse but robust initial alignment, followed by non-rigid registration, used multiresolution, and were carefully tuned for the data at hand. They outperformed off-the-shelf methods, mostly by being more robust. The best methods could successfully register over 98% of all landmarks and their mean landmark registration accuracy (TRE) was 0.44% of the image diagonal. The challenge remains open to submissions and all images are available for download.},
  doi      = {10.1109/TMI.2020.2986331},
  pmid     = {32275587},
}

@Article{art/VeelingB_201806,
  author        = {Veeling, Bastiaan S. and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Rotation equivariant CNNs for digital pathology}},
  year          = {2018},
  issn          = {16113349},
  month         = {jun},
  pages         = {210--218},
  volume        = {11071 LNCS},
  abstract      = {We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.},
  archiveprefix = {arXiv},
  arxivid       = {1806.03962},
  doi           = {10.1007/978-3-030-00934-2_24},
  eprint        = {1806.03962},
  isbn          = {9783030009335},
  url           = {http://arxiv.org/abs/1806.03962},
}

@Article{art/BorovecJ_201912,
  author        = {Borovec, Jiri},
  journal       = {arXiv},
  title         = {{Birl: Benchmark on Image Registration Methods With Landmark Validation}},
  year          = {2019},
  issn          = {23318422},
  month         = {dec},
  abstract      = {This report presents a generic image registration benchmark with automatic evaluation using landmark annotations. The key features of the BIRL framework are: easily extendable, performance evaluation, parallel experimentation, simple visualisations, experiment's time-out limit, resuming unfinished experiments. From the research practice, we identified and focused on these two main use-cases: (a) comparison of user's (newly developed) method with some State-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA methods on user's custom dataset (which should contain landmark annotation).
Moreover, we present an integration of several standard image registration methods aiming at biomedical imaging into the BIRL framework. This report also contains experimental results of these SOTA methods on the CIMA dataset, which is a dataset of Whole Slice Imaging (WSI) from histology/pathology containing several multi-stain tissue samples from three tissue kinds.
Source and results: https://borda.github.io/BIRL},
  archiveprefix = {arXiv},
  arxivid       = {1912.13452},
  eprint        = {1912.13452},
  keywords      = {Image registration {\textperiodcentered} Benchmark {\textperiodcentered} Landmark annotati},
  url           = {http://arxiv.org/abs/1912.13452},
}

@Article{art/CampanellaG_2019,
  author   = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and {Werneck Krauss Silva}, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
  journal  = {Nature Medicine},
  title    = {{Clinical-grade computational pathology using weakly supervised deep learning on whole slide images}},
  year     = {2019},
  issn     = {1546170X},
  number   = {8},
  pages    = {1301--1309},
  volume   = {25},
  abstract = {The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65–75% of slides while retaining 100% sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.},
  doi      = {10.1038/s41591-019-0508-1},
  pmid     = {31308507},
}

@Article{art/LiZ_201803,
  author        = {Li, Zhang and Hu, Zheyu and Xu, Jiaolong and Tan, Tao and Chen, Hui and Duan, Zhi and Liu, Ping and Tang, Jun and Cai, Guoping and Ouyang, Quchang and Tang, Yuling and Litjens, Geert and Li, Qiang},
  journal       = {arXiv},
  title         = {{Title: Computer-aided diagnosis of lung carcinoma using deep learning – a pilot study}},
  year          = {2018},
  issn          = {23318422},
  month         = {mar},
  abstract      = {Aim: Early detection and correct diagnosis of lung cancer are the most important steps in improving patient outcome. This study aims to assess which deep learning models perform best in lung cancer diagnosis. Methods: Non-small cell lung carcinoma and small cell lung carcinoma biopsy specimens were consecutively obtained and stained. The specimen slides were diagnosed by two experienced pathologists (over 20 years). Several deep learning models were trained to discriminate cancer and non-cancer biopsies. Result: Deep learning models give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning analysis could help to speed up the detection process for the whole-slide image (WSI) and keep the comparable detection rate with human observer.},
  archiveprefix = {arXiv},
  arxivid       = {1803.05471},
  eprint        = {1803.05471},
  keywords      = {Artificial intelligence,Classification,Convolutional Neural Networks,Deep learning,Diagnosis,Lung cancer},
  url           = {http://arxiv.org/abs/1803.05471},
}

@Misc{art/NatashaHonomichl_2019,
  author    = {{Natasha Honomichl}},
  title     = {{SN-AM Dataset: White Blood cancer dataset of B-ALL and MM for stain normalization}},
  year      = {2019},
  booktitle = {Wiki Cancer Image archive},
  doi       = {10.7937/TCIA.2019.OF2W8LXR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/EQIlAw},
}

@Misc{art/CampanellaG_2019a,
  author    = {Campanella, Gabriele and Hanna, Matthew G and Brogi, Edi and Fuchs, Thomas J},
  title     = {{Breast Metastases to Axillary Lymph Nodes}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.3XBN2JCC},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/yxolAw},
}

@InProceedings{art/DuggalR_2016,
  author    = {Duggal, Rahul and Gupta, Anubha and Gupta, Ritu and Wadhwa, Manya and Ahuja, Chirag},
  booktitle = {ACM International Conference Proceeding Series},
  title     = {{Overlapping cell nuclei segmentation in microscopic images using deep belief networks}},
  year      = {2016},
  abstract  = {This paper proposes a method for segmentation of nuclei of single/isolated and overlapping/touching immature white blood cells from microscopic images of B-Lineage acute lymphoblastic leukemia (ALL) prepared from peripheral blood and bone marrow aspirate. We propose deep belief network approach for the segmentation of these nuclei. Simulation results and comparison with some of the existing methods demonstrate the efficacy of the proposed method.},
  doi       = {10.1145/3009977.3010043},
  isbn      = {9781450347532},
  keywords  = {B-ALL,Deep belief network,Machine learning,Overlapping nuclei segmentation},
}

@Article{art/DuggalR_2017,
  author   = {Duggal, Rahul and Gupta, Anubha and Gupta, Ritu and Mallick, Pramit},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title    = {{SD-Layer: Stain deconvolutional layer for CNNs in medical microscopic imaging}},
  year     = {2017},
  issn     = {16113349},
  pages    = {435--443},
  volume   = {10435 LNCS},
  abstract = {Convolutional Neural Networks (CNNs) are typically trained in the RGB color space. However, in medical imaging, we believe that pixel stain quantities offer a fundamental view of the interaction between tissues and stain chemicals. Since the optical density (OD) colorspace allows to compute pixel stain quantities from pixel RGB intensities using the Beer-Lambert's law, we propose a stain deconvolutional layer, hereby named as SD-Layer, affixed at the front of CNN that performs two functions: (1) it transforms the input RGB microscopic images to Optical Density (OD) space and (2) this layer deconvolves OD image with the stain basis learned through backpropagation and provides tissue-specific stain absorption quantities as input to the following CNN layers. With the introduction of only nine additional learnable parameters in the proposed SD-Layer, we obtain a considerably improved performance on two standard CNN architectures: AlexNet and T-CNN. Using the T-CNN architecture prefixed with the proposed SD-Layer, we obtain 5-fold cross-validation accuracy of 93.2% in the problem of differentiating malignant immature White Blood Cells (WBCs) from normal immature WBCs for cancer detection.},
  doi      = {10.1007/978-3-319-66179-7_50},
  isbn     = {9783319661780},
  keywords = {Cancer imaging,Classification,Deep learning,Stain deconvolution},
}

@InProceedings{art/SunW_2016,
  author    = {Sun, Wenqing and Zheng, Bin and Qian, Wei},
  booktitle = {Medical Imaging 2016: Computer-Aided Diagnosis},
  title     = {{Computer aided lung cancer diagnosis with deep learning algorithms}},
  year      = {2016},
  pages     = {97850Z},
  volume    = {9785},
  abstract  = {Deep learning is considered as a popular and powerful method in pattern recognition and classification. However, there are not many deep structured applications used in medical imaging diagnosis area, because large dataset is not always available for medical images. In this study we tested the feasibility of using deep learning algorithms for lung cancer diagnosis with the cases from Lung Image Database Consortium (LIDC) database. The nodules on each computed tomography (CT) slice were segmented according to marks provided by the radiologists. After down sampling and rotating we acquired 174412 samples with 52 by 52 pixel each and the corresponding truth files. Three deep learning algorithms were designed and implemented, including Convolutional Neural Network (CNN), Deep Belief Networks (DBNs), Stacked Denoising Autoencoder (SDAE). To compare the performance of deep learning algorithms with traditional computer aided diagnosis (CADx) system, we designed a scheme with 28 image features and support vector machine. The accuracies of CNN, DBNs, and SDAE are 0.7976, 0.8119, and 0.7929, respectively; the accuracy of our designed traditional CADx is 0.7940, which is slightly lower than CNN and DBNs. We also noticed that the mislabeled nodules using DBNs are 4% larger than using traditional CADx, this might be resulting from down sampling process lost some size information of the nodules.},
  doi       = {10.1117/12.2216307},
  isbn      = {9781510600201},
  issn      = {16057422},
}

@Article{art/ChenouardN_201403,
  author   = {Chenouard, Nicolas and Smal, Ihor and {De Chaumont}, Fabrice and Ma{\v{s}}ka, Martin and Sbalzarini, Ivo F. and Gong, Yuanhao and Cardinale, Janick and Carthel, Craig and Coraluppi, Stefano and Winter, Mark and Cohen, Andrew R. and Godinez, William J. and Rohr, Karl and Kalaidzidis, Yannis and Liang, Liang and Duncan, James and Shen, Hongying and Xu, Yingke and Magnusson, Klas E.G. and Jald{\'{e}}n, Joakim and Blau, Helen M. and Paul-Gilloteaux, Perrine and Roudot, Philippe and Kervrann, Charles and Waharte, Fran{\c{c}}ois and Tinevez, Jean Yves and Shorte, Spencer L. and Willemse, Joost and Celler, Katherine and {Van Wezel}, Gilles P. and Dan, Han Wei and Tsai, Yuh Show and {De Sol{\'{o}}rzano}, Carlos Ortiz and Olivo-Marin, Jean Christophe and Meijering, Erik},
  journal  = {Nature Methods},
  title    = {{Objective comparison of particle tracking methods}},
  year     = {2014},
  issn     = {15487091},
  month    = {mar},
  number   = {3},
  pages    = {281--289},
  volume   = {11},
  abstract = {Particle tracking is of key importance for quantitative analysis of intracellular dynamic processes from time-lapse microscopy image data. Because manually detecting and following large numbers of individual particles is not feasible, automated computational methods have been developed for these tasks by many groups. Aiming to perform an objective comparison of methods, we gathered the community and organized an open competition in which participating teams applied their own methods independently to a commonly defined data set including diverse scenarios. Performance was assessed using commonly defined measures. Although no single method performed best across all scenarios, the results revealed clear differences between the various approaches, leading to notable practical conclusions for users and developers. {\textcopyright} 2014 Nature America, Inc.},
  doi      = {10.1038/nmeth.2808},
  pmid     = {24441936},
  url      = {http://www.nature.com/articles/nmeth.2808},
}

@Article{art/LiJ_201907,
  author        = {Li, Jiahui and Yang, Shuang and Huang, Xiaodi and Da, Qian and Yang, Xiaoqun and Hu, Zhiqiang and Duan, Qi and Wang, Chaofu and Li, Hongsheng},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Signet Ring Cell Detection with a Semi-supervised Learning Framework}},
  year          = {2019},
  issn          = {16113349},
  month         = {jul},
  pages         = {842--854},
  volume        = {11492 LNCS},
  abstract      = {Signet ring cell carcinoma is a type of rare adenocarcinoma with poor prognosis. Early detection leads to huge improvement of patients' survival rate. However, pathologists can only visually detect signet ring cells under the microscope. This procedure is not only laborious but also prone to omission. An automatic and accurate signet ring cell detection solution is thus important but has not been investigated before. In this paper, we take the first step to present a semi-supervised learning framework for the signet ring cell detection problem. Self-training is proposed to deal with the challenge of incomplete annotations, and cooperative-training is adapted to explore the unlabeled regions. Combining the two techniques, our semi-supervised learning framework can make better use of both labeled and unlabeled data. Experiments on large real clinical data demonstrate the effectiveness of our design. Our framework achieves accurate signet ring cell detection and can be readily applied in the clinical trails. The dataset will be released soon to facilitate the development of the area.},
  archiveprefix = {arXiv},
  arxivid       = {1907.03954},
  doi           = {10.1007/978-3-030-20351-1_66},
  eprint        = {1907.03954},
  isbn          = {9783030203504},
  keywords      = {Semi-supervised learning,Signet ring cell benchmark,Signet ring cell detection},
  url           = {http://arxiv.org/abs/1907.03954},
}

@Article{art/UlmanV_201712,
  author   = {Ulman, Vladim{\'{i}}r and Ma{\v{s}}ka, Martin and Magnusson, Klas E.G. and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and Smal, Ihor and Rohr, Karl and Jald{\'{e}}n, Joakim and Blau, Helen M. and Dzyubachyk, Oleh and Lelieveldt, Boudewijn and Xiao, Pengdong and Li, Yuexiang and Cho, Siu Yeung and Dufour, Alexandre C. and Olivo-Marin, Jean Christophe and Reyes-Aldasoro, Constantino C. and Solis-Lemus, Jose A. and Bensch, Robert and Brox, Thomas and Stegmaier, Johannes and Mikut, Ralf and Wolf, Steffen and Hamprecht, Fred A. and Esteves, Tiago and Quelhas, Pedro and Demirel, {\"{O}}mer and Malmstr{\"{o}}m, Lars and Jug, Florian and Tomancak, Pavel and Meijering, Erik and Mu{\~{n}}oz-Barrutia, Arrate and Kozubek, Michal and Ortiz-De-Solorzano, Carlos},
  journal  = {Nature Methods},
  title    = {{An objective comparison of cell-tracking algorithms}},
  year     = {2017},
  issn     = {15487105},
  month    = {dec},
  number   = {12},
  pages    = {1141--1152},
  volume   = {14},
  abstract = {We present a combined report on the results of three editions of the Cell Tracking Challenge, an ongoing initiative aimed at promoting the development and objective evaluation of cell segmentation and tracking algorithms. With 21 participating algorithms and a data repository consisting of 13 data sets from various microscopy modalities, the challenge displays today's state-of-the-art methodology in the field. We analyzed the challenge results using performance measures for segmentation and tracking that rank all participating methods. We also analyzed the performance of all of the algorithms in terms of biological measures and practical usability. Although some methods scored high in all technical aspects, none obtained fully correct solutions. We found that methods that either take prior information into account using learning strategies or analyze cells in a global spatiotemporal video context performed better than other methods under the segmentation and tracking scenarios included in the challenge.},
  doi      = {10.1038/nmeth.4473},
  pmid     = {29083403},
  url      = {http://www.nature.com/articles/nmeth.4473},
}

@Article{art/GuptaA_2018,
  author   = {Gupta, Anubha and Mallick, Pramit and Sharma, Ojaswa and Gupta, Ritu and Duggal, Rahul},
  journal  = {PLoS ONE},
  title    = {{PCSEG: Color model driven probabilistic multiphase level set based tool for plasma cell segmentation in multiple myeloma}},
  year     = {2018},
  issn     = {19326203},
  number   = {12},
  volume   = {13},
  abstract = {Plasma cell segmentation is the first stage of a computer assisted automated diagnostic tool for multiple myeloma (MM). Owing to large variability in biological cell types, a method for one cell type cannot be applied directly on the other cell types. In this paper, we present PCSeg Tool for plasma cell segmentation from microscopic medical images. These images were captured from bone marrow aspirate slides of patients with MM. PCSeg has a robust pipeline consisting of a pre-processing step, the proposed modified multiphase level set method followed by post-processing steps including the watershed and circular Hough transform to segment clusters of cells of interest and to remove unwanted cells. Our modified level set method utilizes prior information about the probability densities of regions of interest (ROIs) in the color spaces and provides a solution to the minimal-partition problem to segment ROIs in one of the level sets of a two-phase level set formulation. PCSeg tool is tested on a number of microscopic images and provides good segmentation results on single cells as well as efficient segmentation of plasma cell clusters.},
  doi      = {10.1371/journal.pone.0207908},
  pmid     = {30540767},
}

@Article{art/GuptaR_201702,
  author  = {Gupta, Ritu and Mallick, Pramit and Duggal, Rahul and Gupta, Anubha and Sharma, Ojaswa},
  journal = {Clinical Lymphoma Myeloma and Leukemia},
  title   = {{Stain Color Normalization and Segmentation of Plasma Cells in Microscopic Images as a Prelude to Development of Computer Assisted Automated Disease Diagnostic Tool in Multiple Myeloma}},
  year    = {2017},
  issn    = {21522650},
  month   = {feb},
  number  = {1},
  pages   = {e99},
  volume  = {17},
  doi     = {10.1016/j.clml.2017.03.178},
  url     = {https://linkinghub.elsevier.com/retrieve/pii/S2152265017304688},
}

@Misc{art/GuptaR_2019,
  author    = {Gupta, Ritu and Gupta, Anubha},
  title     = {{MiMM_SBILab Dataset: Microscopic Images of Multiple Myeloma}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.PNN6AYPL},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/-AElAw},
}

@Article{art/KarimR_201812,
  author   = {Karim, Rashed and Blake, Lauren Emma and Inoue, Jiro and Tao, Qian and Jia, Shuman and Housden, R. James and Bhagirath, Pranav and Duval, Jean Luc and Varela, Marta and Behar, Jonathan and Cadour, Loic and van der Geest, Rob J. and Cochet, Hubert and Drangova, Maria and Sermesant, Maxime and Razavi, Reza and Aslanidi, Oleg and Rajani, Ronak and Rhode, Kawal},
  journal  = {Medical Image Analysis},
  title    = {{Algorithms for left atrial wall segmentation and thickness – Evaluation on an open-source CT and MRI image database}},
  year     = {2018},
  issn     = {13618423},
  month    = {dec},
  pages    = {36--53},
  volume   = {50},
  abstract = {Structural changes to the wall of the left atrium are known to occur with conditions that predispose to Atrial fibrillation. Imaging studies have demonstrated that these changes may be detected non-invasively. An important indicator of this structural change is the wall's thickness. Present studies have commonly measured the wall thickness at few discrete locations. Dense measurements with computer algorithms may be possible on cardiac scans of Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). The task is challenging as the atrial wall is a thin tissue and the imaging resolution is a limiting factor. It is unclear how accurate algorithms may get and how they compare in this new emerging area. We approached this problem of comparability with the Segmentation of Left Atrial Wall for Thickness (SLAWT) challenge organised in conjunction with MICCAI 2016 conference. This manuscript presents the algorithms that had participated and evaluation strategies for comparing them on the challenge image database that is now open-source. The image database consisted of cardiac CT (n=10) and MRI (n=10) of healthy and diseased subjects. A total of 6 algorithms were evaluated with different metrics, with 3 algorithms in each modality. Segmentation of the wall with algorithms was found to be feasible in both modalities. There was generally a lack of accuracy in the algorithms and inter-rater differences showed that algorithms could do better. Benchmarks were determined and algorithms were ranked to allow future algorithms to be ranked alongside the state-of-the-art techniques presented in this work. A mean atlas was also constructed from both modalities to illustrate the variation in thickness within this small cohort.},
  doi      = {10.1016/j.media.2018.08.004},
  keywords = {Atrial fibrillation,Left atrial wall thickness,Left atrium,Myocardium},
  pmid     = {30208355},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518306431},
}

@Article{art/HeuvelT_201808,
  author    = {van den Heuvel, Thomas L. A. and de Bruijn, Dagmar and de Korte, Chris L. and van Ginneken, Bram},
  journal   = {PLOS ONE},
  title     = {{Automated measurement of fetal head circumference using 2D ultrasound images}},
  year      = {2018},
  issn      = {1932-6203},
  month     = {aug},
  number    = {8},
  pages     = {e0200412},
  volume    = {13},
  abstract  = {In this paper we present a computer aided detection (CAD) system for automated measurement of the fetal head circumference (HC) in 2D ultrasound images for all trimesters of the pregnancy. The HC can be used to estimate the gestational age and monitor growth of the fetus. Automated HC assessment could be valuable in developing countries, where there is a severe shortage of trained sonographers. The CAD system consists of two steps: First, Haar-like features were computed from the ultrasound images to train a random forest classifier to locate the fetal skull. Secondly, the HC was extracted using Hough transform, dynamic programming and an ellipse fit. The CAD system was trained on 999 images and validated on an independent test set of 335 images from all trimesters. The test set was manually annotated by an experienced sonographer and a medical researcher. The reference gestational age (GA) was estimated using the crown-rump length measurement (CRL). The mean difference between the reference GA and the GA estimated by the experienced sonographer was 0.8 ± 2.6, −0.0 ± 4.6 and 1.9 ± 11.0 days for the first, second and third trimester, respectively. The mean difference between the reference GA and the GA estimated by the medical researcher was 1.6 ± 2.7, 2.0 ± 4.8 and 3.9 ± 13.7 days. The mean difference between the reference GA and the GA estimated by the CAD system was 0.6 ± 4.3, 0.4 ± 4.7 and 2.5 ± 12.4 days. The results show that the CAD system performs comparable to an experienced sonographer. The presented system shows similar or superior results compared to systems published in literature. This is the first automated system for HC assessment evaluated on a large test set which contained data of all trimesters of the pregnancy.},
  doi       = {10.1371/journal.pone.0200412},
  editor    = {Reyes-Aldasoro, Constantino Carlos},
  isbn      = {1111111111},
  pmid      = {30138319},
  publisher = {Zenodo},
  url       = {https://zenodo.org/record/1322001 https://dx.plos.org/10.1371/journal.pone.0200412},
}

@Article{art/AliS_201905,
  author        = {Ali, Sharib and Zhou, Felix and Daul, Christian and Braden, Barbara and Bailey, Adam and Realdon, Stefano and East, James and Wagnieres, Georges and Loschenov, Victor and Grisan, Enrico and Blondel, Walter and Rittscher, Jens},
  journal       = {arXiv},
  title         = {{Endoscopy artifact detection (EAD 2019) challenge dataset}},
  year          = {2019},
  issn          = {23318422},
  month         = {may},
  abstract      = {Endoscopic artifacts are a core challenge in facilitating the diagnosis and treatment of diseases in hollow organs. Precise detection of specific artifacts like pixel saturations, motion blur, specular reections, bubbles and debris is essential for high-quality frame restoration and is crucial for realizing reliable computer-assisted tools for improved patient care. At present most videos in endoscopy are currently not analyzed due to the abundant presence of multiclass artifacts in video frames. Through the endoscopic artifact detection (EAD 2019) challenge, we address this key bottleneck problem by solving the accurate identification and localization of endoscopic frame artifacts to enable further key quantitative analysis of unusable video frames such as mosaicking and 3D reconstruction which is crucial for delivering improved patient care. This paper summarizes the challenge tasks and describes the dataset and evaluation criteria established in the EAD 2019 challenge.},
  archiveprefix = {arXiv},
  arxivid       = {1905.03209},
  doi           = {10.17632/C7FJBXCGJ9.1},
  eprint        = {1905.03209},
  keywords      = {Endoscopy,artifact detection,challenge,machine learning,semantic segmentation},
  url           = {http://arxiv.org/abs/1905.03209},
}

@Misc{art/MartelA_2019,
  author    = {Martel, Anne L. and Salama, Sherine and Nofech-Mozes, Sharon and Akbar, Shazia and Peikari, Mohammad},
  title     = {{Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology [Data set]}},
  year      = {2019},
  abstract  = {Breast cancer (BC) is the second most commonly diagnosed cancer in the U.S. with more than 250,000 new cases of invasive breast cancers reported in 2017. The majority of women with locally advanced and a subset of patients with operable breast cancer will undergo systemic therapy prior to their surgery (neoadjuvant therapy/ NAT) to reduce the size of tumor(s) and possibly further undergo breast conserving surgery. The Post-NAT-BRCA dataset is a collection of representative sections from breast resections in patients with residual invasive BC following NAT. Histologic sections were prepared and digitized to produce high resolution, microscopic images of treated BC tumors. Also included, are clinical features and expert pathology annotations of tumor cellularity and cell types. The Residual Cancer Burden Index (RCBi), is a clinically validated tool for assessment of response to NAT associated with prognosis. Tumor cellularity is one of the parameters used for calculating the RCBi. In this dataset, tumor cellularity refers to a measure of residual disease after NAT, in the form of proportion of malignant tumor inside the tumor bed region; also annotated. (See MD Anderson RCB Calculator for a detailed description of tumor cellularity.) Malignant, healthy, lymphocyte and other labels were also provided for individual cells to aid development of cell segmentation algorithms.},
  doi       = {10.7937/TCIA.2019.4YIBTJNO},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758117},
}

@Article{art/KirisliH_201312,
  author   = {Kirişli, H. A. and Schaap, M. and Metz, C. T. and Dharampal, A. S. and Meijboom, W. B. and Papadopoulou, S. L. and Dedic, A. and Nieman, K. and de Graaf, M. A. and Meijs, M. F.L. and Cramer, M. J. and Broersen, A. and Cetin, S. and Eslami, A. and Fl{\'{o}}rez-Valencia, L. and Lor, K. L. and Matuszewski, B. and Melki, I. and Mohr, B. and {\"{O}}ks{\"{u}}z, I. and Shahzad, R. and Wang, C. and Kitslaar, P. H. and Unal, G. and Katouzian, A. and Orkisz, M. and Chen, C. M. and Precioso, F. and Najman, L. and Masood, S. and {\"{U}}nay, D. and van Vliet, L. and Moreno, R. and Goldenberg, R. and Vu{\c{c}}ini, E. and Krestin, G. P. and Niessen, W. J. and {Van Walsum}, T.},
  journal  = {Medical Image Analysis},
  title    = {{Standardized evaluation framework for evaluating coronary artery stenosis detection, stenosis quantification and lumen segmentation algorithms in computed tomography angiography}},
  year     = {2013},
  issn     = {13618415},
  month    = {dec},
  number   = {8},
  pages    = {859--876},
  volume   = {17},
  abstract = {Though conventional coronary angiography (CCA) has been the standard of reference for diagnosing coronary artery disease in the past decades, computed tomography angiography (CTA) has rapidly emerged, and is nowadays widely used in clinical practice. Here, we introduce a standardized evaluation framework to reliably evaluate and compare the performance of the algorithms devised to detect and quantify the coronary artery stenoses, and to segment the coronary artery lumen in CTA data. The objective of this evaluation framework is to demonstrate the feasibility of dedicated algorithms to: (1) (semi-)automatically detect and quantify stenosis on CTA, in comparison with quantitative coronary angiography (QCA) and CTA consensus reading, and (2) (semi-)automatically segment the coronary lumen on CTA, in comparison with expert's manual annotation. A database consisting of 48 multicenter multivendor cardiac CTA datasets with corresponding reference standards are described and made available. The algorithms from 11 research groups were quantitatively evaluated and compared. The results show that (1) some of the current stenosis detection/quantification algorithms may be used for triage or as a second-reader in clinical practice, and that (2) automatic lumen segmentation is possible with a precision similar to that obtained by experts. The framework is open for new submissions through the website, at http://coronary.bigr.nl/stenoses/. {\textcopyright} 2013 Elsevier B.V.},
  doi      = {10.1016/j.media.2013.05.007},
  keywords = {Computed tomography angiography (CTA),Coronary arteries,Standardized evaluation framework,Stenoses detection,Stenoses quantification},
  pmid     = {23837963},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841513000819},
}

@Article{art/PeikariM_2017,
  author   = {Peikari, Mohammad and Salama, Sherine and Nofech-Mozes, Sharon and Martel, Anne L.},
  journal  = {Cytometry Part A},
  title    = {{Automatic cellularity assessment from post-treated breast surgical specimens}},
  year     = {2017},
  issn     = {15524930},
  number   = {11},
  pages    = {1078--1087},
  volume   = {91},
  abstract = {Neoadjuvant treatment (NAT) of breast cancer (BCa) is an option for patients with the locally advanced disease. It has been compared with standard adjuvant therapy with the aim of improving prognosis and surgical outcome. Moreover, the response of the tumor to the therapy provides useful information for patient management. The pathological examination of the tissue sections after surgery is the gold-standard to estimate the residual tumor and the assessment of cellularity is an important component of tumor burden assessment. In the current clinical practice, tumor cellularity is manually estimated by pathologists on hematoxylin and eosin (H&E) stained slides, the quality, and reliability of which might be impaired by inter-observer variability which potentially affects prognostic power assessment in NAT trials. This procedure is also qualitative and time-consuming. In this paper, we describe a method of automatically assessing cellularity. A pipeline to automatically segment nuclei figures and estimate residual cancer cellularity from within patches and whole slide images (WSIs) of BCa was developed. We have compared the performance of our proposed pipeline in estimating residual cancer cellularity with that of two expert pathologists. We found an intra-class agreement coefficient (ICC) of 0.89 (95% CI of [0.70, 0.95]) between pathologists, 0.74 (95% CI of [0.70, 0.77]) between pathologist #1 and proposed method, and 0.75 (95% CI of [0.71, 0.79]) between pathologist #2 and proposed method. We have also successfully applied our proposed technique on a WSI to locate areas with high concentration of residual cancer. The main advantage of our approach is that it is fully automatic and can be used to find areas with high cellularity in WSIs. This provides a first step in developing an automatic technique for post-NAT tumor response assessment from pathology slides. {\textcopyright} 2017 International Society for Advancement of Cytometry.},
  doi      = {10.1002/cyto.a.23244},
  keywords = {breast cancer,machine learning,neoadjuvant therapy,pathology image analysis},
  pmid     = {28976721},
}

@Misc{art/BlochB_2015,
  author    = {Bloch, B Nicolas and Jain, Ashali and conrade carl Jaffe},
  title     = {{Data From BREAST-DIAGNOSIS}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.SDNRQXXR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/JQAo},
}

@Misc{art/GavrielidesM_2015,
  author    = {Gavrielides, Marios A and Kinnard, Lisa M and Myers, Kyle J and Peregoy, Jenifer and Pritchard, William F and Zeng, Rongping and Esparza, Juan and Karanian, John and Petrick, Nicholas},
  title     = {{Data From Phantom_FDA}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.ORBJKMUX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/CAA9},
}

@Article{art/KostakogluL_201511,
  author   = {Kostakoglu, Lale and Duan, Fenghai and Idowu, Michael O. and Jolles, Paul R. and Bear, Harry D. and Muzi, Mark and Cormack, Jean and Muzi, John P. and Pryma, Daniel A. and Specht, Jennifer M. and Hovanessian-Larsen, Linda and Miliziano, John and Mallett, Sharon and Shields, Anthony F. and Mankoff, David A.},
  journal  = {Journal of Nuclear Medicine},
  title    = {{A phase II study of 3′-Deoxy-3′-18F-fluorothymidine PET in the assessment of early response of breast cancer to neoadjuvant chemotherapy: Results from ACRIN 6688}},
  year     = {2015},
  issn     = {2159662X},
  month    = {nov},
  number   = {11},
  pages    = {1681--1689},
  volume   = {56},
  abstract = {Our objective was to determine whether early change in standardized uptake values (SUVs) of 3′deoxy-3′-18F-fluorothymidine (18F-FLT) using PET with CT could predict pathologic complete response (pCR) of primary breast cancer to neoadjuvant chemotherapy (NAC). The key secondary objective was to correlate SUV with the proliferation marker Ki-67 at baseline and after NAC Methods: This prospective, multicenter phase II study did not specify the therapeutic regimen, thus, NAC varied among centers. Al evaluable patients underwent 18F-FLT PET/CT at baseline (FLT1) and after 1 cycle of NAC (FLT2); 43 patients were imaged at FLT1, FLT2, and after NAC completion (FLT3). The percentage change in maximum SUV (%DSUVmax) between FLT1 and FLT2 and FLT3 was calculated for the primary tumors. The predictive value of DSUVmax for pCR was determined using receiver-operating-characteristic curve analysis. The correlation between SUVmax and Ki-67 was also assessed. Results: Fifty-one of 90 recruited patients (median age, 54 y; stage IIA-IIIC) met the eligibility criteria for the primary objective analysis, with an additional 22 patients totaling 73 patients for secondary analyses. A pCR in the primary breast cancer was achieved in 9 of 51 patients. NAC resulted in a significant reduction in %SUVmax (mean D, 39%; 95% confidence interval, 31-46). There was a marginal difference in %DSUVmax-FLT1-FLT2 between pCR and no-pCR patient groups (Wilcoxon 1 -sided P = 0.050). The area under the curve for DSUVmax in the prediction of pCR was 0.68 (90% confidence interval, 0.50-0.83; Delong 1-sided P = 0.05), with slightly better predictive value for percentage mean SUV (P = 0.02) and similar prediction for peak SUV (P = 0.04). There was a weak correlation with pretherapy SUVmax and Ki-67 (r = 0.29, P = 0.04), but the correlation between SUVmax and Ki-67 after completion of NAC was stronger (r = 0.68, P < 0.0001). Conclusion: 18F-FLT PET imaging of breast cancer after 1 cycle of NAC weakly predicted pCR in the setting of variable NAC regimens. Posttherapy 18F-FLT uptake correlated with Ki-67 on surgical specimens. These results suggest some efficacy of 18F-FLT as an indicator of early therapeutic response of breast cancer to NAC and support future multicenter studies to test 18F-FLT PET in a more uniformly treated patient population.},
  doi      = {10.2967/jnumed.115.160663},
  keywords = {18 F-FLT PET,Breast cancer,Early treatment response,Neoadjuvant therapy},
  url      = {http://jnm.snmjournals.org/cgi/doi/10.2967/jnumed.115.160663},
}

@Article{art/GavrielidesM_201007,
  author   = {Gavrielides, Marios A. and Kinnard, Lisa M. and Myers, Kyle J. and Peregoy, Jennifer and Pritchard, William F. and Zeng, Rongping and Esparza, Juan and Karanian, John and Petrick, Nicholas},
  journal  = {Optics Express},
  title    = {{A resource for the assessment of lung nodule size estimation methods: database of thoracic CT scans of an anthropomorphic phantom}},
  year     = {2010},
  issn     = {1094-4087},
  month    = {jul},
  number   = {14},
  pages    = {15244},
  volume   = {18},
  abstract = {A number of interrelated factors can affect the precision and accuracy of lung nodule size estimation. To quantify the effect of these factors, we have been conducting phantom CT studies using an anthropomorphic thoracic phantom containing a vasculature insert to which synthetic nodules were inserted or attached. Ten repeat scans were acquired on different multi-detector scanners, using several sets of acquisition and reconstruction protocols and various nodule characteristics (size, shape, density, location). This study design enables both bias and variance analysis for the nodule size estimation task. The resulting database is in the process of becoming publicly available as a resource to facilitate the assessment of lung nodule size estimation methodologies and to enable comparisons between different methods regarding measurement error. This resource complements public databases of clinical data and will contribute towards the development of procedures that will maximize the utility of CT imaging for lung cancer screening and tumor therapy evaluation.},
  doi      = {10.1364/oe.18.015244},
  pmid     = {20640011},
  url      = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-18-14-15244},
}

@Misc{art/KinahanP_2017,
  author    = {Kinahan, Paul and Muzi, Mark and Bialecki, Brian and Coombs, Laura},
  title     = {{Data from ACRIN-FLT-Breast}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.OL20ZMXG},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/pAHUAQ},
}

@Misc{art/WeeL_2019,
  author    = {Wee, Leonard and Aerts, Hugo J W L and Kalendralis, Petros and Dekker, Andre},
  title     = {{Data from NSCLC-Radiomics-Interobserver1}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.CWVLPD26},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bgAlAw},
}

@Article{art/IrvinJ_201901,
  author        = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  journal       = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
  title         = {{CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison}},
  year          = {2019},
  issn          = {2159-5399},
  month         = {jan},
  pages         = {590--597},
  volume        = {33},
  abstract      = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.},
  archiveprefix = {arXiv},
  arxivid       = {1901.07031},
  doi           = {10.1609/aaai.v33i01.3301590},
  eprint        = {1901.07031},
  isbn          = {9781577358091},
  url           = {http://arxiv.org/abs/1901.07031},
}

@Article{art/ArmatoS_201506,
  author   = {Armato, Samuel G. and Hadjiiski, Lubomir and Tourassi, Georgia D. and Drukker, Karen and Giger, Maryellen L. and Li, Feng and Redmond, George and Farahani, Keyvan and Kirby, Justin S. and Clarke, Laurence P.},
  journal  = {Journal of Medical Imaging},
  title    = {{Guest Editorial: LUNGx Challenge for computerized lung nodule classification: reflections and lessons learned}},
  year     = {2015},
  issn     = {2329-4302},
  month    = {jun},
  number   = {2},
  pages    = {020103},
  volume   = {2},
  abstract = {Challenges, in the context of medical imaging, are valuable in that they allow for a direct comparison of different algorithms designed for a specific radiologic task, with all algorithms abiding by the same set of rules, operating on a common set of images, and being evaluated with a uniform performance assessment paradigm. The variability of system performance based on database composition and subtlety, definition of “truth,” and scoring metric is well-known;1–3 challenges serve to level the differences across these various dimensions. The medical imaging community has hosted a number of successful thoracic imaging challenges that have spanned a wide range of tasks,4,5 including lung nodule detection,6 lung nodule change, vessel segmentation,7 and vessel tree extraction.8 Each challenge presents its own unique set of circumstances and considerations; however, important common themes exist. Future challenge organizers (and participants) could benefit from an open discussion of successes achieved, pitfalls encountered, and lessons learned from each completed challenge.},
  doi      = {10.1117/1.jmi.2.2.020103},
  url      = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.2.2.020103},
}

@Article{art/ArmatoS_201612,
  author   = {Armato, Samuel G. and Drukker, Karen and Li, Feng and Hadjiiski, Lubomir and Tourassi, Georgia D. and Engelmann, Roger M. and Giger, Maryellen L. and Redmond, George and Farahani, Keyvan and Kirby, Justin S. and Clarke, Laurence P.},
  journal  = {Journal of Medical Imaging},
  title    = {{LUNGx Challenge for computerized lung nodule classification}},
  year     = {2016},
  issn     = {2329-4302},
  month    = {dec},
  number   = {4},
  pages    = {044506},
  volume   = {3},
  abstract = {{\textcopyright} 2016 Society of Photo-Optical Instrumentation Engineers (SPIE). The purpose of this work is to describe the LUNGx Challenge for the computerized classification of lung nodules on diagnostic computed tomography (CT) scans as benign or malignant and report the performance of participants' computerized methods along with that of six radiologists who participated in an observer study performing the same Challenge task on the same dataset. The Challenge provided sets of calibration and testing scans, established a performance assessment process, and created an infrastructure for case dissemination and result submission. Ten groups applied their own methods to 73 lung nodules (37 benign and 36 malignant) that were selected to achieve approximate size matching between the two cohorts. Area under the receiver operating characteristic curve (AUC) values for these methods ranged from 0.50 to 0.68; only three methods performed statistically better than random guessing. The radiologists' AUC values ranged from 0.70 to 0.85; three radiologists performed statistically better than the best-performing computer method. The LUNGx Challenge compared the performance of computerized methods in the task of differentiating benign from malignant lung nodules on CT scans, placed in the context of the performance of radiologists on the same task. The continued public availability of the Challenge cases will provide a valuable resource for the medical imaging research community.},
  doi      = {10.1117/1.jmi.3.4.044506},
  url      = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.3.4.044506},
}

@Article{art/GevaertO_201208,
  author   = {Gevaert, Olivier and Xu, Jiajing and Hoang, Chuong D. and Leung, Ann N. and Xu, Yue and Quon, Andrew and Rubin, Daniel L. and Napel, Sandy and Plevritis, Sylvia K.},
  journal  = {Radiology},
  title    = {{Non-small cell lung cancer: Identifying prognostic imaging biomarkers by leveraging public gene expression microarray data - Methods and preliminary results}},
  year     = {2012},
  issn     = {00338419},
  month    = {aug},
  number   = {2},
  pages    = {387--396},
  volume   = {264},
  abstract = {Purpose: To identify prognostic imaging biomarkers in non-small cell lung cancer (NSCLC) by means of a radiogenomics strategy that integrates gene expression and medical images in patients for whom survival outcomes are not available by leveraging survival data in public gene expression data sets. Materials and Methods: A radiogenomics strategy for associating image features with clusters of coexpressed genes (metagenes) was defined. First, a radiogenomics correlation map is created for a pairwise association between image features and metagenes. Next, predictive models of metagenes are built in terms of image features by using sparse linear regression. Similarly, predictive models of image features are built in terms of metagenes. Finally, the prognostic significance of the predicted image features are evaluated in a public gene expression data set with survival outcomes. This radiogenomics strategy was applied to a cohort of 26 patients with NSCLC for whom gene expression and 180 image features from computed tomography (CT) and positron emission tomography (PET)/CT were available. Results: There were 243 statistically significant pairwise correlations between image features and metagenes of NSCLC. Metagenes were predicted in terms of image features with an accuracy of 59%-83%. One hundred fourteen of 180 CT image features and the PET standardized uptake value were predicted in terms of metagenes with an accuracy of 65%-86%. When the predicted image features were mapped to a public gene expression data set with survival outcomes, tumor size, edge shape, and sharpness ranked highest for prognostic significance. Conclusion: This radiogenomics strategy for identifying imaging biomarkers may enable a more rapid evaluation of novel imaging modalities, thereby accelerating their translation to personalized medicine. {\textcopyright} RSNA, 2012.},
  doi      = {10.1148/radiol.12111607},
  pmid     = {22723499},
  url      = {http://pubs.rsna.org/doi/10.1148/radiol.12111607},
}

@Misc{art/ArmatoIIIS.G.HadjiiskiL.TourassiG.D.DrukkerK.GigerM.L.LiF.R_2015,
  author    = {{Armato III, S. G., Hadjiiski, L., Tourassi, G.D., Drukker, K., Giger, M.L., Li, F.}, Redmond and {G., Farahani, K., Kirby, J.S. and Clarke}, L.P.},
  title     = {{SPIE-AAPM-NCI Lung Nodule Classification Challenge}},
  year      = {2015},
  booktitle = {Cancer Imaging Arch 10},
  doi       = {10.7937/K9/TCIA.2015.UZLSU3FL},
  pages     = {p.K9},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/bImiAQ},
}

@Misc{art/NapelSandy&PlevritisS_2014,
  author    = {{Napel, Sandy, & Plevritis}, Sylvia K.},
  title     = {{NSCLC Radiogenomics: Initial Stanford Study of 26 Cases. The Cancer Imaging Archive.}},
  year      = {2014},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2014.X7ONY6B1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/Gglp},
}

@Article{art/ArmatoS_201101,
  author   = {Armato, Samuel G. and McLennan, Geoffrey and Bidaut, Luc and McNitt-Gray, Michael F. and Meyer, Charles R. and Reeves, Anthony P. and Zhao, Binsheng and Aberle, Denise R. and Henschke, Claudia I. and Hoffman, Eric A. and Kazerooni, Ella A. and MacMahon, Heber and {Van Beek}, Edwin J.R. and Yankelevitz, David and Biancardi, Alberto M. and Bland, Peyton H. and Brown, Matthew S. and Engelmann, Roger M. and Laderach, Gary E. and Max, Daniel and Pais, Richard C. and Qing, David P.Y. and Roberts, Rachael Y. and Smith, Amanda R. and Starkey, Adam and Batra, Poonam and Caligiuri, Philip and Farooqi, Ali and Gladish, Gregory W. and Jude, C. Matilda and Munden, Reginald F. and Petkovska, Iva and Quint, Leslie E. and Schwartz, Lawrence H. and Sundaram, Baskaran and Dodd, Lori E. and Fenimore, Charles and Gur, David and Petrick, Nicholas and Freymann, John and Kirby, Justin and Hughes, Brian and {Vande Casteele}, Alessi and Gupte, Sangeeta and Sallam, Maha and Heath, Michael D. and Kuhn, Michael H. and Dharaiya, Ekta and Burns, Richard and Fryd, David S. and Salganicoff, Marcos and Anand, Vikram and Shreter, Uri and Vastagh, Stephen and Croft, Barbara Y. and Clarke, Laurence P.},
  journal  = {Medical Physics},
  title    = {{The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans}},
  year     = {2011},
  issn     = {00942405},
  month    = {jan},
  number   = {2},
  pages    = {915--931},
  volume   = {38},
  abstract = {Purpose: The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Methods: Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (" nodule≥3 mm," " nodule<3 mm," and "non- nodule≥3 mm "). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus. Results: The Database contains 7371 lesions marked "nodule" by at least one radiologist. 2669 of these lesions were marked " nodul≥3 mm " by at least one radiologist, of which 928 (34.7%) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings. Conclusions: The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice. {\textcopyright} 2011 U.S. Government.},
  doi      = {10.1118/1.3528204},
  keywords = {computed tomography (CT),computer-aided diagnosis (CAD),interobserver variability,lung nodule,thoracic imaging},
  pmid     = {21452728},
  url      = {http://doi.wiley.com/10.1118/1.3528204},
}

@Misc{art/ArmatoI_2015,
  author    = {III, Armato and G., Samuel and McLennan and Geoffrey and Bidaut and Luc and McNitt-Gray and Michael, F. and R., Meyer and Charles and Reeves},
  title     = {{Data From LIDC-IDRI}},
  year      = {2015},
  abstract  = {The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases. Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule > or =3 mm," "nodule <3 mm," and "non-nodule > or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.LO9QL9SX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/rgAe},
}

@Article{art/JohnsonC_200809,
  author   = {Johnson, C. Daniel and Chen, Mei-Hsiu and Toledano, Alicia Y. and Heiken, Jay P. and Dachman, Abraham and Kuo, Mark D. and Menias, Christine O. and Siewert, Betina and Cheema, Jugesh I. and Obregon, Richard G. and Fidler, Jeff L. and Zimmerman, Peter and Horton, Karen M. and Coakley, Kevin and Iyer, Revathy B. and Hara, Amy K. and Halvorsen, Robert A. and Casola, Giovanna and Yee, Judy and Herman, Benjamin A. and Burgart, Lawrence J. and Limburg, Paul J.},
  journal  = {New England Journal of Medicine},
  title    = {{Accuracy of CT Colonography for Detection of Large Adenomas and Cancers}},
  year     = {2008},
  issn     = {0028-4793},
  month    = {sep},
  number   = {12},
  pages    = {1207--1217},
  volume   = {359},
  abstract = {BACKGROUND: Computed tomographic (CT) colonography is a noninvasive option in screening for colorectal cancer. However, its accuracy as a screening tool in asymptomatic adults has not been well defined. METHODS: We recruited 2600 asymptomatic study participants, 50 years of age or older, at 15 study centers. CT colonographic images were acquired with the use of standard bowel preparation, stool and fluid tagging, mechanical insufflation, and multidetector-row CT scanners (with 16 or more rows). Radiologists trained in CT colonography reported all lesions measuring 5 mm or more in diameter. Optical colonoscopy and histologic review were performed according to established clinical protocols at each center and served as the reference standard. The primary end point was detection by CT colonography of histologically confirmed large adenomas and adenocarcinomas (10 mm in diameter or larger) that had been detected by colonoscopy; detection of smaller colorectal lesions (6 to 9 mm in diameter) was also evaluated. RESULTS: Complete data were available for 2531 participants (97%). For large adenomas and cancers, the mean (±SE) per-patient estimates of the sensitivity, specificity, positive and negative predictive values, and area under the receiver-operating-characteristic curve for CT colonography were 0.90±0.03, 0.86±0.02, 0.23±0.02, 0.99±<0.01, and 0.89±0.02, respectively. The sensitivity of 0.90 (i.e., 90%) indicates that CT colonography failed to detect a lesion measuring 10 mm or more in diameter in 10% of patients. The per-polyp sensitivity for large adenomas or cancers was 0.84±0.04. The per-patient sensitivity for detecting adenomas that were 6 mm or more in diameter was 0.78. CONCLUSIONS: In this study of asymptomatic adults, CT colonographic screening identified 90% of subjects with adenomas or cancers measuring 10 mm or more in diameter. These findings augment published data on the role of CT colonography in screening patients with an average risk of colorectal cancer. (ClinicalTrials.gov number, NCT00084929; American College of Radiology Imaging Network [ACRIN] number, 6664.) Copyright {\textcopyright} 2008 Massachusetts Medical Society. All rights reserved.},
  doi      = {10.1056/nejmoa0800996},
  pmid     = {18799557},
  url      = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0800996},
}

@Article{art/ZhaoB_200907,
  author   = {Zhao, Binsheng and James, Leonard P. and Moskowitz, Chaya S. and Guo, Pingzhen and Ginsberg, Michelle S. and Lefkowitz, Robert A. and Qin, Yilin and Riely, Gregory J. and Kris, Mark G. and Schwartz, Lawrence H.},
  journal  = {Radiology},
  title    = {{Evaluating variability in tumor measurements from same-day repeat CT scans of patients with non-small cell lung cancer}},
  year     = {2009},
  issn     = {00338419},
  month    = {jul},
  number   = {1},
  pages    = {263--272},
  volume   = {252},
  abstract = {Purpose: To evaluate the variability of tumor unidimensional, bidimensional, and volumetric measurements on same-day repeat computed tomographic (CT) scans in patients with non-small cell lung cancer. Materials and Methods: This HIPAA-compliant study was approved by the institutional review board, with informed patient consent. Thirty-two patients with non-small cell lung cancer, each of whom underwent two CT scans of the chest within 15 minutes by using the same imaging protocol, were included in this study. Three radiologists independently measured the two greatest diameters of each lesion on both scans and, during another session, measured the same tumors on the first scan. In a separate analysis, computer software was applied to assist in the calculation of the two greatest diameters and the volume of each lesion on both scans. Concordance correlation coefficients (CCCs) and Bland-Altman plots were used to assess the agreements between the measurements of the two repeat scans (reproducibility) and between the two repeat readings of the same scan (repeatability). Results: The reproducibility and repeatability of the three radiologists' measurements were high (all CCCs, ≥0.96). The reproducibility of the computer-aided measurements was even higher (all CCCs, 1.00). The 95% limits of agreements for the computer-aided unidimensional, bidimensional, and volumetric measurements on two repeat scans were (-7.3%, 6.2%), (-17.6%, 19.8%), and (-12.1%, 13.4%), respectively. Conclusion: Chest CT scans are well reproducible. Changes in unidimensional lesion size of 8% or greater exceed the measurement variability of the computer method and can be considered significant when estimating the outcome of therapy in a patient. {\textcopyright} RSNA, 2009.},
  doi      = {10.1148/radiol.2522081593},
  pmid     = {19561260},
  url      = {http://pubs.rsna.org/doi/10.1148/radiol.2522081593},
}

@Article{art/GinnekenB_2010,
  author   = {van Ginneken, Bram and Armato, Samuel G. and de Hoop, Bartjan and {van Amelsvoort-van de Vorst}, Saskia and Duindam, Thomas and Niemeijer, Meindert and Murphy, Keelin and Schilham, Arnold and Retico, Alessandra and Fantacci, Maria Evelina and Camarlinghi, Niccol and Bagagli, Francesco and Gori, Ilaria and Hara, Takeshi and Fujita, Hiroshi and Gargano, Gianfranco and Bellotti, Roberto and Tangaro, Sabina and Bolaos, Lourdes and Carlo, Francesco De and Cerello, Piergiorgio and {Cristian Cheran}, Sorin and {Lopez Torres}, Ernesto and Prokop, Mathias},
  journal  = {Medical Image Analysis},
  title    = {{Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: The ANODE09 study}},
  year     = {2010},
  issn     = {13618415},
  number   = {6},
  pages    = {707--722},
  volume   = {14},
  abstract = {Numerous publications and commercial systems are available that deal with automatic detection of pulmonary nodules in thoracic computed tomography scans, but a comparative study where many systems are applied to the same data set has not yet been performed. This paper introduces ANODE09 ( http://anode09.isi.uu.nl), a database of 55 scans from a lung cancer screening program and a web-based framework for objective evaluation of nodule detection algorithms. Any team can upload results to facilitate benchmarking. The performance of six algorithms for which results are available are compared; five from academic groups and one commercially available system. A method to combine the output of multiple systems is proposed. Results show a substantial performance difference between algorithms, and demonstrate that combining the output of algorithms leads to marked performance improvements. {\textcopyright} 2010 Elsevier B.V.},
  doi      = {10.1016/j.media.2010.05.005},
  keywords = {Computed tomography,Computer-aided detection,Lung cancer,Lung nodules},
  pmid     = {20573538},
}

@Article{art/SuinesiaputraA_201803,
  author   = {Suinesiaputra, Avan and Ablin, Pierre and Alba, Xenia and Alessandrini, Martino and Allen, Jack and Bai, Wenjia and Cimen, Serkan and Claes, Peter and Cowan, Brett R. and Dhooge, Jan and Duchateau, Nicolas and Ehrhardt, Jan and Frangi, Alejandro F. and Gooya, Ali and Grau, Vicente and Lekadir, Karim and Lu, Allen and Mukhopadhyay, Anirban and Oksuz, Ilkay and Parajuli, Nripesh and Pennec, Xavier and Pereanez, Marco and Pinto, Catarina and Piras, Paolo and Rohe, Marc Michel and Rueckert, Daniel and Saring, Dennis and Sermesant, Maxime and Siddiqi, Kaleem and Tabassian, Mahdi and Teresi, Luciano and Tsaftaris, Sotirios A. and Wilms, Matthias and Young, Alistair A. and Zhang, Xingyu and Medrano-Gracia, Pau},
  journal  = {IEEE Journal of Biomedical and Health Informatics},
  title    = {{Statistical Shape Modeling of the Left Ventricle: Myocardial Infarct Classification Challenge}},
  year     = {2018},
  issn     = {21682194},
  month    = {mar},
  number   = {2},
  pages    = {503--515},
  volume   = {22},
  abstract = {Statistical shape modeling is a powerful tool for visualizing and quantifying geometric and functional patterns of the heart. After myocardial infarction (MI), the left ventricle typically remodels in response to physiological challenges. Several methods have been proposed in the literature to describe statistical shape changes. Which method best characterizes the left ventricular remodeling after MI is an open research question. A better descriptor of remodeling is expected to provide a more accurate evaluation of disease status in MI patients. We therefore designed a challenge to test shape characterization in MI given a set of three-dimensional left ventricular surface points. The training set comprised 100 MI patients, and 100 asymptomatic volunteers (AV). The challenge was initiated in 2015 at the Statistical Atlases and Computational Models of the Heart workshop, in conjunction with the MICCAI conference. The training set with labels was provided to participants, who were asked to submit the likelihood of MI from a different (validation) set of 200 cases (100 AV and 100 MI). Sensitivity, specificity, accuracy, and area under the receiver operating characteristic curve were used as the outcome measures. The goals of this challenge were to 1) establish a common dataset for evaluating statistical shape modeling algorithms in MI, and 2) test whether statistical shape modeling provides additional information characterizing MI patients over standard clinical measures. Eleven groups with a wide variety of classification and feature extraction approaches participated in this challenge. All methods achieved excellent classification results with accuracy ranges from 0.83 to 0.98. The areas under the receiver operating characteristic curves were all above 0.90. Four methods showed significantly higher performance than standard clinical measures. The dataset and software for evaluation are available from the Cardiac Atlas Project website.11 http://www.cardiacatlas.org.},
  doi      = {10.1109/JBHI.2017.2652449},
  keywords = {Cardiac modeling,classification,myocardial infarct,statistical shape analysis},
  pmid     = {28103561},
  url      = {https://ieeexplore.ieee.org/document/7820042/},
}

@Misc{art/ZhaoB_2015,
  author    = {Zhao, Binsheng and Schwartz, Lawrence H and Kris, Mark G},
  title     = {{Data From RIDER_Lung CT}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.U1X8A5NR},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/XIRXAQ},
}

@Article{art/WierstraD_2009,
  author   = {Wierstra, Daan and F{\"{o}}rster, Alexander and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
  journal  = {Logic Journal of the IGPL},
  title    = {{Recurrent policy gradients}},
  year     = {2009},
  issn     = {13689894},
  number   = {5},
  pages    = {620--634},
  volume   = {18},
  abstract = {Reinforcement learning for partially observable Markov decision problems (POMDPs) is a challenge as it requires policies with an internal state. Traditional approaches suffer significantly from this shortcoming and usually make strong assumptions on the problem domain such as perfect system models, state-estimators and a Markovian hidden system. Recurrent neural networks (RNNs) offer a natural framework for dealing with policy learning using hidden state and require only few limiting assumptions. As they can be trained well using gradient descent, they are suited for policy gradient approaches. In this paper, we present a policy gradient method, the Recurrent Policy Gradient which constitutes a model-free reinforcement learning method. It is aimed at training limited-memory stochastic policies on problems which require long-term memories of past observations. The approach involves approximating a policy gradient for a recurrent neural network by backpropagating return-weighted characteristic eligibilities through time. Using a "Long Short-Term Memory" RNN architecture, we are able to outperform previous RL methods on three important benchmark tasks. Furthermore, we show that using history-dependent baselines helps reducing estimation variance significantly, thus enabling our approach to tackle more challenging, highly stochastic environments. {\textcopyright} The Author 2009. Published by Oxford University Press. All rights reserved.},
  doi      = {10.1093/jigpal/jzp049},
  keywords = {Partially Observable Markov Decision Problems (POM,Policy gradient methods,Recurrent neural networks,Reinforcement learning},
}

@InProceedings{art/SchulmanJ_2016,
  author        = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
  booktitle     = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  title         = {{High-dimensional continuous control using generalized advantage estimation}},
  year          = {2016},
  abstract      = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archiveprefix = {arXiv},
  arxivid       = {1506.02438},
  eprint        = {1506.02438},
}

@InProceedings{art/GomezF_2005,
  author    = {Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Evolving modular fast-weight networks for control}},
  year      = {2005},
  pages     = {383--389},
  volume    = {3697 LNCS},
  abstract  = {In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
  doi       = {10.1007/11550907_61},
  isbn      = {3540287558},
  issn      = {03029743},
}

@Article{art/DeisenrothM_2011,
  author   = {Deisenroth, Marc Peter},
  journal  = {Foundations and Trends in Robotics},
  title    = {{A Survey on Policy Search for Robotics}},
  year     = {2011},
  issn     = {1935-8253},
  number   = {1-2},
  pages    = {1--142},
  volume   = {2},
  abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
  doi      = {10.1561/2300000021},
}

@InProceedings{art/LillicrapT_2016,
  author        = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle     = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  title         = {{Continuous control with deep reinforcement learning}},
  year          = {2016},
  abstract      = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
  archiveprefix = {arXiv},
  arxivid       = {1509.02971},
  eprint        = {1509.02971},
}

@Misc{art/LevineS_2016,
  author        = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  title         = {{End-to-end training of deep visuomotor policies}},
  year          = {2016},
  abstract      = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
  archiveprefix = {arXiv},
  arxivid       = {1504.00702},
  booktitle     = {Journal of Machine Learning Research},
  eprint        = {1504.00702},
  issn          = {15337928},
  keywords      = {Neural networks,Optimal control,Reinforcement learning,Vision},
  volume        = {17},
}

@InProceedings{art/KoutnikJ_2013,
  author    = {Koutn{\'{i}}k, Jan and Cuccu, Giuseppe and Schmidhuber, J{\"{u}}rgen and Gomez, Faustino},
  booktitle = {GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference},
  title     = {{Evolving large-scale neural networks for vision-based reinforcement learning}},
  year      = {2013},
  pages     = {1061--1068},
  abstract  = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), for reinforcement learning (RL) tasks has now been around for over 20 years. However, as RL tasks become more challenging, the networks required become larger, so do their genomes. But, scaling NE to large nets (i.e. tens of thousands of weights) is infeasible using direct encodings that map genes one-to-one to network components. In this paper, we scale-up our "compressed" network encoding where network weight matrices are represented indirectly as a set of Fourier-type coefficients, to tasks that require very-large networks due to the high-dimensionality of their input space. The approach is demonstrated successfully on two reinforcement learning tasks in which the control networks receive visual input: (1) a vision-based version of the octopus control task requiring networks with over 3 thousand weights, and (2) a version of the TORCS driving game where networks with over 1 million weights are evolved to drive a car around a track using video images from the driver's perspective. Copyright {\textcopyright} 2013 ACM.},
  doi       = {10.1145/2463372.2463509},
  isbn      = {9781450319638},
  keywords  = {Games,Indirect encodings,Neuroevolution,Reinforcement learning,Vision-based TORCS},
}

@Article{art/WilliaR_1992,
  author   = {Willia, Ronald J.},
  journal  = {Machine Learning},
  title    = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
  year     = {1992},
  issn     = {15730565},
  number   = {3},
  pages    = {229--256},
  volume   = {8},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms. {\textcopyright} 1992, Kluwer Academic Publishers. All rights reserved.},
  doi      = {10.1023/A:1022672621406},
  keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
}

@Article{art/KalpathyCramerJ_2015,
  author   = {Kalpathy-Cramer, Jayashree and de Herrera, Alba Garc{\'{i}}a Seco and Demner-Fushman, Dina and Antani, Sameer and Bedrick, Steven and M{\"{u}}ller, Henning},
  journal  = {Computerized Medical Imaging and Graphics},
  title    = {{Evaluating performance of biomedical image retrieval systems-An overview of the medical image retrieval task at ImageCLEF 2004-2013}},
  year     = {2015},
  issn     = {18790771},
  pages    = {55--61},
  volume   = {39},
  abstract = {Medical image retrieval and classification have been extremely active research topics over the past 15 years. Within the ImageCLEF benchmark in medical image retrieval and classification, a standard test bed was created that allows researchers to compare their approaches and ideas on increasingly large and varied data sets including generated ground truth. This article describes the lessons learned in ten evaluation campaigns. A detailed analysis of the data also highlights the value of the resources created.},
  doi      = {10.1016/j.compmedimag.2014.03.004},
  keywords = {Biomedical literature,Content-based retrieval,Image retrieval,Multimodal medical retrieval,Text-based image retrieval},
  pmid     = {24746250},
}

@Misc{art/MadabhushiA_2018,
  author    = {Madabhushi, Anant and Rusu, Mirabela},
  title     = {{Fused Radiology-Pathology Lung Dataset}},
  year      = {2018},
  doi       = {10.7937/K9/TCIA.2018.SMT36LPN},
  keywords  = {Computed Tomography,Invasive Adenocarcinoma,Pathology,Pulmonary Nodules},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/LoBgAg},
}

@Article{art/FlohrT_2005,
  author   = {Flohr, T. G. and Stierstorfer, K. and Ulzheimer, S. and Bruder, H. and Primak, A. N. and McCollough, C. H.},
  journal  = {Medical Physics},
  title    = {{Image reconstruction and image quality evaluation for a 64-slice CT scanner with z-flying focal spot}},
  year     = {2005},
  issn     = {00942405},
  number   = {8},
  pages    = {2536--2547},
  volume   = {32},
  abstract = {We present a theoretical overview and a performance evaluation of a novel z-sampling technique for multidetector row CT (MDCT), relying on a periodic motion of the focal spot in the longitudinal direction (z-flying focal spot) to double the number of simultaneously acquired slices. The z-flying focal spot technique has been implemented in a recently introduced MDCT scanner. Using 32 × 0.6 mm collimation, this scanner acquires 64 overlapping 0.6 mm slices per rotation in its spiral (helical) mode of operation, with the goal of improved longitudinal resolution and reduction of spiral artifacts. The longitudinal sampling distance at isocenter is 0.3 mm. We discuss in detail the impact of the z-flying focal spot technique on image reconstruction. We present measurements of spiral slice sensitivity profiles (SSPs) and of longitudinal resolution, both in the isocenter and off-center. We evaluate the pitch dependence of the image noise measured in a centered 20 cm water phantom. To investigate spiral image quality we present images of an anthropomorphic thorax phantom and patient scans. The full width at half maximum (FWHM) of the spiral SSPs shows only minor variations as a function of the pitch, measured values differ by less than 0.15 mm from the nominal values 0.6, 0.75, 1, 1.5, and 2 mm. The measured FWHM of the smallest slice ranges between 0.66 and 0.68 mm at isocenter, except for pitch 0.55 (0.72 mm). In a centered z-resolution phantom, bar patterns up to 15 lp/cm can be visualized independent of the pitch, corresponding to 0.33 mm longitudinal resolution. 100 mm off-center, bar patterns up to 14 lp/cm are visible, corresponding to an object size of 0.36 mm that can be resolved in the z direction. Image noise for constant effective mAs is almost independent of the pitch. Measured values show a variation of less than 7% as a function of the pitch, which demonstrates correct utilization of the applied radiation dose at any pitch. The product of image noise and square root of the slice width (FWHM of the respective SSP) is the same constant for all slices except 0.6 mm. For the thinnest slice, relative image noise is increased by 17%. Spiral windmill-type artifacts are effectively suppressed with the z-flying focal spot technique, which has the potential to maintain a low artifact level up to pitch 1.5, in this way increasing the maximum volume coverage speed that can be clinically used. {\textcopyright} 2005 American Association of Physicists in Medicine.},
  doi      = {10.1118/1.1949787},
  keywords = {CT data sampling,CT image quality evaluation,Cone-beam CT,Multi-detector row CT},
  pmid     = {16193784},
}

@InProceedings{art/WangX_2017,
  author        = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases}},
  year          = {2017},
  pages         = {3462--3471},
  volume        = {2017-Janua},
  abstract      = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weaklysupervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.},
  archiveprefix = {arXiv},
  arxivid       = {1705.02315},
  doi           = {10.1109/CVPR.2017.369},
  eprint        = {1705.02315},
  isbn          = {9781538604571},
}

@Article{art/KachelriessM_2006,
  author   = {Kachelrie{\ss}, Marc and Knaup, Michael and Pen{\ss}el, Christian and Kalender, Willi A.},
  journal  = {IEEE Transactions on Nuclear Science},
  title    = {{Flying focal spot (FFS) in cone-beam CT}},
  year     = {2006},
  issn     = {00189499},
  number   = {3},
  pages    = {1238--1247},
  volume   = {53},
  abstract = {In the beginning of 2004 medical spiral-CT scanners that acquire up to 64 slices simultaneously became available. Most manufacturers use a straightforward acquisition principle, namely an x-ray focus rotating on a circular path and an opposing cylindrical detector whose rotational center coincides with the x-ray focus. The 64-slice scanner available to us, a Somatom Sensation 64 spiral cone-beam CT scanner (Siemens, Medical Solutions, Forchheim, Germany), makes use of a flying focal spot (FFS) that allows for view-by-view deflections of the focal spot in the rotation direction (aFFS) and in the z-direction (zFFS) with the goal of reducing aliasing artifacts. The FFS feature doubles the sampling density in the radial direction (channel direction, aFFS) and in the longitudinal direction (detector row direction or z-direction, zFFS). The cost of increased radial and azimuthal sampling is a two- or four-fold reduction of azimuthal sampling (angular sampling). To compensate for the potential reduction of azimuthal sampling the scanner simply increases the number of detector read-outs (readings) per rotation by a factor two or four. Then, up to four detector readings contribute to what we define as one view or one projection. A significant reduction of in-plane aliasing and of aliasing in the z -direction can be expected. Especially the latter is of importance to spiral CT scans where aliasing is known to produce so-called windmill artifacts. We have derived and analyzed the optimal focal spot deflection values da and dz as they would ideally occur in our scanner. Based upon these we show how image reconstruction can be performed in general. A simulation study showing reconstructions of mathematical phantoms further provides evidence that image quality can be significantly improved with the FFS. Aliasing artifacts, that manifest as streaks emerging from high-contrast objects, and windmill artifacts are reduced by almost an order of magnitude with the FFS compared to a simulation without FFS. Patient images acquired with our 64-slice cone-beam CT scanner support these results. {\textcopyright} 2006 IEEE.},
  doi      = {10.1109/TNS.2006.874076},
  keywords = {Computed tomography,Cone-beam CT,Image quality,Image reconstruction,Spiral-CT},
}

@InProceedings{art/KohlN_2004,
  author    = {Kohl, Nate and Stone, Peter},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  title     = {{Policy gradient reinforcement learning for fast quadrupedal locomotion}},
  year      = {2004},
  abstract  = {This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a commercially available quadrupedal robot platform, namely the Sony Aibo robot. After about three hours of learning, all on the physical robots and with no human intervention other than to change the batteries, the robots achieved a gait faster than any previously known gait known for the Aibo, significantly outperforming, a variety of existing hand-coded and learned solutions.},
  doi       = {10.1109/robot.2004.1307456},
  issn      = {10504729},
  keywords  = {Learning Control,Multi Legged Robots,Walking Robots},
}

@Article{art/NgA_2006,
  author   = {Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
  journal  = {Springer Tracts in Advanced Robotics},
  title    = {{Autonomous inverted helicopter flight via reinforcement earning}},
  year     = {2006},
  issn     = {16107438},
  pages    = {363--372},
  volume   = {21},
  abstract = {Helicopters have highly stochastic, nonlinear, dynamics, and autonomous helicopter flight is widely regarded to be a challenging control problem. As helicopters are highly unstable at low speeds, it is particularly difficult to design controllers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopter's dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform. {\textcopyright} Springer-Verlag Berlin/Heidelberg 2006.},
  doi      = {10.1007/11552246_35},
  isbn     = {3540288163},
}

@Article{art/SinghS_2002,
  author   = {Singh, Satinder and Man, Diane Lit and Kearns, Michael and Walker, Marilyn},
  journal  = {Journal of Artificial Intelligence Research},
  title    = {{Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system}},
  year     = {2002},
  issn     = {10769757},
  pages    = {105--133},
  volume   = {16},
  abstract = {Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance. {\textcopyright} 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
  doi      = {10.1613/jair.859},
}

@Article{art/TesauC_1995,
  author  = {Tesau, Covid and Tesau, Gerald},
  journal = {Communications of the ACM},
  title   = {{Temporal Difference Learning and TD-Gammon}},
  year    = {1995},
  issn    = {15577317},
  number  = {3},
  pages   = {58--68},
  volume  = {38},
  doi     = {10.1145/203330.203343},
}

@Article{art/StoeckelH_197008,
  author  = {Stoeckel, H. and Stober, B.},
  journal = {Zeitschrift fur praktische Anasthesie und Wiederbelebung},
  title   = {{Zur Problematik der Massivtransfusion mit ACD-Blut.}},
  year    = {1970},
  issn    = {00443387},
  month   = {aug},
  number  = {4},
  pages   = {237--250},
  volume  = {5},
  pmid    = {4256233},
  url     = {http://www.ncbi.nlm.nih.gov/pubmed/4256233},
}

@Misc{art/JohnsonA_2019,
  author    = {Johnson, Alistair E W and Pollard, Tom and Mark, Roger and Berkowitz, Seth and Horng, Steven},
  title     = {{The MIMIC-CXR Database}},
  year      = {2019},
  doi       = {10.13026/C2JT1Q},
  publisher = {physionet.org},
  url       = {https://physionet.org/content/mimic-cxr/},
}

@Misc{art/LitjensG_2017,
  author    = {Litjens, Geert and Debats, Oscar and Barentsz, Jelle and Karssemeijer, Nico and Huisman, Henkjan},
  title     = {{ProstateX challenge data}},
  year      = {2017},
  abstract  = {This collection is a retrospective set of prostate MR studies. All studies included T2-weighted (T2W), proton density-weighted (PD-W), dynamic contrast enhanced (DCE), and diffusion-weighted (DW) imaging. The images were acquired on two different types of Siemens 3T MR scanners, the MAGNETOM Trio and Skyra. T2-weighted images were acquired using a turbo spin echo sequence and had a resolution of around 0.5 mm in plane and a slice thickness of 3.6 mm. The DCE time series was acquired using a 3-D turbo flash gradient echo sequence with a resolution of around 1.5 mm in-plane, a slice thickness of 4 mm and a temporal resolution of 3.5 s. The proton density weighted image was acquired prior to the DCE time series using the same sequence with different echo and repetition times and a different flip angle. Finally, the DWI series were acquired with a single-shot echo planar imaging sequence with a resolution of 2 mm in-plane and 3.6 mm slice thickness and with diffusion-encoding gradients in three directions. Three b-values were acquired (50, 400, and 800), and subsequently, the ADC map was calculated by the scanner software. All images were acquired without an endorectal coil.},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9TCIA.2017.MURS5CL},
  pages     = {K9TCIA},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/iIFpAQ},
  volume    = {10},
}

@Article{art/JohnsonA_2019a,
  author   = {Johnson, Alistair E.W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and ying Deng, Chih and Mark, Roger G. and Horng, Steven},
  journal  = {Scientific Data},
  title    = {{MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports}},
  year     = {2019},
  issn     = {20524463},
  number   = {1},
  volume   = {6},
  abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's chest, but requires specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. Here we describe MIMIC-CXR, a large dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011–2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in the dataset. Studies are made available with a semi-structured free-text radiology report that describes the radiological findings of the images, written by a practicing radiologist contemporaneously during routine clinical care. All images and reports have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in computer vision, natural language processing, and clinical data mining.},
  doi      = {10.1038/s41597-019-0322-0},
  pmid     = {31831740},
}

@Article{art/VallieresM_201507,
  author   = {Valli{\`{e}}res, M. and Freeman, C. R. and Skamene, S. R. and {El Naqa}, I.},
  journal  = {Physics in Medicine and Biology},
  title    = {{A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities}},
  year     = {2015},
  issn     = {13616560},
  month    = {jul},
  number   = {14},
  pages    = {5471--5496},
  volume   = {60},
  abstract = {This study aims at developing a joint FDG-PET and MRI texture-based model for the early evaluation of lung metastasis risk in soft-tissue sarcomas (STSs). We investigate if the creation of new composite textures from the combination of FDG-PET and MR imaging information could better identify aggressive tumours. Towards this goal, a cohort of 51 patients with histologically proven STSs of the extremities was retrospectively evaluated. All patients had pre-treatment FDG-PET and MRI scans comprised of T1-weighted and T2-weighted fat-suppression sequences (T2FS). Nine non-texture features (SUV metrics and shape features) and forty-one texture features were extracted from the tumour region of separate (FDG-PET, T1 and T2FS) and fused (FDG-PET/T1 and FDG-PET/T2FS) scans. Volume fusion of the FDG-PET and MRI scans was implemented using the wavelet transform. The influence of six different extraction parameters on the predictive value of textures was investigated. The incorporation of features into multivariable models was performed using logistic regression. The multivariable modeling strategy involved imbalance-adjusted bootstrap resampling in the following four steps leading to final prediction model construction: (1) feature set reduction; (2) feature selection; (3) prediction performance estimation; and (4) computation of model coefficients. Univariate analysis showed that the isotropic voxel size at which texture features were extracted had the most impact on predictive value. In multivariable analysis, texture features extracted from fused scans significantly outperformed those from separate scans in terms of lung metastases prediction estimates. The best performance was obtained using a combination of four texture features extracted from FDG-PET/T1 and FDG-PET/T2FS scans. This model reached an area under the receiver-operating characteristic curve of 0.984 ± 0.002, a sensitivity of 0.955 ± 0.006, and a specificity of 0.926 ± 0.004 in bootstrapping evaluations. Ultimately, lung metastasis risk assessment at diagnosis of STSs could improve patient outcomes by allowing better treatment adaptation.},
  doi      = {10.1088/0031-9155/60/14/5471},
  keywords = {FDG-PET,MRI,lung metastases,outcome prediction,radiomics,soft-tissue sarcoma,texture analysis},
  pmid     = {26119045},
  url      = {https://iopscience.iop.org/article/10.1088/0031-9155/60/14/5471},
}

@Article{art/LitjensG_201405,
  author   = {Litjens, Geert and Debats, Oscar and Barentsz, Jelle and Karssemeijer, Nico and Huisman, Henkjan},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Computer-aided detection of prostate cancer in MRI}},
  year     = {2014},
  issn     = {1558254X},
  month    = {may},
  number   = {5},
  pages    = {1083--1092},
  volume   = {33},
  abstract = {Prostate cancer is one of the major causes of cancer death for men in the western world. Magnetic resonance imaging (MRI) is being increasingly used as a modality to detect prostate cancer. Therefore, computer-aided detection of prostate cancer in MRI images has become an active area of research. In this paper we investigate a fully automated computer-aided detection system which consists of two stages. In the first stage, we detect initial candidates using multi-atlas-based prostate segmentation, voxel feature extraction, classification and local maxima detection. The second stage segments the candidate regions and using classification we obtain cancer likelihoods for each candidate. Features represent pharmacokinetic behavior, symmetry and appearance, among others. The system is evaluated on a large consecutive cohort of 347 patients with MR-guided biopsy as the reference standard. This set contained 165 patients with cancer and 182 patients without prostate cancer. Performance evaluation is based on lesion-based free-response receiver operating characteristic curve and patient-based receiver operating characteristic analysis. The system is also compared to the prospective clinical performance of radiologists. Results show a sensitivity of 0.42, 0.75, and 0.89 at 0.1, 1, and 10 false positives per normal case. In clinical workflow the system could potentially be used to improve the sensitivity of the radiologist. At the high specificity reading setting, which is typical in screening situations, the system does not perform significantly different from the radiologist and could be used as an independent second reader instead of a second radiologist. Furthermore, the system has potential in a first-reader setting. {\textcopyright} 2014 IEEE.},
  doi      = {10.1109/TMI.2014.2303821},
  keywords = {Computer-aided detection,image analysis,machine learning,magnetic resonance imaging,prostate cancer},
  pmid     = {24770913},
  url      = {http://ieeexplore.ieee.org/document/6729091/},
}

@Article{art/BernardO_201811,
  author   = {Bernard, Olivier and Lalande, Alain and Zotti, Clement and Cervenansky, Frederick and Yang, Xin and Heng, Pheng Ann and Cetin, Irem and Lekadir, Karim and Camara, Oscar and {Gonzalez Ballester}, Miguel Angel and Sanroma, Gerard and Napel, Sandy and Petersen, Steffen and Tziritas, Georgios and Grinias, Elias and Khened, Mahendra and Kollerathu, Varghese Alex and Krishnamurthi, Ganapathy and Rohe, Marc Michel and Pennec, Xavier and Sermesant, Maxime and Isensee, Fabian and Jager, Paul and Maier-Hein, Klaus H. and Full, Peter M. and Wolf, Ivo and Engelhardt, Sandy and Baumgartner, Christian F. and Koch, Lisa M. and Wolterink, Jelmer M. and Isgum, Ivana and Jang, Yeonggul and Hong, Yoonmi and Patravali, Jay and Jain, Shubham and Humbert, Olivier and Jodoin, Pierre Marc},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?}},
  year     = {2018},
  issn     = {1558254X},
  month    = {nov},
  number   = {11},
  pages    = {2514--2525},
  volume   = {37},
  abstract = {Delineation of the left ventricular cavity, myocardium, and right ventricle from cardiac magnetic resonance images (multi-slice 2-D cine MRI) is a common clinical task to establish diagnosis. The automation of the corresponding tasks has thus been the subject of intense research over the past decades. In this paper, we introduce the 'Automatic Cardiac Diagnosis Challenge' dataset (ACDC), the largest publicly available and fully annotated dataset for the purpose of cardiac MRI (CMR) assessment. The dataset contains data from 150 multi-equipments CMRI recordings with reference measurements and classification from two medical experts. The overarching objective of this paper is to measure how far state-of-the-art deep learning methods can go at assessing CMRI, i.e., segmenting the myocardium and the two ventricles as well as classifying pathologies. In the wake of the 2017 MICCAI-ACDC challenge, we report results from deep learning methods provided by nine research groups for the segmentation task and four groups for the classification task. Results show that the best methods faithfully reproduce the expert analysis, leading to a mean value of 0.97 correlation score for the automatic extraction of clinical indices and an accuracy of 0.96 for automatic diagnosis. These results clearly open the door to highly accurate and fully automatic analysis of cardiac CMRI. We also identify scenarios for which deep learning methods are still failing. Both the dataset and detailed results are publicly available online, while the platform will remain open for new submissions.},
  doi      = {10.1109/TMI.2018.2837502},
  keywords = {Cardiac segmentation and diagnosis,MRI,deep learning,left and right ventricles,myocardium},
  url      = {https://ieeexplore.ieee.org/document/8360453/},
}

@Article{art/KaelblingL_199604,
  author        = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
  journal       = {Journal of Artificial Intelligence Research},
  title         = {{Reinforcement learning: A survey}},
  year          = {1996},
  issn          = {10769757},
  month         = {apr},
  pages         = {237--285},
  volume        = {4},
  abstract      = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
  archiveprefix = {arXiv},
  arxivid       = {cs/9605103},
  doi           = {10.1613/jair.301},
  eprint        = {9605103},
  primaryclass  = {cs},
  url           = {https://arxiv.org/abs/cs/9605103},
}

@Article{art/CordonnierJ_201911,
  author        = {Cordonnier, Jean Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal       = {arXiv},
  title         = {{On the relationship between self-attention and convolutional layers}},
  year          = {2019},
  issn          = {23318422},
  month         = {nov},
  abstract      = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: Do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {1911.03584},
  eprint        = {1911.03584},
  url           = {http://arxiv.org/abs/1911.03584},
}

@Article{art/TobonGomezC_2015,
  author   = {Tobon-Gomez, Catalina and Geers, Arjan J. and Peters, Jochen and Weese, J{\"{u}}rgen and Pinto, Karen and Karim, Rashed and Ammar, Mohammed and Daoudi, Abdelaziz and Margeta, Jan and Sandoval, Zulma and Stender, Birgit and Zheng, Yefeng and Zuluaga, Maria A. and Betancur, Julian and Ayache, Nicholas and Chikh, Mohammed Amine and Dillenseger, Jean Louis and Kelm, B. Michael and Mahmoudi, Sa{\"{i}}d and Ourselin, S{\'{e}}bastien and Schlaefer, Alexander and Schaeffter, Tobias and Razavi, Reza and Rhode, Kawal S.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Benchmark for Algorithms Segmenting the Left Atrium From 3D CT and MRI Datasets}},
  year     = {2015},
  issn     = {1558254X},
  number   = {7},
  pages    = {1460--1473},
  volume   = {34},
  abstract = {Knowledge of left atrial (LA) anatomy is important for atrial fibrillation ablation guidance, fibrosis quantification and biophysical modelling. Segmentation of the LA from Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) images is a complex problem. This manuscript presents a benchmark to evaluate algorithms that address LA segmentation. The datasets, ground truth and evaluation code have been made publicly available through the http://www.cardiacatlas.org website. This manuscript also reports the results of the Left Atrial Segmentation Challenge (LASC) carried out at the STACOM'13 workshop, in conjunction with MICCAI'13. Thirty CT and 30 MRI datasets were provided to participants for segmentation. Each participant segmented the LA including a short part of the LA appendage trunk and proximal sections of the pulmonary veins (PVs). We present results for nine algorithms for CT and eight algorithms for MRI. Results showed that methodologies combining statistical models with region growing approaches were the most appropriate to handle the proposed task. The ground truth and automatic segmentations were standardised to reduce the influence of inconsistently defined regions (e.g., mitral plane, PVs end points, LA appendage). This standardisation framework, which is a contribution of this work, can be used to label and further analyse anatomical regions of the LA. By performing the standardisation directly on the left atrial surface, we can process multiple input data, including meshes exported from different electroanatomical mapping systems.},
  doi      = {10.1109/TMI.2015.2398818},
  keywords = {Image segmentation,benchmark testing,cardiovascular disease,computed tomography,left atrium,magnetic resonance imaging},
}

@Article{art/LoP_2012,
  author   = {Lo, Pechin and {Van Ginneken}, Bram and Reinhardt, Joseph M. and Yavarna, Tarunashree and {De Jong}, Pim A. and Irving, Benjamin and Fetita, Catalin and Ortner, Margarete and Pinho, R{\^{o}}mulo and Sijbers, Jan and Feuerstein, Marco and Fabijanska, Anna and Bauer, Christian and Beichel, Reinhard and Mendoza, Carlos S. and Wiemker, Rafael and Lee, Jaesung and Reeves, Anthony P. and Born, Silvia and Weinheimer, Oliver and {Van Rikxoort}, Eva M. and Tschirren, Juerg and Mori, Ken and Odry, Benjamin and Naidich, David P. and Hartmann, Ieneke and Hoffman, Eric A. and Prokop, Mathias and Pedersen, Jesper H. and {De Bruijne}, Marleen},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Extraction of airways from CT (EXACT'09)}},
  year     = {2012},
  issn     = {02780062},
  number   = {11},
  pages    = {2093--2107},
  volume   = {31},
  abstract = {This paper describes a framework for establishing a reference airway tree segmentation, which was used to quantitatively evaluate 15 different airway tree extraction algorithms in a standardized manner. Because of the sheer difficulty involved in manually constructing a complete reference standard from scratch, we propose to construct the reference using results from all algorithms that are to be evaluated. We start by subdividing each segmented airway tree into its individual branch segments. Each branch segment is then visually scored by trained observers to determine whether or not it is a correctly segmented part of the airway tree. Finally, the reference airway trees are constructed by taking the union of all correctly extracted branch segments. Fifteen airway tree extraction algorithms from different research groups are evaluated on a diverse set of 20 chest computed tomography (CT) scans of subjects ranging from healthy volunteers to patients with severe pathologies, scanned at different sites, with different CT scanner brands, models, and scanning protocols. Three performance measures covering different aspects of segmentation quality were computed for all participating algorithms. Results from the evaluation showed that no single algorithm could extract more than an average of 74% of the total length of all branches in the reference standard, indicating substantial differences between the algorithms. A fusion scheme that obtained superior results is presented, demonstrating that there is complementary information provided by the different algorithms and there is still room for further improvements in airway segmentation algorithms. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2012.2209674},
  keywords = {Computed tomography,evaluation,pulmonary airways,segmentation},
  pmid     = {22855226},
}

@Article{art/BaloccoS_201403,
  author   = {Balocco, Simone and Gatta, Carlo and Ciompi, Francesco and Wahle, Andreas and Radeva, Petia and Carlier, Stephane and Unal, Gozde and Sanidas, Elias and Mauri, Josepa and Carillo, Xavier and Kovarnik, Tomas and Wang, Ching Wei and Chen, Hsiang Chou and Exarchos, Themis P. and Fotiadis, Dimitrios I. and Destrempes, Fran{\c{c}}ois and Cloutier, Guy and Pujol, Oriol and Alberti, Marina and Mendizabal-Ruiz, E. Gerardo and Rivera, Mariano and Aksoy, Timur and Downe, Richard W. and Kakadiaris, Ioannis A.},
  journal  = {Computerized Medical Imaging and Graphics},
  title    = {{Standardized evaluation methodology and reference database for evaluating IVUS image segmentation}},
  year     = {2014},
  issn     = {08956111},
  month    = {mar},
  number   = {2},
  pages    = {70--90},
  volume   = {38},
  abstract = {This paper describes an evaluation framework that allows a standardized and quantitative comparison of IVUS lumen and media segmentation algorithms. This framework has been introduced at the MICCAI 2011 Computing and Visualization for (Intra)Vascular Imaging (CVII) workshop, comparing the results of eight teams that participated.We describe the available data-base comprising of multi-center, multi-vendor and multi-frequency IVUS datasets, their acquisition, the creation of the reference standard and the evaluation measures. The approaches address segmentation of the lumen, the media, or both borders; semi- or fully-automatic operation; and 2-D vs. 3-D methodology. Three performance measures for quantitative analysis have been proposed. The results of the evaluation indicate that segmentation of the vessel lumen and media is possible with an accuracy that is comparable to manual annotation when semi-automatic methods are used, as well as encouraging results can be obtained also in case of fully-automatic segmentation. The analysis performed in this paper also highlights the challenges in IVUS segmentation that remains to be solved. {\textcopyright} 2013 Elsevier Ltd.},
  doi      = {10.1016/j.compmedimag.2013.07.001},
  keywords = {Algorithm comparison,Evaluation framework,IVUS (intravascular ultrasound),Image segmentation},
  pmid     = {24012215},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0895611113001298},
}

@Book{art/KainzB_2017,
  author    = {Kainz, Bernhard and Bhatia, Kanwal and Vaillant, Ghislain and Zuluaga, Maria A.},
  editor    = {Zuluaga, Maria A. and Bhatia, Kanwal and Kainz, Bernhard and Moghari, Mehdi H. and Pace, Danielle F.},
  publisher = {Springer International Publishing},
  title     = {{Preface}},
  year      = {2017},
  address   = {Cham},
  isbn      = {9783319522791},
  series    = {Lecture Notes in Computer Science},
  volume    = {10129 LNCS},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi       = {10.1007/978-3-319-52280-7},
  issn      = {16113349},
  url       = {http://link.springer.com/10.1007/978-3-319-52280-7},
}

@InProceedings{art/PaceD_2015,
  author    = {Pace, Danielle F. and Dalca, Adrian V. and Geva, Tal and Powell, Andrew J. and Moghari, Mehdi H. and Golland, Polina},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Interactive whole-heart segmentation in congenital heart disease}},
  year      = {2015},
  pages     = {80--88},
  volume    = {9351},
  abstract  = {We present an interactive algorithm to segment the heart chambers and epicardial surfaces, including the great vessel walls, in pediatric cardiac MRI of congenital heart disease. Accurate whole-heart segmentation is necessary to create patient-specific 3D heart models for surgical planning in the presence of complex heart defects. Anatomical variability due to congenital defects precludes fully automatic atlas-based segmentation. Our interactive segmentation method exploits expert segmentations of a small set of short-axis slice regions to automatically delineate the remaining volume using patch-based segmentation. We also investigate the potential of active learning to automatically solicit user input in areas where segmentation error is likely to be high. Validation is performed on four subjects with double outlet right ventricle, a severe congenital heart defect. We show that strategies asking the user to manually segment regions of interest within short-axis slices yield higher accuracy with less user input than those querying entire short-axis slices.},
  doi       = {10.1007/978-3-319-24574-4_10},
  isbn      = {9783319245737},
  issn      = {16113349},
}

@Article{art/TrulloR_2019,
  author   = {Trullo, Roger and Petitjean, Caroline and Dubray, Bernard and Ruan, Su},
  journal  = {Journal of Medical Imaging},
  title    = {{Multiorgan segmentation using distance-aware adversarial networks}},
  year     = {2019},
  issn     = {2329-4302},
  number   = {01},
  pages    = {1},
  volume   = {6},
  abstract = {{\textcopyright} 2019 Society of Photo-Optical Instrumentation Engineers (SPIE). Segmentation of organs at risk (OAR) in computed tomography (CT) is of vital importance in radiotherapy treatment. This task is time consuming and for some organs, it is very challenging due to low-intensity contrast in CT. We propose a framework to perform the automatic segmentation of multiple OAR: esophagus, heart, trachea, and aorta. Different from previous works using deep learning techniques, we make use of global localization information, based on an original distance map that yields not only the localization of each organ, but also the spatial relationship between them. Instead of segmenting directly the organs, we first generate the localization map by minimizing a reconstruction error within an adversarial framework. This map that includes localization information of all organs is then used to guide the segmentation task in a fully convolutional setting. Experimental results show encouraging performance on CT scans of 60 patients totaling 11,084 slices in comparison with other state-of-the-art methods.},
  doi      = {10.1117/1.jmi.6.1.014001},
}

@Article{art/KavurA_202001,
  author   = {Kavur, A. Emre and Gezer, Naciye Sinem and Barış, Mustafa and Şahin, Yusuf and {\"{O}}zkan, Savaş and Baydar, Bora and Y{\"{u}}ksel, Ulaş and Kılık{\c{c}}ıer, {\c{C}}ağlar and Olut, Şahin and Akar, G{\"{o}}zde Bozdağı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oğuz and Selver, M. Alper},
  journal  = {Diagnostic and Interventional Radiology},
  title    = {{Comparison of semi-automatic and deep learning-based automatic methods for liver segmentation in living liver transplant donors}},
  year     = {2020},
  issn     = {13053612},
  month    = {jan},
  number   = {1},
  pages    = {11--21},
  volume   = {26},
  abstract = {PURPOSE We aimed to compare the accuracy and repeatability of emerging machine learning-based (i.e., deep learning) automatic segmentation algorithms with those of well-established interactive semi-automatic methods for determining liver volume in living liver transplant donors at computed tomography (CT) imaging. METHODS A total of 12 methods (6 semi-automatic, 6 full-automatic) were evaluated. The semi-automatic segmentation algorithms were based on both traditional iterative models including watershed, fast marching, region growing, active contours and modern techniques including robust statistics segmenter and super-pixels. These methods entailed some sort of interaction mechanism such as placing initialization seeds on images or determining a parameter range. The automatic methods were based on deep learning and included three framework templates (DeepMedic, NiftyNet and U-Net), the first two of which were applied with default parameter sets and the last two involved adapted novel model designs. For 20 living donors (8 training and 12 test data-sets), a group of imaging scientists and radiologists created ground truths by performing manual segmentations on contrast-enhanced CT images. Each segmentation was evaluated using five metrics (i.e., volume overlap and relative volume errors, average/root-mean-square/maximum symmetrical surface distances). The results were mapped to a scoring system and a final grade was calculated by taking their average. Accuracy and repeatability were evaluated using slice-by-slice comparisons and volumetric analysis. Diversity and complementarity were observed through heatmaps. Majority voting (MV) and simultaneous truth and performance level estimation (STAPLE) algorithms were utilized to obtain the fusion of the individual results. RESULTS The top four methods were automatic deep learning models, with scores of 79.63, 79.46, 77.15, and 74.50. Intra-user score was determined as 95.14. Overall, automatic deep learning segmentation outperformed interactive techniques on all metrics. The mean volume of liver of ground truth was 1409.93±271.28 mL, while it was calculated as 1342.21±231.24 mL using automatic and 1201.26±258.13 mL using interactive methods, showing higher accuracy and less variation with automatic methods. The qualitative analysis of segmentation results showed significant diversity and complementarity, enabling the idea of using ensembles to obtain superior results. The fusion score of automatic methods reached 83.87 with MV and 86.20 with STAPLE, which were only slightly less than fusion of all methods (MV, 86.70) and (STAPLE, 88.74). CONCLUSION Use of the new deep learning-based automatic segmentation algorithms substantially increases the accuracy and repeatability for segmentation and volumetric measurements of liver. Fusion of automatic methods based on ensemble approaches exhibits best results with almost no additional time cost due to potential parallel execution of multiple models.},
  doi      = {10.5152/dir.2019.19025},
  pmid     = {31904568},
  url      = {https://www.dirjournal.org/en/comparison-of-semi-automatic-and-deep-learning-based-automatic-methods-for-liver-segmentation-in-living-liver-transplant-donors-132076},
}

@Article{art/JimenezDelToroO_201611,
  author   = {Jimenez-Del-Toro, Oscar and Muller, Henning and Krenn, Markus and Gruenberg, Katharina and Taha, Abdel Aziz and Winterstein, Marianne and Eggel, Ivan and Foncubierta-Rodriguez, Antonio and Goksel, Orcun and Jakab, Andras and Kontokotsios, Georgios and Langs, Georg and Menze, Bjoern H. and {Salas Fernandez}, Tomas and Schaer, Roger and Walleyo, Anna and Weber, Marc Andre and {Dicente Cid}, Yashin and Gass, Tobias and Heinrich, Mattias and Jia, Fucang and Kahl, Fredrik and Kechichian, Razmig and Mai, Dominic and Spanier, Assaf B. and Vincent, Graham and Wang, Chunliang and Wyeth, Daniel and Hanbury, Allan},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Cloud-Based Evaluation of Anatomical Structure Segmentation and Landmark Detection Algorithms: VISCERAL Anatomy Benchmarks}},
  year     = {2016},
  issn     = {1558254X},
  month    = {nov},
  number   = {11},
  pages    = {2459--2475},
  volume   = {35},
  abstract = {Variations in the shape and appearance of anatomical structures in medical images are often relevant radiological signs of disease. Automatic tools can help automate parts of this manual process. A cloud-based evaluation framework is presented in this paper including results of benchmarking current state-of-the-art medical imaging algorithms for anatomical structure segmentation and landmark detection: the VISCERAL Anatomy benchmarks. The algorithms are implemented in virtual machines in the cloud where participants can only access the training data and can be run privately by the benchmark administrators to objectively compare their performance in an unseen common test set. Overall, 120 computed tomography and magnetic resonance patient volumes were manually annotated to create a standard Gold Corpus containing a total of 1295 structures and 1760 landmarks. Ten participants contributed with automatic algorithms for the organ segmentation task, and three for the landmark localization task. Different algorithms obtained the best scores in the four available imaging modalities and for subsets of anatomical structures. The annotation framework, resulting data set, evaluation setup, results and performance analysis from the three VISCERAL Anatomy benchmarks are presented in this article. Both the VISCERAL data set and Silver Corpus generated with the fusion of the participant algorithms on a larger set of non-manually-annotated medical images are available to the research community.},
  doi      = {10.1109/TMI.2016.2578680},
  keywords = {Evaluation framework,landmark detection,organ segmentation},
  pmid     = {27305669},
  url      = {http://ieeexplore.ieee.org/document/7488206/},
}

@Misc{art/YangJ_2017,
  author    = {Yang, Jinzhong and Sharp, Greg and Veeraraghavan, Harini and {Van Elmpt}, Wouter and Dekker, Andre and Lustberg, Tim and Gooding, Mark},
  title     = {{Data from Lung CT Segmentation Challenge}},
  year      = {2017},
  doi       = {10.7937/K9/TCIA.2017.3R3FVZ08},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/e41yAQ},
}

@Article{art/SimpsonA_201902,
  author        = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {Van Ginneken}, Bram and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K.G. and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and {Jorge Cardoso}, M.},
  journal       = {arXiv},
  title         = {{A large annotated medical image dataset for the development and evaluation of segmentation algorithms}},
  year          = {2019},
  month         = {feb},
  abstract      = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
  archiveprefix = {arXiv},
  arxivid       = {1902.09063},
  eprint        = {1902.09063},
  url           = {http://arxiv.org/abs/1902.09063},
}

@Misc{art/ZhaoJ_,
  author = {Zhao, Jichao and Xiong, Zhaohan},
  title  = {{2018 Atrial Segmentation Challenge – Atrial Segmentation Challenge}},
  url    = {http://atriaseg2018.cardiacatlas.org/},
}

@Article{art/LeclercS_201909,
  author        = {Leclerc, Sarah and Smistad, Erik and Pedrosa, Joao and Ostvik, Andreas and Cervenansky, Frederic and Espinosa, Florian and Espeland, Torvald and Berg, Erik Andreas Rye and Jodoin, Pierre Marc and Grenier, Thomas and Lartizien, Carole and Dhooge, Jan and Lovstakken, Lasse and Bernard, Olivier},
  journal       = {IEEE transactions on medical imaging},
  title         = {{Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography}},
  year          = {2019},
  issn          = {1558254X},
  month         = {sep},
  number        = {9},
  pages         = {2198--2210},
  volume        = {38},
  abstract      = {Delineation of the cardiac structures from 2D echocardiographic images is a common clinical task to establish a diagnosis. Over the past decades, the automation of this task has been the subject of intense research. In this paper, we evaluate how far the state-of-the-art encoder-decoder deep convolutional neural network methods can go at assessing 2D echocardiographic images, i.e., segmenting cardiac structures and estimating clinical indices, on a dataset, especially, designed to answer this objective. We, therefore, introduce the cardiac acquisitions for multi-structure ultrasound segmentation dataset, the largest publicly-available and fully-annotated dataset for the purpose of echocardiographic assessment. The dataset contains two and four-chamber acquisitions from 500 patients with reference measurements from one cardiologist on the full dataset and from three cardiologists on a fold of 50 patients. Results show that encoder-decoder-based architectures outperform state-of-the-art non-deep learning methods and faithfully reproduce the expert analysis for the end-diastolic and end-systolic left ventricular volumes, with a mean correlation of 0.95 and an absolute mean error of 9.5 ml. Concerning the ejection fraction of the left ventricle, results are more contrasted with a mean correlation coefficient of 0.80 and an absolute mean error of 5.6%. Although these results are below the inter-observer scores, they remain slightly worse than the intra-observer's ones. Based on this observation, areas for improvement are defined, which open the door for accurate and fully-automatic analysis of 2D echocardiographic images.},
  archiveprefix = {arXiv},
  arxivid       = {1908.06948},
  doi           = {10.1109/TMI.2019.2900516},
  eprint        = {1908.06948},
  pmid          = {30802851},
  url           = {https://ieeexplore.ieee.org/document/8649738/},
}

@Misc{art/RisterB_2019,
  author    = {Rister, Blaine and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
  title     = {{CT-ORG: A Dataset of CT Volumes With Multiple Organ Segmentations}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.TT7F4V7O},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/OgWkAw},
}

@Misc{art/BlochN_2015,
  author    = {Bloch, N.},
  title     = {{Nc-isbi 2013 challenge: automated segmentation of prostate structures}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.ZF0VLOPV},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/B4NEAQ},
}

@Misc{art/BlochB_2015a,
  author    = {Bloch, B. Nicolas and Jain, Ashali and Jaffe, C. Carl},
  title     = {{Data From PROSTATE-DIAGNOSIS}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2015.FOQEUJVT},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/xgEy},
}

@Article{art/ZhuangX_2019,
  author        = {Zhuang, Xiahai},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source Images}},
  year          = {2019},
  issn          = {19393539},
  number        = {12},
  pages         = {2933--2946},
  volume        = {41},
  abstract      = {The author proposes a method for simultaneous registration and segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. Specifically, the method is applied to the problem of myocardial segmentation combining the complementary information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. For the image misalignment and incongruent data, the MvMM is formulated with transformations and is further generalized for dealing with the hetero-coverage multi-modality images (HC-MMIs). The segmentation of MvMM is performed in a virtual common space, to which all the images and misaligned slices are simultaneously registered. Furthermore, this common space can be divided into a number of sub-regions, each of which contains congruent data, thus the HC-MMIs can be modeled using a set of conventional MvMMs. Results show that MvMM obtained significantly better performance compared to the conventional approaches and demonstrated good potential for scar quantification as well as myocardial segmentation. The generalized MvMM has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest, and the full coverage can only be reconstructed combining the images from multiple sources.},
  archiveprefix = {arXiv},
  arxivid       = {1612.08820},
  doi           = {10.1109/TPAMI.2018.2869576},
  eprint        = {1612.08820},
  keywords      = {Multivariate image,cardiac MRI,medical image analysis,multi-modality,registration,segmentation},
  pmid          = {30207950},
}

@InProceedings{art/ZhuangX_2016,
  author    = {Zhuang, Xiahai},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Multivariate mixture model for cardiac segmentation from multi-sequence MRI}},
  year      = {2016},
  pages     = {581--588},
  volume    = {9901 LNCS},
  abstract  = {Cardiac segmentation is commonly a prerequisite for functional analysis of the heart,such as to identify and quantify the infarcts and edema from the normal myocardium using the late-enhanced (LE) and T2-weighted MRI. The automatic delineation of myocardium is however challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this work,we present a multivariate mixture model (MvMM) for text classification,which combines the complementary information from multi-sequence (MS) cardiac MRI and perform the segmentation of them simultaneously. The expectation maximization (EM) method is adopted to estimate the segmentation and model parameters from the log-likelihood (LL) of the mixture model,where a probabilistic atlas is used for initialization. Furthermore,to correct the intra- and inter-image misalignments,we formulate the MvMM with transformations,which are embedded into the LL framework and thus can be optimized by the iterative conditional mode approach. We applied MvMM for segmentation of eighteen subjects with three sequences and obtained promising results. We compared with two conventional methods,and the improvements of segmentation performance on LE and T2 MRI were evident and statistically significant by MvMM.},
  doi       = {10.1007/978-3-319-46723-8_67},
  isbn      = {9783319467221},
  issn      = {16113349},
}

@Misc{art/HellerN_2019,
  author    = {Heller, N. and Sathianathen, N. and {Kalapara, A., Walczak}, E. and Moore, K. and Kaluzniak, H. and Rosenberg, J. and Blake, P. and Rengel, Z. and Oestreich, M. and Dean, J. and Tradewell, M. and Shah, A. and Tejpaul, R. and Edgerton, Z. and Peterson, M. and Raza, S. and Regmi, S. and Papanikolopoulos, N. and Weight, C.},
  title     = {{Data from C4KC-KiTS [Data set]}},
  year      = {2019},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/TCIA.2019.IX49E8NX},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/UwakAw},
}

@InCollection{art/RothH_2015,
  author        = {Roth, Holger R. and Lu, Le and Farag, Amal and Shin, Hoo Chang and Liu, Jiamin and Turkbey, Evrim B. and Summers, Ronald M.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation}},
  year          = {2015},
  pages         = {556--564},
  volume        = {9349},
  abstract      = {Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-tofine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via P-ConvNet and nearest neighbor fusion. Then we describe a regional ConvNet (R1−ConvNet) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a “zoom-out” fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked R2−ConvNet leveraging the joint space of CT intensities and the P−ConvNet dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6±6.3% in training and 71.8±10.7% in testing.},
  archiveprefix = {arXiv},
  arxivid       = {1506.06448},
  doi           = {10.1007/978-3-319-24553-9_68},
  eprint        = {1506.06448},
  issn          = {16113349},
}

@Misc{art/RothH_2016,
  author    = {Roth, Holger and Farag, Amal and Turkbey, Evrim B and Lu, Le and Liu, Jiamin and Summers, Ronald M},
  title     = {{Data From Pancreas-CT}},
  year      = {2016},
  doi       = {10.7937/K9/TCIA.2016.TNB1KQBU},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/eIlXAQ},
}

@InProceedings{art/StirenkoS_2018,
  author    = {Stirenko, Sergii and Kochura, Yuriy and Alienin, Oleg and Rokovyi, Oleksandr and Gordienko, Yuri and Gang, Peng and Zeng, Wei},
  booktitle = {2018 IEEE 38th International Conference on Electronics and Nanotechnology, ELNANO 2018 - Proceedings},
  title     = {{Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation}},
  year      = {2018},
  pages     = {422--428},
  abstract  = {The results of chest X-ray (CXR) analysis of 2D images to get the statistically reliable predictions (availability of tuberculosis) by computer-aided diagnosis (CADx) on the basis of deep learning are presented. They demonstrate the efficiency of lung segmentation, lossless and lossy data augmentation for CADx of tuberculosis by deep convolutional neural network (CNN) applied to the small and not well-balanced dataset even. CNN demonstrates ability to train (despite overfitting) on the pre-processed dataset obtained after lung segmentation in contrast to the original not-segmented dataset. Lossless data augmentation of the segmented dataset leads to the lowest validation loss (without overfitting) and nearly the same accuracy (within the limits of standard deviation) in comparison to the original and other pre-processed datasets after lossy data augmentation. The additional limited lossy data augmentation results in the lower validation loss, but with a decrease of the validation accuracy. In conclusion, besides the more complex deep CNNs and bigger datasets, the better progress of CADx for the small and not well-balanced datasets even could be obtained by better segmentation, data augmentation, dataset stratification, and exclusion of non-evident outliers.},
  doi       = {10.1109/ELNANO.2018.8477564},
  isbn      = {9781538663837},
  keywords  = {TensorFlow,chest X-ray,computer-aided diagnosis,convolutional neural network,data augmentation,deep learning,lung,mask,open dataset,segmentation,tuberculosis},
}

@Misc{art/NewittD_2016,
  author    = {Newitt, David and Hylton, Nola},
  title     = {{Single site breast DCE-MRI data and segmentations from patients undergoing neoadjuvant chemotherapy}},
  year      = {2016},
  doi       = {10.7937/K9/TCIA.2016.QHsyhJKy},
  pages     = {The Cancer Imaging Archive},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/ZIhXAQ},
}

@Article{art/CandemirS_2014,
  author   = {Candemir, Sema and Jaeger, Stefan and Palaniappan, Kannappan and Musco, Jonathan P. and Singh, Rahul K. and Xue, Zhiyun and Karargyris, Alexandros and Antani, Sameer and Thoma, George and McDonald, Clement J.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration}},
  year     = {2014},
  issn     = {02780062},
  number   = {2},
  pages    = {577--590},
  volume   = {33},
  abstract = {The National Library of Medicine (NLM) is developing a digital chest X-ray (CXR) screening system for deployment in resource constrained communities and developing countries worldwide with a focus on early detection of tuberculosis. A critical component in the computer-aided diagnosis of digital CXRs is the automatic detection of the lung regions. In this paper, we present a nonrigid registration-driven robust lung segmentation method using image retrieval-based patient specific adaptive lung models that detects lung boundaries, surpassing state-of-the-art performance. The method consists of three main stages: 1) a content-based image retrieval approach for identifying training images (with masks) most similar to the patient CXR using a partial Radon transform and Bhattacharyya shape similarity measure, 2) creating the initial patient-specific anatomical model of lung shape using SIFT-flow for deformable registration of training masks to the patient CXR, and 3) extracting refined lung boundaries using a graph cuts optimization approach with a customized energy function. Our average accuracy of 95.4% on the public JSRT database is the highest among published results. A similar degree of accuracy of 94.1% and 91.7% on two new CXR datasets from Montgomery County, MD, USA, and India, respectively, demonstrates the robustness of our lung segmentation approach. {\textcopyright} 2013 IEEE.},
  doi      = {10.1109/TMI.2013.2290491},
  keywords = {Chest X-ray imaging,Computer-aided detection,Image registration,Image segmentation,Tuberculosis (TB)},
  pmid     = {24239990},
}

@Article{art/JaegerS_2014,
  author   = {Jaeger, Stefan and Karargyris, Alexandros and Candemir, Sema and Folio, Les and Siegelman, Jenifer and Callaghan, Fiona and Xue, Zhiyun and Palaniappan, Kannappan and Singh, Rahul K. and Antani, Sameer and Thoma, George and Wang, Yi Xiang and Lu, Pu Xuan and McDonald, Clement J.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Automatic tuberculosis screening using chest radiographs}},
  year     = {2014},
  issn     = {02780062},
  number   = {2},
  pages    = {233--245},
  volume   = {33},
  abstract = {Tuberculosis is a major health threat in many regions of the world. Opportunistic infections in immunocompromised HIV/AIDS patients and multi-drug-resistant bacterial strains have exacerbated the problem, while diagnosing tuberculosis still remains a challenge. When left undiagnosed and thus untreated, mortality rates of patients with tuberculosis are high. Standard diagnostics still rely on methods developed in the last century. They are slow and often unreliable. In an effort to reduce the burden of the disease, this paper presents our automated approach for detecting tuberculosis in conventional posteroanterior chest radiographs. We first extract the lung region using a graph cut segmentation method. For this lung region, we compute a set of texture and shape features, which enable the X-rays to be classified as normal or abnormal using a binary classifier. We measure the performance of our system on two datasets: a set collected by the tuberculosis control program of our local county's health department in the United States, and a set collected by Shenzhen Hospital, China. The proposed computer-aided diagnostic system for TB screening, which is ready for field deployment, achieves a performance that approaches the performance of human experts. We achieve an area under the ROC curve (AUC) of 87% (78.3% accuracy) for the first set, and an AUC of 90% (84% accuracy) for the second set. For the first set, we compare our system performance with the performance of radiologists. When trying not to miss any positive cases, radiologists achieve an accuracy of about 82% on this set, and their false positive rate is about half of our system's rate. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2013.2284099},
  keywords = {Computer-aided detection and diagnosis,X-ray imaging,lung,pattern recognition and classification,segmentation,tuberculosis (TB)},
  pmid     = {24108713},
}

@InProceedings{art/SeffA_2014,
  author        = {Seff, Ari and Lu, Le and Cherry, Kevin M. and Roth, Holger R. and Liu, Jiamin and Wang, Shijun and Hoffman, Joanne and Turkbey, Evrim B. and Summers, Ronald M.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{2D view aggregation for lymph node detection using a shallow hierarchy of linear classifiers}},
  year          = {2014},
  number        = {PART 1},
  pages         = {544--552},
  volume        = {8673 LNCS},
  abstract      = {Enlarged lymph nodes (LNs) can provide important information for cancer diagnosis, staging, and measuring treatment reactions, making automated detection a highly sought goal. In this paper, we propose a new algorithm representation of decomposing the LN detection problem into a set of 2D object detection subtasks on sampled CT slices, largely alleviating the curse of dimensionality issue. Our 2D detection can be effectively formulated as linear classification on a single image feature type of Histogram of Oriented Gradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We exploit both max-pooling and sparse linear fusion schemes to aggregate these 2D detection scores for the final 3D LN detection. In this manner, detection is more tractable and does not need to perform perfectly at instance level (as weak hypotheses) since our aggregation process will robustly harness collective information for LN detection. Two datasets (90 patients with 389 mediastinal LNs and 86 patients with 595 abdominal LNs) are used for validation. Cross-validation demonstrates 78.0% sensitivity at 6 false positives/volume (FP/vol.) (86.1% at 10 FP/vol.) and 73.1% sensitivity at 6 FP/vol. (87.2% at 10 FP/vol.), for the mediastinal and abdominal datasets respectively. Our results compare favorably to previous state-of-the-art methods. {\textcopyright} 2014 Springer International Publishing.},
  archiveprefix = {arXiv},
  arxivid       = {1408.3337},
  doi           = {10.1007/978-3-319-10404-1_68},
  eprint        = {1408.3337},
  isbn          = {9783319104034},
  issn          = {16113349},
  pmid          = {25333161},
}

@InProceedings{art/RothH_2014,
  author        = {Roth, Holger R. and Lu, Le and Seff, Ari and Cherry, Kevin M. and Hoffman, Joanne and Wang, Shijun and Liu, Jiamin and Turkbey, Evrim and Summers, Ronald M.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations}},
  year          = {2014},
  number        = {PART 1},
  pages         = {520--527},
  volume        = {8673 LNCS},
  abstract      = {Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9% sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9% at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards ∼100% sensitivity at the cost of high FP levels (∼40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70%/83% at 3 FP/vol. and 84%/90% at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work. {\textcopyright} 2014 Springer International Publishing.},
  archiveprefix = {arXiv},
  arxivid       = {1406.2639},
  doi           = {10.1007/978-3-319-10404-1_65},
  eprint        = {1406.2639},
  isbn          = {9783319104034},
  issn          = {16113349},
  pmid          = {25333158},
}

@Misc{art/HolgerR_2015,
  author    = {Holger, Roth and Lu, Le and Seff, Ari and Cherry, Kevin M and Hoffman, Joanne and Wang, Shijun and Summers, Ronald M},
  title     = {{A new 2.5 D representation for lymph node detection in CT.}},
  year      = {2015},
  booktitle = {The Cancer Imaging Archive.},
  doi       = {10.7937/K9/TCIA.2015.AQIIDCNM},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/0gAtAQ},
}

@Misc{art/LitjensGeertFuttererJ_2015,
  author    = {{Litjens Geert; Futterer}, Jurgen; Huisman Henkjan;},
  title     = {{Data From Prostate-3T}},
  year      = {2015},
  doi       = {10.7937/K9/TCIA.2015.QJTV5IL5},
  publisher = {The Cancer Imaging Archive},
  url       = {http://dx.doi.org/10.7937/K9/TCIA.2015.QJTV5IL5},
}

@InProceedings{art/SeffA_2015,
  author    = {Seff, Ari and Lu, Le and Barbu, Adrian and Roth, Holger and Shin, Hoo Chang and Summers, Ronald M.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Leveraging mid-level semantic boundary cues for automated lymph node detection}},
  year      = {2015},
  abstract  = {Histograms of oriented gradients (HOG) are widely employed image descriptors in modern computer-aided diagnosis systems. Built upon a set of local, robust statistics of low-level image gradients, HOG features are usually computed on raw intensity images. In this paper, we explore a learned image transformation scheme for producing higher-level inputs to HOG. Leveraging semantic object boundary cues, our methods compute data-driven image feature maps via a supervised boundary detector. Compared with the raw image map, boundary cues offer mid-level, more object-specific visual responses that can be suited for subsequent HOG encoding. We validate integrations of several image transformation maps with an application of computer-aided detection of lymph nodes on thoracoabdominal CT images. Our experiments demonstrate that semantic boundary cues based HOG descriptors complement and enrich the raw intensity alone. We observe an overall system with substantially improved results (∼ 78% versus 60% recall at 3 FP/volume for two target regions). The proposed system also moderately outperforms the state-of-the-art deep convolutional neural network (CNN) system in the mediastinum region, without relying on data augmentation and requiring significantly fewer training samples.},
  doi       = {10.1007/978-3-319-24571-3_7},
  isbn      = {9783319245706},
  issn      = {16113349},
}

@Article{art/DeselaersT_2008,
  author   = {Deselaers, Thomas and Deserno, Thomas M. and M{\"{u}}ller, Henning},
  journal  = {Pattern Recognition Letters},
  title    = {{Automatic medical image annotation in ImageCLEF 2007: Overview, results, and discussion}},
  year     = {2008},
  issn     = {01678655},
  number   = {15},
  pages    = {1988--1995},
  volume   = {29},
  abstract = {In this paper, the automatic medical annotation task of the 2007 CLEF cross language image retrieval campaign (ImageCLEF) is described. The paper focusses on the images used, the task setup, and the results obtained in the evaluation campaign. Since 2005, the medical automatic image annotation task exists in ImageCLEF with increasing complexity to evaluate the performance of state-of-the-art methods for completely automatic annotation of medical images based on visual properties. The paper also describes the evolution of the task from its origin in 2005-2007. The 2007 task, comprising 11,000 fully annotated training images and 1000 test images to be annotated, is a realistic task with a large number of possible classes at different levels of detail. Detailed analysis of the methods across participating groups is presented with respect to the (i) image representation, (ii) classification method, and (iii) use of the class hierarchy. The results show that methods which build on local image descriptors and discriminative models are able to provide good predictions of the image classes, mostly by using techniques that were originally developed in the machine learning and computer vision domain for object recognition in non-medical images. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.patrec.2008.03.001},
  keywords = {Automatic image annotation,Benchmark,Evaluation,Medical images},
}

@Article{art/FuH_2020,
  author        = {Fu, Huazhu and Li, Fei and Sun, Xu and Cao, Xingxing and Liao, Jingan and Orlando, Jos{\'{e}} Ignacio and Tao, Xing and Li, Yuexiang and Zhang, Shihao and Tan, Mingkui and Yuan, Chenglang and Bian, Cheng and Xie, Ruitao and Li, Jiongcheng and Li, Xiaomeng and Wang, Jing and Geng, Le and Li, Panming and Hao, Huaying and Liu, Jiang and Kong, Yan and Ren, Yongyong and Bogunovi{\'{c}}, Hrvoje and Zhang, Xiulan and Xu, Yanwu},
  journal       = {arXiv},
  title         = {{AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography}},
  year          = {2020},
  abstract      = {Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually leads to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits the progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large data of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results in the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixel (10µm) in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved the satisfactory performances, especially, 100% accuracy rate for top-two performances. These artificial intelligence techniques were shown to have the potential to enable new developments in AS-OCT image analysis and image-based angle closure glaucoma assessment in particular.},
  archiveprefix = {arXiv},
  arxivid       = {2005.02258},
  eprint        = {2005.02258},
  url           = {http://arxiv.org/abs/2005.02258},
}

@Article{art/NiemeijerM_2010,
  author   = {Niemeijer, Meindert and {Van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'{e}}nol{\'{e}} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'{i}}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'{e}}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'{i}}a and Fujita, Hiroshi and Abramoff, Michael D.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Retinopathy online challenge: Automatic detection of microaneurysms in digital color fundus photographs}},
  year     = {2010},
  issn     = {02780062},
  number   = {1},
  pages    = {185--195},
  volume   = {29},
  abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was witheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. Abrmoff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions. {\textcopyright} 2006 IEEE.},
  doi      = {10.1109/TMI.2009.2033909},
  keywords = {Computer aided detection,Computer aided diagnosis,Diabetic retinopathy,Fundus photographs,Retina,Retinopathy Online Challenge (ROC) competition},
  pmid     = {19822469},
}

@Misc{art/WeeL_2019a,
  author    = {Wee, Leonard and Dekker, Andre},
  title     = {{Data from Head-Neck-Radiomics-HN1}},
  year      = {2019},
  doi       = {10.7937/TCIA.2019.8KAP372N},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/iBglAw},
}

@Article{art/FloutyE_201906,
  author        = {Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Luengo, Imanol and Fuentes-Hurtado, Felix and Taleb, Hinde and Barbarisi, Santiago and Quellec, Gwenol{\'{e}} and Stoyanov, Danail},
  journal       = {arXiv},
  title         = {{CaDIS: Cataract dataset for image segmentation}},
  year          = {2019},
  issn          = {23318422},
  month         = {jun},
  abstract      = {Video signals provide a wealth of information about surgical procedures and are the main sensory cue for surgeons. Video processing and understanding can be used to empower computer assisted interventions (CAI) as well as the development of detailed post-operative analysis of the surgical intervention. A fundamental building block to such capabilities is the ability to understand and segment video into semantic labels that differentiate and localize tissue types and different instruments. Deep learning has advanced semantic segmentation techniques dramatically in recent years but is fundamentally reliant on the availability of labelled datasets used to train models. In this paper, we introduce a high quality dataset for semantic segmentation in Cataract surgery. We generated this dataset from the CATARACTS challenge dataset, which is publicly available. To the best of our knowledge, this dataset has the highest quality annotation in surgical data to date. We introduce the dataset and then show the automatic segmentation performance of state-of-the-art models on that dataset as a benchmark.},
  archiveprefix = {arXiv},
  arxivid       = {1906.11586},
  eprint        = {1906.11586},
  url           = {http://arxiv.org/abs/1906.11586},
}

@Article{art/_2020,
  journal  = {International Journal of Engineering and Advanced Technology},
  title    = {{Diabetic Retinopathy Detection}},
  year     = {2020},
  number   = {4},
  pages    = {1022--1026},
  volume   = {9},
  abstract = {Diabetic retinopathy is becoming a more prevalent disease in diabetic patients nowadays. The surprising fact about the disease is it leaves no symptoms at the beginning stage and the patient can realize the disease only when his vision starts to fall. If the disease is not found at the earliest it leads to a stage where the probability of curing the disease is less. But if we find the disease at that stage, the patient might be in a situation of losing the vision completely. Hence, this paper aims at finding the disease at the earliest possible stage by extracting two features from the retinal image namely Microaneurysms which is found to be the starting symptom showing feature and Hemorrhage which shows symptoms of the other stages. Based on these two features we classify the stage of the disease as normal, beginning, mild and severe using convolutional neural network, a deep learning technique which reduces the burden of manual feature extraction and gives higher accuracy. We also locate the position of these features in the disease affected retinal images to help the doctors offer better medical treatment.},
  doi      = {10.35940/ijeat.d7786.049420},
  url      = {https://www.kaggle.com/c/diabetic-retinopathy-detection},
}

@Misc{art/BougetD_2017,
  author    = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
  title     = {{Vision-based and marker-less surgical tool detection and tracking: a review of the literature}},
  year      = {2017},
  abstract  = {In recent years, tremendous progress has been made in surgical practice for example with Minimally Invasive Surgery (MIS). To overcome challenges coming from deported eye-to-hand manipulation, robotic and computer-assisted systems have been developed. Having real-time knowledge of the pose of surgical tools with respect to the surgical camera and underlying anatomy is a key ingredient for such systems. In this paper, we present a review of the literature dealing with vision-based and marker-less surgical tool detection. This paper includes three primary contributions: (1) identification and analysis of data-sets used for developing and testing detection algorithms, (2) in-depth comparison of surgical tool detection methods from the feature extraction process to the model learning strategy and highlight existing shortcomings, and (3) analysis of validation techniques employed to obtain detection performance results and establish comparison between surgical tool detectors. The papers included in the review were selected through PubMed and Google Scholar searches using the keywords: “surgical tool detection”, “surgical tool tracking”, “surgical instrument detection” and “surgical instrument tracking” limiting results to the year range 2000 2015. Our study shows that despite significant progress over the years, the lack of established surgical tool data-sets, and reference format for performance assessment and method ranking is preventing faster improvement.},
  booktitle = {Medical Image Analysis},
  doi       = {10.1016/j.media.2016.09.003},
  issn      = {13618423},
  keywords  = {Data-set,Endoscopic/microscopic images,Object detection,Tool detection,Validation},
  pages     = {633--654},
  pmid      = {27744253},
  volume    = {35},
}

@Article{art/AlHajjH_201902,
  author   = {{Al Hajj}, Hassan and Lamard, Mathieu and Conze, Pierre Henri and Roychowdhury, Soumali and Hu, Xiaowei and Mar{\v{s}}alkaitė, Gabija and Zisimopoulos, Odysseas and Dedmari, Muneer Ahmad and Zhao, Fenqiang and Prellberg, Jonas and Sahu, Manish and Galdran, Adrian and Ara{\'{u}}jo, Teresa and Vo, Duc My and Panda, Chandan and Dahiya, Navdeep and Kondo, Satoshi and Bian, Zhengbing and Vahdat, Arash and Bialopetravi{\v{c}}ius, Jonas and Flouty, Evangello and Qiu, Chenhui and Dill, Sabrina and Mukhopadhyay, Anirban and Costa, Pedro and Aresta, Guilherme and Ramamurthy, Senthil and Lee, Sang Woong and Campilho, Aur{\'{e}}lio and Zachow, Stefan and Xia, Shunren and Conjeti, Sailesh and Stoyanov, Danail and Armaitis, Jogundas and Heng, Pheng Ann and Macready, William G. and Cochener, B{\'{e}}atrice and Quellec, Gwenol{\'{e}}},
  journal  = {Medical Image Analysis},
  title    = {{CATARACTS: Challenge on automatic tool annotation for cataRACT surgery}},
  year     = {2019},
  issn     = {13618423},
  month    = {feb},
  pages    = {24--41},
  volume   = {52},
  abstract = {Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design efficient solutions for surgical workflow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the specific context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the differential analysis of these solutions are discussed. We expect that they will guide the design of efficient surgery monitoring tools in the near future.},
  doi      = {10.1016/j.media.2018.11.008},
  keywords = {Cataract surgery,Challenge,Deep learning,Video analysis},
  pmid     = {30468970},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S136184151830865X},
}

@Misc{art/TrikhaS_2013,
  author    = {Trikha, S. and Turnbull, A. M.J. and Morris, R. J. and Anderson, D. F. and Hossain, P.},
  title     = {{The journey to femtosecond laser-assisted cataract surgery: New beginnings or a false dawn?}},
  year      = {2013},
  abstract  = {Femtosecond laser-assisted cataract surgery (FLACS) represents a potential paradigm shift in cataract surgery, but it is not without controversy. Advocates of the technology herald FLACS as a revolution that promises superior outcomes and an improved safety profile for patients. Conversely, detractors point to the large financial costs involved and claim that similar results are achievable with conventional small-incision phacoemulsification. This review provides a balanced and comprehensive account of the development of FLACS since its inception. It explains the physiology and mechanics underlying the technology, and critically reviews the outcomes and implications of initial studies. The benefits and limitations of using femtosecond laser accuracy to create corneal incisions, anterior capsulotomy, and lens fragmentation are explored, with reference to the main platforms, which currently offer FLACS. Economic considerations are discussed, in addition to the practicalities associated with the implementation of FLACS in a healthcare setting. The influence on surgical training and skills is considered and possible future applications of the technology introduced. While in its infancy, FLACS sets out the exciting possibility of a new level of precision in cataract surgery. However, further work in the form of large scale, phase 3 randomised controlled trials are required to demonstrate whether its theoretical benefits are significant in practice and worthy of the necessary huge financial investment and system overhaul. Whether it gains widespread acceptance is likely to be influenced by a complex interplay of scientific and socio-economic factors in years to come. {\textcopyright} 2013 Macmillan Publishers Limited All rights reserved 0950-222X/13.},
  booktitle = {Eye (Basingstoke)},
  doi       = {10.1038/eye.2012.293},
  issn      = {14765454},
  keywords  = {capsulorhexis,capsulotomy,cataract surgery,femtosecond,laser,phacoemulsfication},
  number    = {4},
  pages     = {461--473},
  volume    = {27},
}

@Misc{art/APTOSA_2019,
  author    = {APTOS, Asia Pacific Tele-Ophthalmology Society},
  title     = {{APTOS 2019 blindness detection}},
  year      = {2019},
  abstract  = {Detect diabetic retinopathy to stop blindness before it's too late},
  booktitle = {Kaggle Competition},
  keywords  = {4th Symposium,Asia Pacific Tele-Ophthalmology Society(APTOS)},
  url       = {https://www.kaggle.com/c/aptos2019-blindness-detection/data},
}

@Article{art/KermanyD_2018,
  author   = {Kermany, Daniel},
  journal  = {Mendeley Data},
  title    = {{Large Dataset of Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images}},
  year     = {2018},
  volume   = {3},
  abstract = {Dataset of validated OCT and Chest X-Ray images described and analyzed        in "Deep learning-based classification and referral of treatable human        diseases". The OCT Images are split into a training set and a testing        set of independent patients. OCT Images are labeled as        (disease)-(randomized patient ID)-(image number by this patient) and        split into 4 directories: CNV, DME, DRUSEN, and NORMAL.},
  doi      = {10.17632/RSCBJBR9SJ.3},
  url      = {https://data.mendeley.com/datasets/rscbjbr9sj/3},
}

@InCollection{art/YaylaliE_201102,
  author    = {Yaylali, Emine and Ivy, Julie S.},
  booktitle = {Wiley Encyclopedia of Operations Research and Management Science},
  publisher = {John Wiley & Sons, Inc.},
  title     = {{Partially Observable MDPs (POMDPS): Introduction and Examples}},
  year      = {2011},
  address   = {Hoboken, NJ, USA},
  month     = {feb},
  abstract  = {A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process where the states of the model are not completely observable by the decision maker. Noisy observations provide a belief regarding the underlying state, while the decision maker has some control over the progression of the model through the selection of actions. In this article, we introduce POMDPs and discuss the relationship between Markov models and POMDPs. A general POMDP formulation and a wide range of POMDP applications from the literature are also presented.},
  doi       = {10.1002/9780470400531.eorms0646},
  url       = {http://doi.wiley.com/10.1002/9780470400531.eorms0646},
}

@Article{art/HausknechtM_201507,
  author        = {Hausknecht, Matthew and Stone, Peter},
  journal       = {AAAI Fall Symposium - Technical Report},
  title         = {{Deep recurrent q-learning for partially observable MDPs}},
  year          = {2015},
  month         = {jul},
  pages         = {29--37},
  volume        = {FS-15-06},
  abstract      = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  archiveprefix = {arXiv},
  arxivid       = {1507.06527},
  eprint        = {1507.06527},
  isbn          = {9781577357520},
  url           = {http://arxiv.org/abs/1507.06527},
}

@Article{art/KermanyD_201802,
  author   = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C.S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalena and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A.N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
  journal  = {Cell},
  title    = {{Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}},
  year     = {2018},
  issn     = {10974172},
  month    = {feb},
  number   = {5},
  pages    = {1122--1131.e9},
  volume   = {172},
  abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract: [Figure presented] Image-based deep learning classifies macular degeneration and diabetic retinopathy using retinal optical coherence tomography images and has potential for generalized applications in biomedical image interpretation and medical decision making.},
  doi      = {10.1016/j.cell.2018.02.010},
  keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning},
  pmid     = {29474911},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418301545},
}

@Misc{art/DGC_2015,
  author = {{Drive Gran Challenge}},
  title  = {{Digital Retinal Images for Vessel Extraction}},
  year   = {2015},
  url    = {http://www.isi.uu.nl/Research/Databases/DRIVE/},
}

@Misc{art/PorwalP_2018,
  author    = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  title     = {{Indian Diabetic Retinopathy Image Dataset (IDRiD)}},
  year      = {2018},
  abstract  = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting working age population in the world. Recent research has given a better understanding of requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of population with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, the database for this challenge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. Moreover, it is the only dataset constituting typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
  booktitle = {IEEE Dataport},
  doi       = {10.21227/H25W98},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/H25W98},
}

@Misc{art/FuH_2020a,
  author    = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovi{\'{c}}, Hrvoje and Sun, Xu and Liao, Jingan and Xu, Yanwu and Zhang, Shaochong and Zhang, Xiulan},
  title     = {{ADAM: Automatic Detection challenge on Age-related Macular degeneration}},
  year      = {2020},
  doi       = {10.21227/dt4f-rt59},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/dt4f-rt59},
}

@Article{art/ChiuS_201504,
  author   = {Chiu, Stephanie J. and Allingham, Michael J. and Mettu, Priyatham S. and Cousins, Scott W. and Izatt, Joseph A. and Farsiu, Sina},
  journal  = {Biomedical Optics Express},
  title    = {{Kernel regression based segmentation of optical coherence tomography images with diabetic macular edema}},
  year     = {2015},
  issn     = {2156-7085},
  month    = {apr},
  number   = {4},
  pages    = {1172},
  volume   = {6},
  abstract = {{\textcopyright} 2015 Optical Society of America. We present a fully automatic algorithm to identify fluid-filled regions and seven retinal layers on spectral domain optical coherence tomography images of eyes with diabetic macular edema (DME). To achieve this, we developed a kernel regression (KR)-based classification method to estimate fluid and retinal layer positions. We then used these classification estimates as a guide to more accurately segment the retinal layer boundaries using our previously described graph theory and dynamic programming (GTDP) framework. We validated our algorithm on 110 Bscans from ten patients with severe DME pathology, showing an overall mean Dice coefficient of 0.78 when comparing our KR + GTDP algorithm to an expert grader. This is comparable to the inter-observer Dice coefficient of 0.79. The entire data set is available online, including our automatic and manual segmentation results. To the best of our knowledge, this is the first validated, fully-automated, seven-layer and fluid segmentation method which has been applied to real-world images containing severe DME.},
  doi      = {10.1364/boe.6.001172},
  url      = {https://www.osapublishing.org/abstract.cfm?URI=boe-6-4-1172},
}

@Article{art/BogunovicH_201908,
  author   = {Bogunovic, Hrvoje and Venhuizen, Freerk and Klimscha, Sophie and Apostolopoulos, Stefanos and Bab-Hadiashar, Alireza and Bagci, Ulas and Beg, Mirza Faisal and Bekalo, Loza and Chen, Qiang and Ciller, Carlos and Gopinath, Karthik and Gostar, Amirali K. and Jeon, Kiwan and Ji, Zexuan and Kang, Sung Ho and Koozekanani, Dara D. and Lu, Donghuan and Morley, Dustin and Parhi, Keshab K. and Park, Hyoung Suk and Rashno, Abdolreza and Sarunic, Marinko and Shaikh, Saad and Sivaswamy, Jayanthi and Tennakoon, Ruwan and Yadav, Shivin and {De Zanet}, Sandro and Waldstein, Sebastian M. and Gerendas, Bianca S. and Klaver, Caroline and Sanchez, Clara I. and Schmidt-Erfurth, Ursula},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{RETOUCH: The Retinal OCT Fluid Detection and Segmentation Benchmark and Challenge}},
  year     = {2019},
  issn     = {1558254X},
  month    = {aug},
  number   = {8},
  pages    = {1858--1874},
  volume   = {38},
  abstract = {Retinal swelling due to the accumulation of fluid is associated with the most vision-threatening retinal diseases. Optical coherence tomography (OCT) is the current standard of care in assessing the presence and quantity of retinal fluid and image-guided treatment management. Deep learning methods have made their impact across medical imaging, and many retinal OCT analysis methods have been proposed. However, it is currently not clear how successful they are in interpreting the retinal fluid on OCT, which is due to the lack of standardized benchmarks. To address this, we organized a challenge RETOUCH in conjunction with MICCAI 2017, with eight teams participating. The challenge consisted of two tasks: fluid detection and fluid segmentation. It featured for the first time: all three retinal fluid types, with annotated images provided by two clinical centers, which were acquired with the three most common OCT device vendors from patients with two different retinal diseases. The analysis revealed that in the detection task, the performance on the automated fluid detection was within the inter-grader variability. However, in the segmentation task, fusing the automated methods produced segmentations that were superior to all individual methods, indicating the need for further improvements in the segmentation performance.},
  doi      = {10.1109/TMI.2019.2901398},
  keywords = {Evaluation,image classification,image segmentation,optical coherence tomography,retina},
  pmid     = {30835214},
  url      = {https://ieeexplore.ieee.org/document/8653407/},
}

@Misc{art/FuH_2019,
  author    = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovic, Hrvoje and Sun, Xu and Liao, Jingan and XU, Yanwu and ZHANG, Shaochong and ZHANG, Xiulan},
  title     = {{PALM: Pathologic myopia challenge}},
  year      = {2019},
  booktitle = {Proc. IEEE Dataport},
  doi       = {10.21227/55pk-8z03},
  pages     = {1},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/55pk-8z03},
  volume    = {[Online]},
}

@Article{art/PorwalP_201807,
  author   = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
  journal  = {Data},
  title    = {{Indian diabetic retinopathy image dataset (IDRiD): A database for diabetic retinopathy screening research}},
  year     = {2018},
  issn     = {23065729},
  month    = {jul},
  number   = {3},
  pages    = {25},
  volume   = {3},
  abstract = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting the working-age population in the world. Recent research has given a better understanding of the requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of populations with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. It constitutes typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. The dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
  doi      = {10.3390/data3030025},
  keywords = {Diabetic macular edema,Diabetic retinopathy,Retinal fundus images},
  url      = {http://www.mdpi.com/2306-5729/3/3/25},
}

@Article{art/ThomasC_201411,
  author   = {Thomas, Cibu and Ye, Frank Q. and Irfanoglu, M. Okan and Modi, Pooja and Saleem, Kadharbatcha S. and Leopold, David A. and Pierpaoli, Carlo},
  journal  = {Proceedings of the National Academy of Sciences of the United States of America},
  title    = {{Anatomical accuracy of brain connections derived from diffusion MRI tractography is inherently limited}},
  year     = {2014},
  issn     = {10916490},
  month    = {nov},
  number   = {46},
  pages    = {16574--16579},
  volume   = {111},
  abstract = {Tractography based on diffusion-weighted MRI (DWI) is widely used for mapping the structural connections of the human brain. Its accuracy is known to be limited by technical factors affecting in vivo data acquisition, such as noise, artifacts, and data undersampling resulting from scan time constraints. It generally is assumed that improvements in data quality and implementation of sophisticated tractography methods will lead to increasingly accurate maps of human anatomical connections. However, assessing the anatomical accuracy of DWI tractography is difficult because of the lack of independent knowledge of the true anatomical connections in humans. Here we investigate the future prospects of DWI-based connectional imaging by applying advanced tractography methods to an ex vivo DWI dataset of the macaque brain. The results of different tractography methods were compared with maps of known axonal projections from previous tracer studies in the macaque. Despite the exceptional quality of the DWI data, none of the methods demonstrated high anatomical accuracy. The methods that showed the highest sensitivity showed the lowest specificity, and vice versa. Additionally, anatomical accuracy was highly dependent upon parameters of the tractography algorithm, with different optimal values for mapping different pathways. These results suggest that there is an inherent limitation in determining long-range anatomical projections based on voxelaveraged estimates of local fiber orientation obtained from DWI data that is unlikely to be overcome by improvements in data acquisition and analysis alone.},
  doi      = {10.1073/pnas.1405672111},
  pmid     = {25368179},
  url      = {http://www.pnas.org/lookup/doi/10.1073/pnas.1405672111},
}

@Article{art/CarlinJ_201707,
  author   = {Carlin, Johan D. and Kriegeskorte, Nikolaus},
  journal  = {PLoS Computational Biology},
  title    = {{Adjudicating between face-coding models with individual-face fMRI responses}},
  year     = {2017},
  issn     = {15537358},
  month    = {jul},
  number   = {7},
  pages    = {e1005604},
  volume   = {13},
  abstract = {The perceptual representation of individual faces is often explained with reference to a norm-based face space. In such spaces, individuals are encoded as vectors where identity is primarily conveyed by direction and distinctiveness by eccentricity. Here we measured human fMRI responses and psychophysical similarity judgments of individual face exemplars, which were generated as realistic 3D animations using a computer-graphics model. We developed and evaluated multiple neurobiologically plausible computational models, each of which predicts a representational distance matrix and a regional-mean activation profile for 24 face stimuli. In the fusiform face area, a face-space coding model with sigmoidal ramp tuning provided a better account of the data than one based on exemplar tuning. However, an image-processing model with weighted banks of Gabor filters performed similarly. Accounting for the data required the inclusion of a measurement-level population averaging mechanism that approximates how fMRI voxels locally average distinct neuronal tunings. Our study demonstrates the importance of comparing multiple models and of modeling the measurement process in computational neuroimaging.},
  doi      = {10.1371/journal.pcbi.1005604},
  editor   = {Daunizeau, Jean},
  pmid     = {28746335},
  url      = {https://dx.plos.org/10.1371/journal.pcbi.1005604},
}

@Article{art/KumarN_202005,
  author   = {Kumar, Neeraj and Verma, Ruchika and Anand, Deepak and Zhou, Yanning and Onder, Omer Fahri and Tsougenis, Efstratios and Chen, Hao and Heng, Pheng Ann and Li, Jiahui and Hu, Zhiqiang and Wang, Yunzhi and Koohbanani, Navid Alemi and Jahanifar, Mostafa and Tajeddin, Neda Zamani and Gooya, Ali and Rajpoot, Nasir and Ren, Xuhua and Zhou, Sihang and Wang, Qian and Shen, Dinggang and Yang, Cheng Kun and Weng, Chi Hung and Yu, Wei Hsiang and Yeh, Chao Yuan and Yang, Shuang and Xu, Shuoyu and Yeung, Pak Hei and Sun, Peng and Mahbod, Amirreza and Schaefer, Gerald and Ellinger, Isabella and Ecker, Rupert and Smedby, Orjan and Wang, Chunliang and Chidester, Benjamin and Ton, That Vinh and Tran, Minh Triet and Ma, Jian and Do, Minh N. and Graham, Simon and Vu, Quoc Dang and Kwak, Jin Tae and Gunda, Akshaykumar and Chunduri, Raviteja and Hu, Corey and Zhou, Xiaoyang and Lotfi, Dariush and Safdari, Reza and Kascenas, Antanas and O'Neil, Alison and Eschweiler, Dennis and Stegmaier, Johannes and Cui, Yanping and Yin, Baocai and Chen, Kailin and Tian, Xinmei and Gruening, Philipp and Barth, Erhardt and Arbel, Elad and Remer, Itay and Ben-Dor, Amir and Sirazitdinova, Ekaterina and Kohl, Matthias and Braunewell, Stefan and Li, Yuexiang and Xie, Xinpeng and Shen, Linlin and Ma, Jun and Baksi, Krishanu Das and Khan, Mohammad Azam and Choo, Jaegul and Colomer, Adrian and Naranjo, Valery and Pei, Linmin and Iftekharuddin, Khan M. and Roy, Kaushiki and Bhattacharjee, Debotosh and Pedraza, Anibal and Bueno, Maria Gloria and Devanathan, Sabarinathan and Radhakrishnan, Saravanan and Koduganty, Praveen and Wu, Zihan and Cai, Guanyu and Liu, Xiaojie and Wang, Yuqin and Sethi, Amit},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{A Multi-Organ Nucleus Segmentation Challenge}},
  year     = {2020},
  issn     = {1558254X},
  month    = {may},
  number   = {5},
  pages    = {1380--1391},
  volume   = {39},
  abstract = {Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.},
  doi      = {10.1109/TMI.2019.2947628},
  keywords = {Multi-organ,aggregated Jaccard index,digital pathology,instance segmentation,nucleus segmentation},
  pmid     = {31647422},
  url      = {https://ieeexplore.ieee.org/document/8880654/},
}

@Article{art/DaducciA_201402,
  author   = {Daducci, Alessandro and Canales-Rodriguez, Erick Jorge and Descoteaux, Maxime and Garyfallidis, Eleftherios and Gur, Yaniv and Lin, Ying Chia and Mani, Merry and Merlet, Sylvain and Paquette, Michael and Ramirez-Manzanares, Alonso and Reisert, Marco and Rodrigues, Paulo Reis and Sepehrband, Farshid and Caruyer, Emmanuel and Choupan, Jeiran and Deriche, Rachid and Jacob, Mathews and Menegaz, Gloria and Prckovska, Vesna and Rivera, Mariano and Wiaux, Yves and Thiran, Jean Philippe},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Quantitative comparison of reconstruction methods for intra-voxel fiber recovery from diffusion MRI}},
  year     = {2014},
  issn     = {02780062},
  month    = {feb},
  number   = {2},
  pages    = {384--399},
  volume   = {33},
  abstract = {Validation is arguably the bottleneck in the diffusion magnetic resonance imaging (MRI) community. This paper evaluates and compares 20 algorithms for recovering the local intra-voxel fiber structure from diffusion MRI data and is based on the results of the 'HARDI reconstruction challenge' organized in the context of the 'ISBI 2012' conference. Evaluated methods encompass a mixture of classical techniques well known in the literature such as diffusion tensor, Q-Ball and diffusion spectrum imaging, algorithms inspired by the recent theory of compressed sensing and also brand new approaches proposed for the first time at this contest. To quantitatively compare the methods under controlled conditions, two datasets with known ground-truth were synthetically generated and two main criteria were used to evaluate the quality of the reconstructions in every voxel: correct assessment of the number of fiber populations and angular accuracy in their orientation. This comparative study investigates the behavior of every algorithm with varying experimental conditions and highlights strengths and weaknesses of each approach. This information can be useful not only for enhancing current algorithms and develop the next generation of reconstruction methods, but also to assist physicians in the choice of the most adequate technique for their studies. {\textcopyright} 1982-2012 IEEE.},
  doi      = {10.1109/TMI.2013.2285500},
  keywords = {Diffusion magnetic resonance imaging (dMRI),local reconstruction,quantitative comparison,synthetic data,validation},
  pmid     = {24132007},
  url      = {http://ieeexplore.ieee.org/document/6630106/},
}

@Article{art/LuM_202004,
  author        = {Lu, Ming Y. and Williamson, Drew F.K. and Chen, Tiffany Y. and Chen, Richard J. and Barbieri, Matteo and Mahmood, Faisal},
  journal       = {arXiv},
  title         = {{Data efficient and weakly supervised computational pathology on whole slide images}},
  year          = {2020},
  month         = {apr},
  abstract      = {The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images (WSIs) in fully-supervised settings or thousands of WSIs with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation, interpretability and visualization issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present CLAM - Clustering-constrained Attention Multiple instance learning (https://github.com/mahmoodlab/CLAM), an easy-to-use, high-throughput and interpretable WSI-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. CLAM is a deep-learning based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of CLAM and its superior performance over standard weakly-supervised classification. We demonstrate that CLAM models are interpretable and can be used to identify well-known and new morphological features without using any spatial labels during training. We further show that models trained using CLAM are adaptable to independent test cohorts, cell phone microscopy images, and varying slide tissue content. CLAM is a flexible, general purpose, and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.},
  archiveprefix = {arXiv},
  arxivid       = {2004.09666},
  eprint        = {2004.09666},
  url           = {http://arxiv.org/abs/2004.09666},
}

@Article{art/ArulkumaranK_201711,
  author   = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal  = {IEEE Signal Processing Magazine},
  title    = {{Deep reinforcement learning: A brief survey}},
  year     = {2017},
  issn     = {10535888},
  month    = {nov},
  number   = {6},
  pages    = {26--38},
  volume   = {34},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  doi      = {10.1109/MSP.2017.2743240},
  url      = {http://ieeexplore.ieee.org/document/8103164/},
}

@Article{art/QaiserT_2019,
  author        = {Qaiser, Talha and Tsang, Yee Wah and Taniyama, Daiki and Sakamoto, Naoya and Nakane, Kazuaki and Epstein, David and Rajpoot, Nasir},
  journal       = {Medical Image Analysis},
  title         = {{Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features}},
  year          = {2019},
  issn          = {13618423},
  pages         = {1--14},
  volume        = {55},
  abstract      = {Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.},
  archiveprefix = {arXiv},
  arxivid       = {1805.03699},
  doi           = {10.1016/j.media.2019.03.014},
  eprint        = {1805.03699},
  keywords      = {Colorectal (colon) cancer,Computational pathology,Deep learning,Histology image analysis,Persistent homology,Tumor segmentation},
  pmid          = {30991188},
  url           = {http://www.sciencedirect.com/science/article/pii/S1361841518302688},
}

@Article{art/VahadaneA_2016,
  author   = {Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images}},
  year     = {2016},
  issn     = {1558254X},
  number   = {8},
  pages    = {1962--1971},
  volume   = {35},
  abstract = {Staining and scanning of tissue samples for microscopic examination is fraught with undesirable color variations arising from differences in raw materials and manufacturing techniques of stain vendors, staining protocols of labs, and color responses of digital scanners. When comparing tissue samples, color normalization and stain separation of the tissue images can be helpful for both pathologists and software. Techniques that are used for natural images fail to utilize structural properties of stained tissue samples and produce undesirable color distortions. The stain concentration cannot be negative. Tissue samples are stained with only a few stains and most tissue regions are characterized by at most one effective stain. We model these physical phenomena that define the tissue structure by first decomposing images in an unsupervised manner into stain density maps that are sparse and non-negative. For a given image, we combine its stain density maps with stain color basis of a pathologist-preferred target image, thus altering only its color while preserving its structure described by the maps. Stain density correlation with ground truth and preference by pathologists were higher for images normalized using our method when compared to other alternatives. We also propose a computationally faster extension of this technique for large whole-slide images that selects an appropriate patch sample instead of using the entire image to compute the stain color basis.},
  doi      = {10.1109/TMI.2016.2529665},
  keywords = {Color normalization,Unsupervised stain separation,histopathological images,non-negative matrix factorization,sparse regularization},
  pmid     = {27164577},
}

@Article{art/FaustK_2019,
  author   = {Faust, Kevin and Bala, Sudarshan and van Ommeren, Randy and Portante, Alessia and {Al Qawahmed}, Raniah and Djuric, Ugljesa and Diamandis, Phedias},
  journal  = {Nature Machine Intelligence},
  title    = {{Intelligent feature engineering and ontological mapping of brain tumour histomorphologies by deep learning}},
  year     = {2019},
  issn     = {25225839},
  number   = {7},
  pages    = {316--321},
  volume   = {1},
  abstract = {Deep learning is an emerging transformative tool in diagnostic medicine, yet limited access and the interpretability of learned parameters hinders widespread adoption. Here we have generated a diverse repository of 838,644 histopathologic images and used them to optimize and discretize learned representations into 512-dimensional feature vectors. Importantly, we show that individual machine-engineered features correlate with salient human-derived morphologic constructs and ontological relationships. Deciphering the overlap between human and machine reasoning may aid in eliminating biases and improving automation and accountability for artificial intelligence-assisted medicine.},
  doi      = {10.1038/s42256-019-0068-6},
}

@Article{art/WangW_201810,
  author        = {Wang, William Yang and Li, Jiwei and He, Xiaodong},
  journal       = {ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference Tutorial Abstracts},
  title         = {{Deep reinforcement learning for NLP}},
  year          = {2018},
  month         = {oct},
  pages         = {19--21},
  abstract      = {Many Natural Language Processing (NLP) tasks (including generation, language grounding, reasoning, information extraction, coreference resolution, and dialog) can be formulated as deep reinforcement learning (DRL) problems. However, since language is often discrete and the space for all sentences is infinite, there are many challenges for formulating reinforcement learning problems of NLP tasks. In this tutorial, we provide a gentle introduction to the foundation of deep reinforcement learning, as well as some practical DRL solutions in NLP. We describe recent advances in designing deep reinforcement learning for NLP, with a special focus on generation, dialogue, and information extraction. We discuss why they succeed, and when they may fail, aiming at providing some practical advice about deep reinforcement learning for solving real-world NLP problems.},
  archiveprefix = {arXiv},
  arxivid       = {1810.06339},
  doi           = {10.18653/v1/p18-5007},
  eprint        = {1810.06339},
  isbn          = {9781948087667},
  url           = {http://arxiv.org/abs/1701.07274 http://arxiv.org/abs/1810.06339},
}

@Article{art/ZhangZ_2019,
  author   = {Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and Ahmad, Nazeel and Khalil, Farah K. and Dickinson, Shohreh I. and Shi, Xiaoshuang and Liu, Fujun and Su, Hai and Cai, Jinzheng and Yang, Lin},
  journal  = {Nature Machine Intelligence},
  title    = {{Pathologist-level interpretable whole-slide cancer diagnosis with deep learning}},
  year     = {2019},
  number   = {5},
  pages    = {236--245},
  volume   = {1},
  abstract = {Diagnostic pathology is the foundation and gold standard for identifying carcinomas. However, high inter-observer variability substantially affects productivity in routine pathology and is especially ubiquitous in diagnostician-deficient medical centres. Despite rapid growth in computer-aided diagnosis (CAD), the application of whole-slide pathology diagnosis remains impractical. Here, we present a novel pathology whole-slide diagnosis method, powered by artificial intelligence, to address the lack of interpretable diagnosis. The proposed method masters the ability to automate the human-like diagnostic reasoning process and translate gigapixels directly to a series of interpretable predictions, providing second opinions and thereby encouraging consensus in clinics. Moreover, using 913 collected examples of whole-slide data representing patients with bladder cancer, we show that our method matches the performance of 17 pathologists in the diagnosis of urothelial carcinoma. We believe that our method provides an innovative and reliable means for making diagnostic suggestions and can be deployed at low cost as next-generation, artificial intelligence-enhanced CAD technology for use in diagnostic pathology.},
  doi      = {10.1038/s42256-019-0052-1},
}

@Article{art/KomuraD_2018,
  author   = {Komura, Daisuke and Ishikawa, Shumpei},
  journal  = {Computational and Structural Biotechnology Journal},
  title    = {{Machine Learning Methods for Histopathological Image Analysis}},
  year     = {2018},
  issn     = {20010370},
  pages    = {34--42},
  volume   = {16},
  abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.},
  doi      = {10.1016/j.csbj.2018.01.001},
  keywords = {Computer assisted diagnosis,Deep learning,Digital image analysis,Histopathology,Machine learning,Whole slide images},
}

@Article{art/LiC_2019,
  author   = {Li, Chao and Wang, Xinggang and Liu, Wenyu and Latecki, Longin Jan and Wang, Bo and Huang, Junzhou},
  journal  = {Medical Image Analysis},
  title    = {{Weakly supervised mitosis detection in breast histopathology images using concentric loss}},
  year     = {2019},
  issn     = {13618423},
  pages    = {165--178},
  volume   = {53},
  abstract = {Developing new deep learning methods for medical image analysis is a prevalent research topic in machine learning. In this paper, we propose a deep learning scheme with a novel loss function for weakly supervised breast cancer diagnosis. According to the Nottingham Grading System, mitotic count plays an important role in breast cancer diagnosis and grading. To determine the cancer grade, pathologists usually need to manually count mitosis from a great deal of histopathology images, which is a very tedious and time-consuming task. This paper proposes an automatic method for detecting mitosis. We regard the mitosis detection task as a semantic segmentation problem and use a deep fully convolutional network to address it. Different from conventional training data used in semantic segmentation system, the training label of mitosis data is usually in the format of centroid pixel, rather than all the pixels belonging to a mitosis. The centroid label is a kind of weak label, which is much easier to annotate and can save the effort of pathologists a lot. However, technically this weak label is not sufficient for training a mitosis segmentation model. To tackle this problem, we expand the single-pixel label to a novel label with concentric circles, where the inside circle is a mitotic region and the ring around the inside circle is a “middle ground”. During the training stage, we do not compute the loss of the ring region because it may have the presence of both mitotic and non-mitotic pixels. This new loss termed as “concentric loss” is able to make the semantic segmentation network be trained with the weakly annotated mitosis data. On the generated segmentation map from the segmentation model, we filter out low confidence and obtain mitotic cells. On the challenging ICPR 2014 MITOSIS dataset and AMIDA13 dataset, we achieve a 0.562 F-score and 0.673 F-score respectively, outperforming all previous approaches significantly. On the latest TUPAC16 dataset, we obtain a F-score of 0.669, which is also the state-of-the-art result. The excellent results quantitatively demonstrate the effectiveness of the proposed mitosis segmentation network with the concentric loss. All of our code has been made publicly available at https://github.com/ChaoLi977/SegMitos_mitosis_detection.},
  doi      = {10.1016/j.media.2019.01.013},
  keywords = {Breast cancer grading,Fully convolutional network,Mitosis detection,Weakly supervised learning},
  pmid     = {30798116},
}

@Article{art/KulkarniT_201606,
  author        = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.},
  title         = {{Deep Successor Reinforcement Learning}},
  year          = {2016},
  month         = {jun},
  abstract      = {Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.},
  archiveprefix = {arXiv},
  arxivid       = {1606.02396},
  eprint        = {1606.02396},
  url           = {http://arxiv.org/abs/1606.02396},
}

@Article{art/BlundellC_201606,
  author        = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  title         = {{Model-Free Episodic Control}},
  year          = {2016},
  month         = {jun},
  abstract      = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
  archiveprefix = {arXiv},
  arxivid       = {1606.04460},
  eprint        = {1606.04460},
  url           = {http://arxiv.org/abs/1606.04460},
}

@Article{art/MunosR_201606,
  author        = {Munos, R{\'{e}}mi and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G.},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {{Safe and efficient off-policy reinforcement learning}},
  year          = {2016},
  issn          = {10495258},
  month         = {jun},
  pages         = {1054--1062},
  abstract      = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games.},
  archiveprefix = {arXiv},
  arxivid       = {1606.02647},
  eprint        = {1606.02647},
  url           = {http://arxiv.org/abs/1606.02647},
}

@Article{art/ShackmanA_201103,
  author   = {Shackman, Alexander J. and Salomons, Tim V. and Slagter, Heleen A. and Fox, Andrew S. and Winter, Jameel J. and Davidson, Richard J.},
  journal  = {Nature Reviews Neuroscience},
  title    = {{The integration of negative affect, pain and cognitive control in the cingulate cortex}},
  year     = {2011},
  issn     = {1471003X},
  month    = {mar},
  number   = {3},
  pages    = {154--167},
  volume   = {12},
  abstract = {It has been argued that emotion, pain and cognitive control are functionally segregated in distinct subdivisions of the cingulate cortex. However, recent observations encourage a fundamentally different view. Imaging studies demonstrate that negative affect, pain and cognitive control activate an overlapping region of the dorsal cingulate - the anterior midcingulate cortex (aMCC). Anatomical studies reveal that the aMCC constitutes a hub where information about reinforcers can be linked to motor centres responsible for expressing affect and executing goal-directed behaviour. Computational modelling and other kinds of evidence suggest that this intimacy reflects control processes that are common to all three domains. These observations compel a reconsideration of the dorsal cingulate's contribution to negative affect and pain. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
  doi      = {10.1038/nrn2994},
  pmid     = {21331082},
  url      = {http://www.nature.com/articles/nrn2994},
}

@Article{art/LaMontagneP_201901,
  author   = {LaMontagne, Pamela and Benzinger, Tammie LS and Morris, John and Keefe, Sarah and Hornbeck, Russ and Xiong, Chengjie and Grant, Elizabeth and Hassenstab, Jason and Moulder, Krista and Vlassenko, Andrei and Raichle, Marcus and Cruchaga, Carlos and Marcus, Daniel},
  journal  = {medRxiv},
  title    = {{OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease}},
  year     = {2019},
  month    = {jan},
  pages    = {2019.12.13.19014902},
  abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  doi      = {10.1101/2019.12.13.19014902},
  url      = {http://medrxiv.org/content/early/2019/12/15/2019.12.13.19014902.abstract},
}

@Misc{art/MarcusD_2019,
  author   = {Marcus, D S and Wang, T H and Parker, J and Csernansky, J G and Morris, J C and Buckner, R L},
  title    = {{Open Access Series of Imaging Studies (OASIS)}},
  year     = {2019},
  keywords = {MRI,data,longitudinal,neuroimaging,open source},
  number   = {31 May 2011},
  url      = {https://www.oasis-brains.org/%0Awww.oasis-brains.org%0Ahttp://www.oasis-brains.org/},
}

@Article{art/MarcusD_200709,
  author   = {Marcus, Daniel S. and Wang, Tracy H. and Parker, Jamie and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
  journal  = {Journal of Cognitive Neuroscience},
  title    = {{Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults}},
  year     = {2007},
  issn     = {0898929X},
  month    = {sep},
  number   = {9},
  pages    = {1498--1507},
  volume   = {19},
  abstract = {The Open Access Series of Imaging Studies is a series of magnetic resonance imaging data sets that is publicly available for study and analysis. The initial data set consists of a cross-sectional collection of 416 subjects aged 18 to 96 years. One hundred of the included subjects older than 60 years have been clinically diagnosed with very mild to moderate Alzheimer's disease. The subjects are all right-handed and include both men and women. For each subject, three or four individual T1-weighted magnetic resonance imaging scans obtained in single imaging sessions are included. Multiple within-session acquisitions provide extremely high contrast-to-noise ratio, making the data amenable to a wide range of analytic approaches including automated computational analysis. Additionally, a reliability data set is included containing 20 subjects without dementia imaged on a subsequent visit within 90 days of their initial session. Automated calculation of whole-brain volume and estimated total intracranial volume are presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2007 Massachusetts Institute of Technology.},
  doi      = {10.1162/jocn.2007.19.9.1498},
  pmid     = {17714011},
  url      = {http://www.mitpressjournals.org/doi/10.1162/jocn.2007.19.9.1498},
}

@Article{art/MarcusD_201012,
  author   = {Marcus, Daniel S. and Fotenos, Anthony F. and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
  journal  = {Journal of Cognitive Neuroscience},
  title    = {{Open access series of imaging studies: Longitudinal MRI data in nondemented and demented older adults}},
  year     = {2010},
  issn     = {0898929X},
  month    = {dec},
  number   = {12},
  pages    = {2677--2684},
  volume   = {22},
  abstract = {The Open Access Series of Imaging Studies is a series of neuroimaging data sets that are publicly available for study and analysis. The present MRI data set consists of a longitudinal collection of 150 subjects aged 60 to 96 years all acquired on the same scanner using identical sequences. Each subject was scanned on two or more visits, separated by at least 1 year for a total of 373 imaging sessions. Subjects were characterized using the Clinical Dementia Rating (CDR) as either nondemented or with very mild tomild Alzheimer's disease. Seventy-two of the subjects were characterized as nondemented throughout the study. Sixty-four of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with CDR 0.5 similar level of impairment to individuals elsewhere considered to have "mild cognitive impairment." Another 14 subjects were characterized as nondemented at the time of their initial visit (CDR 0) and were subsequently characterized as demented at a later visit (CDR > 0). The subjects were all right-handed and include bothmen (n=62) and women (n = 88). For each scanning session, three or four individual T1-weighted MRI scans were obtained. Multiple withinsession acquisitions provide extremely high contrast to noise, making the data amenable to a wide range of analytic approaches including automated computational analysis. Automated calculation of whole-brain volume is presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2010 Massachusetts Institute of Technology.},
  doi      = {10.1162/jocn.2009.21407},
  url      = {http://www.mitpressjournals.org/doi/10.1162/jocn.2009.21407},
}

@Article{art/HardA_201811,
  author        = {Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'{e}} and Ramage, Daniel},
  title         = {{Federated Learning for Mobile Keyboard Prediction}},
  year          = {2018},
  month         = {nov},
  abstract      = {We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.},
  archiveprefix = {arXiv},
  arxivid       = {1811.03604},
  eprint        = {1811.03604},
  url           = {http://arxiv.org/abs/1811.03604},
}

@Article{art/RamaswamyS_201906,
  author        = {Ramaswamy, Swaroop and Mathews, Rajiv and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
  journal       = {arXiv},
  title         = {{Federated learning for emoji prediction in a mobile keyboard}},
  year          = {2019},
  issn          = {23318422},
  month         = {jun},
  abstract      = {We show that a word-level recurrent neural network can predict emoji from text typed on a mobile keyboard. We demonstrate the usefulness of transfer learning for predicting emoji by pretraining the model using a language modeling task. We also propose mechanisms to trigger emoji and tune the diversity of candidates. The model is trained using a distributed on-device learning framework called federated learning. The federated model is shown to achieve better performance than a server-trained model. This work demonstrates the feasibility of using federated learning to train production-quality models for natural language understanding tasks while keeping users' data on their devices.},
  archiveprefix = {arXiv},
  arxivid       = {1906.04329},
  eprint        = {1906.04329},
  url           = {http://arxiv.org/abs/1906.04329},
}

@Misc{art/DayT_2019,
  author  = {Day, Trevor K. M. and Madyastha, Tara M. and Boord, Peter and Askren, Mary K. and Montine, Thomas J. and Grabowski, Thomas J.},
  title   = {{ANT: Healthy aging and Parkinson's disease}},
  year    = {2019},
  url     = {https://openneuro.org/datasets/ds001907/versions/2.0.3},
  urldate = {2020-05-27},
}

@Misc{art/TessaC_2018,
  author    = {Tessa, Carlo},
  title     = {{PD De Novo: Resting State fMRI and Physiological Signals}},
  year      = {2018},
  doi       = {10.18112/OPENNEURO.DS001354.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001354/versions/1.0.0},
}

@Article{art/AisenP_201507,
  author   = {Aisen, Paul S. and Petersen, Ronald C. and Donohue, Michael and Weiner, Michael W.},
  journal  = {Alzheimer's and Dementia},
  title    = {{Alzheimer's Disease Neuroimaging Initiative 2 Clinical Core: Progress and plans}},
  year     = {2015},
  issn     = {15525279},
  month    = {jul},
  number   = {7},
  pages    = {734--739},
  volume   = {11},
  abstract = {Introduction This article reviews the current status of the Clinical Core of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and summarizes planning for the next stage of the project. Methods Clinical Core activities and plans were synthesized based on discussions among the Core leaders and external advisors. Results The longitudinal data in ADNI-2 provide natural history data on a clinical trials population and continue to inform refinement and standardization of assessments, models of trajectories, and clinical trial methods that have been extended into sporadic preclinical Alzheimer's disease (AD). Discussion Plans for the next phase of the ADNI project include maintaining longitudinal follow-up of the normal and mild cognitive impairment cohorts, augmenting specific clinical cohorts, and incorporating novel computerized cognitive assessments and patient-reported outcomes. A major hypothesis is that AD represents a gradually progressive disease that can be identified precisely in its long presymptomatic phase, during which intervention with potentially disease-modifying agents may be most useful.},
  doi      = {10.1016/j.jalz.2015.05.005},
  keywords = {Alzheimer's disease,Amyloid,Cognitive assessment},
  pmid     = {26194309},
  url      = {http://doi.wiley.com/10.1016/j.jalz.2015.05.005},
}

@Article{art/TessaC_201901,
  author   = {Tessa, Carlo and Toschi, Nicola and Orsolini, Stefano and Valenza, Gaetano and Lucetti, Claudio and Barbieri, Riccardo and Diciotti, Stefano},
  journal  = {PLoS ONE},
  title    = {{Central modulation of parasympathetic outflow is impaired in de novo Parkinson's disease patients}},
  year     = {2019},
  issn     = {19326203},
  month    = {jan},
  number   = {1},
  pages    = {e0210324},
  volume   = {14},
  abstract = {Task- and stimulus-based neuroimaging studies have begun to unveil the central autonomic network which modulates autonomic nervous system activity. In the present study, we aimed to evaluate the central autonomic network without the bias constituted by the use of a task. Additionally, we assessed whether this circuitry presents signs of dysregulation in the early stages of Parkinson's disease (PD), a condition which may be associated with dysautonomia. We combined heart-rate-variability based methods for time-varying assessments of the autonomic nervous system outflow with resting-state fMRI in 14 healthy controls and 14 de novo PD patients, evaluating the correlations between fMRI time-series and the instantaneous high-frequency component of the heart-rate-variability power spectrum, a marker of parasympathetic outflow. In control subjects, the high-frequency component of the heart-rate-variability power spectrum was significantly anti-correlated with fMRI time-series in several cortical, subcortical and brainstem regions. This complex central network was not detectable in PD patients. In between-group analysis, we found that in healthy controls the brain activation related to the high-frequency component of the heart-rate-variability power spectrum was significantly less than in PD patients in the mid and anterior cingulum, sensorimotor cortex and supplementary motor area, insula and temporal lobe, prefrontal cortex, hippocampus and in a region encompassing posterior cingulum, precuneus and parieto-occipital cortex. Our results indicate that the complex central network which modulates parasympathetic outflow in the resting state is impaired in the early clinical stages of PD.},
  doi      = {10.1371/journal.pone.0210324},
  editor   = {Koenig, Julian},
  pmid     = {30653564},
  url      = {https://dx.plos.org/10.1371/journal.pone.0210324},
}

@Misc{art/MoriH_2008,
  author    = {Mori, Harushi},
  title     = {{Diffusion tensor imaging}},
  year      = {2008},
  abstract  = {Diffusion tensor imaging of magnetic resonance imaging, including diffusion tensor tractography, is a unique tool to visualize and segment the white matter pathways in vivo and one can evaluate the segmented trace quantitatively. Three dimensional visualization of the white matter fibers, such as corticospinal (pyramidal) tracts, with relationship to brain lesions (infarcts, vascular malformations and brain tumors) is extremely helpful for stereotactic radiosurgery, preoperative evaluation and intraoperative navigation. Quantitative measurement of the tract is a very sensitive method to detect differences in the tract in neurodegenerative/neurocognitive/psychiatric patients such as amyotrophic lateral sclerosis, schizophrenia and Alzheimer diseases. Importance of this tool will become more significant in clinical and neuroscience fields in the future.},
  booktitle = {Clinical Neurology},
  doi       = {10.5692/clinicalneurol.48.945},
  issn      = {0009918X},
  keywords  = {Anisotropy,Diffusion tensor,Magnetic resonance imaging,Tractography,White matter},
  number    = {11},
  pages     = {945--946},
  pmid      = {19198126},
  url       = {https://openneuro.org/datasets/ds001378/versions/00003},
  volume    = {48},
}

@Article{art/WeinerM_201705,
  author   = {Weiner, Michael W. and Veitch, Dallas P. and Aisen, Paul S. and Beckett, Laurel A. and Cairns, Nigel J. and Green, Robert C. and Harvey, Danielle and Jack, Clifford R. and Jagust, William and Morris, John C. and Petersen, Ronald C. and Salazar, Jennifer and Saykin, Andrew J. and Shaw, Leslie M. and Toga, Arthur W. and Trojanowski, John Q.},
  journal  = {Alzheimer's and Dementia},
  title    = {{The Alzheimer's Disease Neuroimaging Initiative 3: Continued innovation for clinical trial improvement}},
  year     = {2017},
  issn     = {15525279},
  month    = {may},
  number   = {5},
  pages    = {561--571},
  volume   = {13},
  abstract = {Introduction The overall goal of the Alzheimer's Disease Neuroimaging Initiative (ADNI) is to validate biomarkers for Alzheimer's disease (AD) clinical trials. ADNI-3, which began on August 1, 2016, is a 5-year renewal of the current ADNI-2 study. Methods ADNI-3 will follow current and additional subjects with normal cognition, mild cognitive impairment, and AD using innovative technologies such as tau imaging, magnetic resonance imaging sequences for connectivity analyses, and a highly automated immunoassay platform and mass spectroscopy approach for cerebrospinal fluid biomarker analysis. A Systems Biology/pathway approach will be used to identify genetic factors for subject selection/enrichment. Amyloid positron emission tomography scanning will be standardized using the Centiloid method. The Brain Health Registry will help recruit subjects and monitor subject cognition. Results Multimodal analyses will provide insight into AD pathophysiology and disease progression. Discussion ADNI-3 will aim to inform AD treatment trials and facilitate development of AD disease-modifying treatments.},
  doi      = {10.1016/j.jalz.2016.10.006},
  keywords = {Alzheimer's disease,Amyloid phenotyping,Brain Health Registry,Centiloid method,Clinical trial biomarkers,Functional connectivity,Tau imaging},
  pmid     = {27931796},
  url      = {http://doi.wiley.com/10.1016/j.jalz.2016.10.006},
}

@Article{art/YangQ_201902,
  author        = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
  journal       = {ACM Transactions on Intelligent Systems and Technology},
  title         = {{Federated machine learning: Concept and applications}},
  year          = {2019},
  issn          = {21576912},
  month         = {feb},
  number        = {2},
  volume        = {10},
  abstract      = {Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
  archiveprefix = {arXiv},
  arxivid       = {1902.04885},
  doi           = {10.1145/3298981},
  eprint        = {1902.04885},
  keywords      = {Federated learning,GDPR,transfer learning},
  url           = {http://arxiv.org/abs/1902.04885},
}

@Misc{art/BoysenJ_2017,
  author   = {Boysen, Jacob},
  title    = {{MRI and Alzheimers}},
  year     = {2017},
  keywords = {health,health sciences,healthcare,image data,medical facilities and services,neurological conditions,neurology,neuroscience,old age},
  url      = {https://www.kaggle.com/jboysen/mri-and-alzheimers},
  urldate  = {2020-05-25},
}

@Misc{art/IwatsuboT_2011,
  author    = {Iwatsubo, Takeshi},
  title     = {{[Alzheimer's disease Neuroimaging Initiative (ADNI)].}},
  year      = {2011},
  abstract  = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) unites researchers with study data as they work to define the progression of Alzheimer's disease (AD). ADNI researchers collect, validate and utilize data, including MRI and PET images, genetics, cognitive tests, CSF and blood biomarkers as predictors of the disease. Study resources and data from the North American ADNI study are available through this website, including Alzheimer's disease patients, mild cognitive impairment subjects, and elderly controls.},
  booktitle = {Nihon rinsho. Japanese journal of clinical medicine},
  issn      = {00471852},
  keywords  = {[1] “Alzheimer's Disease Neuroimaging Initiative.”},
  pages     = {570--574},
  pmid      = {22787853},
  url       = {http://adni.loni.usc.edu/},
  urldate   = {2020-05-25},
  volume    = {69 Suppl 8},
}

@Misc{art/MalekzadehS_2019,
  author   = {Malekzadeh, S.},
  title    = {{MRI Hippocampus Segmentation | Kaggle}},
  year     = {2019},
  keywords = {biology,deep learning,health foundations and medical research,medical facilities and services,neurological conditions,object segmentation,old age},
  url      = {https://www.kaggle.com/sabermalek/mrihs},
  urldate  = {2020-05-25},
}

@Article{art/VarmazyarH_2020,
  author = {Varmazyar, Hadi and Ghareaghaji, Zahra and Malekzadeh, Saber},
  title  = {{MRI Hippocampus Segmentation using Deep Learning autoencoders}},
  year   = {2020},
}

@Misc{art/StynerM_2008,
  author  = {Styner, Martin and Warfield, Simon and Lee, Joohwi},
  title   = {{MS lesion segmentation challenage 2008}},
  year    = {2008},
  url     = {http://www.ia.unc.edu/MSseg/},
  urldate = {2020-05-21},
}

@Misc{art/FLIIAM_2016,
  author    = {FLI-IAM},
  title     = {{MS segmentation challenge using a data management and processing infrastructure}},
  year      = {2016},
  booktitle = {FLI-IAM France Life Imaging - Information Analysis and Management},
  url       = {https://portal.fli-iam.irisa.fr/msseg-challenge/overview},
  urldate   = {2020-05-21},
}

@Misc{art/PhamD_2015,
  author  = {Pham, Dzung and Bazin, Pierre-Louis and Carass, Aaron and Calabresi, Peter and Crainiceanu, Ciprian and Ellingsen, Lotta and He, Qing and Prince, Jerry and Reich, Daniel and Roy, Snehashis},
  title   = {{MSChallenge - IACL}},
  year    = {2015},
  url     = {http://iacl.ece.jhu.edu/index.php/MSChallenge},
  urldate = {2020-05-21},
}

@Misc{Egger2016,
  author = {Egger, Karl and Maier, Oskar and Reyes, Mauricio and Wiest, Roland},
  title  = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2016}},
  year   = {2016},
  url    = {http://www.isles-challenge.org/ISLES2017/%0Ahttp://www.isles-challenge.org/ISLES2016/},
}

@Misc{Hakim2017,
  author  = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Winzeck, Stefan},
  title   = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2017}},
  year    = {2017},
  url     = {http://www.isles-challenge.org/ISLES2017/},
  urldate = {2020-05-20},
}

@Misc{PSOM2018,
  author   = {{Perelman School Of Medicine}},
  title    = {{MICCAI BraTS 2018: Clinical Relevance | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2018},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2018/clinical-relevance.html},
}

@Article{Maier2017,
  author   = {Maier, Oskar and Menze, Bjoern H. and von der Gablentz, Janina and H{\"{a}}ni, Levin and Heinrich, Mattias P. and Liebrand, Matthias and Winzeck, Stefan and Basit, Abdul and Bentley, Paul and Chen, Liang and Christiaens, Daan and Dutil, Francis and Egger, Karl and Feng, Chaolu and Glocker, Ben and G{\"{o}}tz, Michael and Haeck, Tom and Halme, Hanna Leena and Havaei, Mohammad and Iftekharuddin, Khan M. and Jodoin, Pierre Marc and Kamnitsas, Konstantinos and Kellner, Elias and Korvenoja, Antti and Larochelle, Hugo and Ledig, Christian and Lee, Jia Hong and Maes, Frederik and Mahmood, Qaiser and Maier-Hein, Klaus H. and McKinley, Richard and Muschelli, John and Pal, Chris and Pei, Linmin and Rangarajan, Janaki Raman and Reza, Syed M.S. and Robben, David and Rueckert, Daniel and Salli, Eero and Suetens, Paul and Wang, Ching Wei and Wilms, Matthias and Kirschke, Jan S. and Kr{\"{a}}mer, Ulrike M. and M{\"{u}}nte, Thomas F. and Schramm, Peter and Wiest, Roland and Handels, Heinz and Reyes, Mauricio},
  journal  = {Medical Image Analysis},
  title    = {{ISLES 2015 - A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI}},
  year     = {2017},
  issn     = {13618423},
  month    = {jan},
  pages    = {250--269},
  volume   = {35},
  abstract = {Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained data enables a critical evaluation of the current state-of-the-art, recommendations for further developments, and the identification of remaining challenges. The segmentation of acute perfusion lesions addressed in SPES was found to be feasible. However, algorithms applied to sub-acute lesion segmentation in SISS still lack accuracy. Overall, no algorithmic characteristic of any method was found to perform superior to the others. Instead, the characteristics of stroke lesion appearances, their evolution, and the observed challenges should be studied in detail. The annotated ISLES image datasets continue to be publicly available through an online evaluation system to serve as an ongoing benchmarking resource (www.isles-challenge.org).},
  doi      = {10.1016/j.media.2016.07.009},
  keywords = {Benchmark,Challenge,Comparison,Ischemic stroke,MRI,Segmentation},
  pmid     = {27475911},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301268},
}

@Article{Prastawa2009,
  author   = {Prastawa, Marcel and Bullitt, Elizabeth and Gerig, Guido},
  journal  = {Medical Image Analysis},
  title    = {{Simulation of brain tumors in MR images for evaluation of segmentation efficacy}},
  year     = {2009},
  issn     = {13618415},
  month    = {apr},
  number   = {2},
  pages    = {297--311},
  volume   = {13},
  abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors). {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.media.2008.11.002},
  keywords = {Brain MRI,Diffusion tensor imaging,Gold standard,Ground truth,Segmentation validation,Simulation of tumor infiltration,Tumor simulation},
  pmid     = {19119055},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841508001357},
}

@Misc{,
  title = {{BRATS - SICAS Medical Image Repository}},
  url   = {https://www.virtualskeleton.ch/BRATS/Start2013},
}

@Misc{,
  title = {{MICCAI BRATS 2012}},
  url   = {http://www2.imm.dtu.dk/projects/BRATS2012/},
}

@Article{Wardlaw2013,
  author   = {Wardlaw, Joanna M. and Smith, Eric E. and Biessels, Geert J. and Cordonnier, Charlotte and Fazekas, Franz and Frayne, Richard and Lindley, Richard I. and O'Brien, John T. and Barkhof, Frederik and Benavente, Oscar R. and Black, Sandra E. and Brayne, Carol and Breteler, Monique and Chabriat, Hugues and DeCarli, Charles and de Leeuw, Frank Erik and Doubal, Fergus and Duering, Marco and Fox, Nick C. and Greenberg, Steven and Hachinski, Vladimir and Kilimann, Ingo and Mok, Vincent and van Oostenbrugge, Robert and Pantoni, Leonardo and Speck, Oliver and Stephan, Blossom C.M. and Teipel, Stefan and Viswanathan, Anand and Werring, David and Chen, Christopher and Smith, Colin and van Buchem, Mark and Norrving, Bo and Gorelick, Philip B. and Dichgans, Martin},
  journal  = {The Lancet Neurology},
  title    = {{Neuroimaging standards for research into small vessel disease and its contribution to ageing and neurodegeneration}},
  year     = {2013},
  issn     = {14744422},
  month    = {aug},
  number   = {8},
  pages    = {822--838},
  volume   = {12},
  abstract = {Cerebral small vessel disease (SVD) is a common accompaniment of ageing. Features seen on neuroimaging include recent small subcortical infarcts, lacunes, white matter hyperintensities, perivascular spaces, microbleeds, and brain atrophy. SVD can present as a stroke or cognitive decline, or can have few or no symptoms. SVD frequently coexists with neurodegenerative disease, and can exacerbate cognitive deficits, physical disabilities, and other symptoms of neurodegeneration. Terminology and definitions for imaging the features of SVD vary widely, which is also true for protocols for image acquisition and image analysis. This lack of consistency hampers progress in identifying the contribution of SVD to the pathophysiology and clinical features of common neurodegenerative diseases. We are an international working group from the Centres of Excellence in Neurodegeneration. We completed a structured process to develop definitions and imaging standards for markers and consequences of SVD. We aimed to achieve the following: first, to provide a common advisory about terms and definitions for features visible on MRI; second, to suggest minimum standards for image acquisition and analysis; third, to agree on standards for scientific reporting of changes related to SVD on neuroimaging; and fourth, to review emerging imaging methods for detection and quantification of preclinical manifestations of SVD. Our findings and recommendations apply to research studies, and can be used in the clinical setting to standardise image interpretation, acquisition, and reporting. This Position Paper summarises the main outcomes of this international effort to provide the STandards for ReportIng Vascular changes on nEuroimaging (STRIVE). {\textcopyright} 2013 Elsevier Ltd.},
  doi      = {10.1016/S1474-4422(13)70124-8},
  pmid     = {23867200},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1474442213701248},
}

@Article{Pantoni2010,
  author   = {Pantoni, Leonardo},
  journal  = {The Lancet Neurology},
  title    = {{Cerebral small vessel disease: from pathogenesis and clinical characteristics to therapeutic challenges}},
  year     = {2010},
  issn     = {14744422},
  month    = {jul},
  number   = {7},
  pages    = {689--701},
  volume   = {9},
  abstract = {The term cerebral small vessel disease refers to a group of pathological processes with various aetiologies that affect the small arteries, arterioles, venules, and capillaries of the brain. Age-related and hypertension-related small vessel diseases and cerebral amyloid angiopathy are the most common forms. The consequences of small vessel disease on the brain parenchyma are mainly lesions located in the subcortical structures such as lacunar infarcts, white matter lesions, large haemorrhages, and microbleeds. Because lacunar infarcts and white matter lesions are easily detected by neuroimaging, whereas small vessels are not, the term small vessel disease is frequently used to describe the parenchyma lesions rather than the underlying small vessel alterations. This classification, however, restricts the definition of small vessel disease to ischaemic lesions and might be misleading. Small vessel disease has an important role in cerebrovascular disease and is a leading cause of cognitive decline and functional loss in the elderly. Small vessel disease should be a main target for preventive and treatment strategies, but all types of presentation and complications should be taken into account. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
  doi      = {10.1016/S1474-4422(10)70104-6},
  pmid     = {20610345},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1474442210701046},
}

@Misc{Erickson2017,
  author    = {Erickson, Bradley and Akkus, Zeynettin and Sedlar, Jiri and Korfiatis, Panagiotis},
  title     = {{Data From LGG-1p19qDeletion}},
  year      = {2017},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2017.dwehtz9v},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/coKJAQ},
}

@Misc{Bakas2019,
  author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Colen, Rivka R. and Marcus, Daniel and Weber, Marc-Andre and Mahajan, Abhishek},
  title  = {{Multimodal Brain Tumor Segmentation Challenge 2019 | CBICA | Perelman School of Medicine at the University of Pennsylvania}},
  year   = {2019},
  url    = {https://www.med.upenn.edu/cbica/brats-2019/},
}

@Misc{Vallieres2017a,
  author    = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang-Shu and Sultanem, Khalil},
  title     = {{Data from Head-Neck-PET-CT}},
  year      = {2017},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2017.8oje5q00},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/24pyAQ},
}

@Misc{Bakas2017a,
  author   = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Reyes, Mauricio and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Shaykh, Hassan Fathallah and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre},
  title    = {{MICCAI BraTS 2017: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
  year     = {2017},
  keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
  url      = {https://www.med.upenn.edu/sbia/brats2017.html},
}

@Misc{Consortium2018,
  author    = {Consortium, National Cancer Institute Clinical Proteomic Tumor Analysis},
  title     = {{Radiology Data from the Clinical Proteomic Tumor Analysis Consortium Glioblastoma Multiforme [CPTAC-GBM] collection}},
  year      = {2018},
  booktitle = {The Cancer Imaging Archive},
  doi       = {10.7937/k9/tcia.2018.3rje41q1},
  publisher = {The Cancer Imaging Archive},
  url       = {https://wiki.cancerimagingarchive.net/x/gAHUAQ},
}

@Misc{Bakas2020,
  author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Tiwari, Pallavi and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Huang, Raymond and Colen, Rivka R. and Marcus, Daniel and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek},
  title  = {{MICCAI BRATS - The Multimodal Brain Tumor Segmentation Challenge 2020}},
  year   = {2020},
  url    = {http://braintumorsegmentation.org/},
}

@Article{Menze2015,
  author   = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
  year     = {2015},
  issn     = {1558254X},
  month    = {oct},
  number   = {10},
  pages    = {1993--2024},
  volume   = {34},
  abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  doi      = {10.1109/TMI.2014.2377694},
  keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
  pmid     = {25494501},
  url      = {http://ieeexplore.ieee.org/document/6975210/},
}

@Article{Wang2004,
  author   = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
  journal  = {IEEE Transactions on Image Processing},
  title    = {{Image quality assessment: From error visibility to structural similarity}},
  year     = {2004},
  issn     = {10577149},
  month    = {apr},
  number   = {4},
  pages    = {600--612},
  volume   = {13},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
  doi      = {10.1109/TIP.2003.819861},
  keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
  pmid     = {15376593},
  url      = {http://ieeexplore.ieee.org/document/1284395/},
}

@Misc{,
  title   = {{ENIGMA Cerebellum | MICCAI 2017 Workshop & Challenge}},
  url     = {https://my.vanderbilt.edu/enigmacerebellum/},
  urldate = {2020-05-12},
}

@Misc{,
  title = {{Products | Neuromorphometrics, Inc.}},
  url   = {http://www.neuromorphometrics.com/?page_id=23},
}

@Misc{Landman,
  author = {Landman, Bennett A and Anderson, Adam W and Schilling, Kurt and Alexander, Simon and Kerins, Fergal and Westin, C-F and Rathi, Yogesh and Dyrby, Tim B. and Descoteaux, Maxime and Houde, Jean-Christophe and Verma, Ragini and Pierpaoli, Carlo and Irfanoglu, Okan and Thomas, Cibu},
  title  = {{3-D Validation of Tractography with Experimental MRI (3D VoTEM)}},
  url    = {https://my.vanderbilt.edu/votem/},
}

@Article{Berrada2018,
  author        = {Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan},
  journal       = {arXiv},
  title         = {{Smooth loss functions for deep top-k classification}},
  year          = {2018},
  issn          = {23318422},
  month         = {feb},
  abstract      = {The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\"{i}}ve algorithm would require O(nk) operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divideand- conquer approach, we provide an algorithm with a time complexity of O(kn). Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k = 5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.},
  archiveprefix = {arXiv},
  arxivid       = {1802.07595},
  eprint        = {1802.07595},
  url           = {http://arxiv.org/abs/1802.07595},
}

@Article{Liu2016,
  author   = {Liu, Yinglu and Zhang, Yan Ming and Zhang, Xu Yao and Liu, Cheng Lin},
  journal  = {Pattern Recognition},
  title    = {{Adaptive spatial pooling for image classification}},
  year     = {2016},
  issn     = {00313203},
  month    = {jul},
  pages    = {58--67},
  volume   = {55},
  abstract = {In this paper, we propose an adaptive spatial pooling method for enhancing the discriminability of feature representation for image classification. The core idea is to adopt a spatial distribution matrix to define how the image patches are pooled together. By formulating the pooling distribution learning and classifier training jointly, our method can extract multiple spatial layouts of arbitrary shapes rather than regular rectangular regions. By proper mathematical transformation, the distributions can be learned via a boosting-like algorithm, which improves the efficiency of learning especially for large distribution matrices. Further, our method allows category-specific pooling operations to take advantage of the different spatial layouts of different categories. Experimental results on three benchmark datasets UIUC-Sports, 21-Land-Use and Scene 15 demonstrate the effectiveness of our method.},
  doi      = {10.1016/j.patcog.2016.01.030},
  keywords = {Distribution matrix,Image classification,Spatial layout,Weighted pooling},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316000510},
}

@Misc{Zhang2019a,
  author    = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
  title     = {{AGE: Angle closure Glaucoma Evaluation Challenge}},
  year      = {2019},
  doi       = {10.21227/petb-fy10},
  publisher = {IEEE Dataport},
  url       = {http://dx.doi.org/10.21227/petb-fy10},
}

@Article{art/YangX_201412,
  author   = {Yang, Xiaofeng and Wu, Ning and Cheng, Guanghui and Zhou, Zhengyang and Yu, David S. and Beitler, Jonathan J. and Curran, Walter J. and Liu, Tian},
  journal  = {International Journal of Radiation Oncology Biology Physics},
  title    = {{Automated segmentation of the parotid gland based on atlas registration and machine learning: A longitudinal mri study in head-and-neck radiation therapy}},
  year     = {2014},
  issn     = {1879355X},
  month    = {dec},
  number   = {5},
  pages    = {1225--1233},
  volume   = {90},
  abstract = {Purpose To develop an automated magnetic resonance imaging (MRI) parotid segmentation method to monitor radiation-induced parotid gland changes in patients after head and neck radiation therapy (RT).
Methods and Materials The proposed method combines the atlas registration method, which captures the global variation of anatomy, with a machine learning technology, which captures the local statistical features, to automatically segment the parotid glands from the MRIs. The segmentation method consists of 3 major steps. First, an atlas (pre-RT MRI and manually contoured parotid gland mask) is built for each patient. A hybrid deformable image registration is used to map the pre-RT MRI to the post-RT MRI, and the transformation is applied to the pre-RT parotid volume. Second, the kernel support vector machine (SVM) is trained with the subject-specific atlas pair consisting of multiple features (intensity, gradient, and others) from the aligned pre-RT MRI and the transformed parotid volume. Third, the well-trained kernel SVM is used to differentiate the parotid from surrounding tissues in the post-RT MRIs by statistically matching multiple texture features. A longitudinal study of 15 patients undergoing head and neck RT was conducted: baseline MRI was acquired prior to RT, and the post-RT MRIs were acquired at 3-, 6-, and 12-month follow-up examinations. The resulting segmentations were compared with the physicians' manual contours.
Results Successful parotid segmentation was achieved for all 15 patients (42 post-RT MRIs). The average percentage of volume differences between the automated segmentations and those of the physicians' manual contours were 7.98% for the left parotid and 8.12% for the right parotid. The average volume overlap was 91.1% ± 1.6% for the left parotid and 90.5% ± 2.4% for the right parotid. The parotid gland volume reduction at follow-up was 25% at 3 months, 27% at 6 months, and 16% at 12 months.
Conclusions We have validated our automated parotid segmentation algorithm in a longitudinal study. This segmentation method may be useful in future studies to address radiation-induced xerostomia in head and neck radiation therapy.},
  doi      = {10.1016/j.ijrobp.2014.08.350},
  pmid     = {25442347},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0360301614040577},
}

@Article{Raudaschl2017,
  author   = {Raudaschl, Patrik F. and Zaffino, Paolo and Sharp, Gregory C. and Spadea, Maria Francesca and Chen, Antong and Dawant, Benoit M. and Albrecht, Thomas and Gass, Tobias and Langguth, Christoph and Luthi, Marcel and Jung, Florian and Knapp, Oliver and Wesarg, Stefan and Mannion-Haworth, Richard and Bowes, Mike and Ashman, Annaliese and Guillard, Gwenael and Brett, Alan and Vincent, Graham and Orbes-Arteaga, Mauricio and Cardenas-Pena, David and Castellanos-Dominguez, German and Aghdasi, Nava and Li, Yangming and Berens, Angelique and Moe, Kris and Hannaford, Blake and Schubert, Rainer and Fritscher, Karl D.},
  journal  = {Medical Physics},
  title    = {{Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015}},
  year     = {2017},
  issn     = {00942405},
  month    = {may},
  number   = {5},
  pages    = {2020--2036},
  volume   = {44},
  abstract = {Purpose: Automated delineation of structures and organs is a key step in medical imaging. However, due to the large number and diversity of structures and the large variety of segmentation algorithms, a consensus is lacking as to which automated segmentation method works best for certain applications. Segmentation challenges are a good approach for unbiased evaluation and comparison of segmentation algorithms. Methods: In this work, we describe and present the results of the Head and Neck Auto-Segmentation Challenge 2015, a satellite event at the Medical Image Computing and Computer Assisted Interventions (MICCAI) 2015 conference. Six teams participated in a challenge to segment nine structures in the head and neck region of CT images: brainstem, mandible, chiasm, bilateral optic nerves, bilateral parotid glands, and bilateral submandibular glands. Results: This paper presents the quantitative results of this challenge using multiple established error metrics and a well-defined ranking system. The strengths and weaknesses of the different auto-segmentation approaches are analyzed and discussed. Conclusions: The Head and Neck Auto-Segmentation Challenge 2015 was a good opportunity to assess the current state-of-the-art in segmentation of organs at risk for radiotherapy treatment. Participating teams had the possibility to compare their approaches to other methods under unbiased and standardized circumstances. The results demonstrate a clear tendency toward more general purpose and fewer structure-specific segmentation algorithms.},
  doi      = {10.1002/mp.12197},
  keywords = {Atlas-based segmentation,Automated segmentation,Model-based segmentation,Segmentation challenge},
  pmid     = {28273355},
  url      = {http://doi.wiley.com/10.1002/mp.12197},
}

@Article{Peng2015,
  author  = {Peng, Hanchuan and Meijering, Erik and Ascoli, Giorgio A.},
  journal = {Neuroinformatics},
  title   = {{From DIADEM to BigNeuron}},
  year    = {2015},
  issn    = {15392791},
  month   = {jul},
  number  = {3},
  pages   = {259--260},
  volume  = {13},
  doi     = {10.1007/s12021-015-9270-9},
  pmid    = {25920534},
  url     = {http://link.springer.com/10.1007/s12021-015-9270-9},
}

@Article{Peng2017,
  author  = {Peng, Hanchuan and Zhou, Zhi and Meijering, Erik and Zhao, Ting and Ascoli, Giorgio A. and Hawrylycz, Michael},
  journal = {Nature Methods},
  title   = {{Automatic tracing of ultra-volumes of neuronal images}},
  year    = {2017},
  issn    = {15487105},
  month   = {apr},
  number  = {4},
  pages   = {332--333},
  volume  = {14},
  doi     = {10.1038/nmeth.4233},
  pmid    = {28362437},
  url     = {http://www.nature.com/articles/nmeth.4233},
}

@Article{Shillcock2016,
  author   = {Shillcock, Julian C. and Hawrylycz, Michael and Hill, Sean and Peng, Hanchuan},
  journal  = {Brain Informatics},
  title    = {{Reconstructing the brain: from image stacks to neuron synthesis}},
  year     = {2016},
  issn     = {21984026},
  month    = {dec},
  number   = {4},
  pages    = {205--209},
  volume   = {3},
  abstract = {Large-scale brain initiatives such as the US BRAIN initiative and the European Human Brain Project aim to marshall a vast amount of data and tools for the purpose of furthering our understanding of brains. Fundamental to this goal is that neuronal morphologies must be seamlessly reconstructed and aggregated on scales up to the whole rodent brain. The experimental labor needed to manually produce this number of digital morphologies is prohibitively large. The BigNeuron initiative is assembling community-generated, open-source, automated reconstruction algorithms into an open platform, and is beginning to generate an increasing flow of high-quality reconstructed neurons. We propose a novel extension of this workflow to use this data stream to generate an unlimited number of statistically equivalent, yet distinct, digital morphologies. This will bring automated processing of reconstructed cells into digital neurons to the wider neuroscience community, and enable a range of morphologically accurate computational models.},
  doi      = {10.1007/s40708-016-0041-7},
  keywords = {Automated reconstruction,BigNeuron,Morphometric analysis,Neuron,Synthesis},
  url      = {http://link.springer.com/10.1007/s40708-016-0041-7},
}

@Article{Carreras2015,
  author   = {Carreras, Ignacio Arganda and Turaga, Srinivas C. and Berger, Daniel R. and San, Dan Cire and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M. and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D. and Bas, Erhan and Uzunbas, Mustafa G. and Cardona, Albert and Schindelin, Johannes and Seung, H. Sebastian},
  journal  = {Frontiers in Neuroanatomy},
  title    = {{Crowdsourcing the creation of image segmentation algorithms for connectomics}},
  year     = {2015},
  issn     = {16625129},
  month    = {nov},
  number   = {November},
  pages    = {1--13},
  volume   = {9},
  abstract = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic(EM) images of the brain. Participants submitted boundary map spredicted for a test set of images, and were scored based on their agreement with a on sensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This “deeplearning” approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from co-operation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test data set. Retrospective evaluation of the challenges coring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
  doi      = {10.3389/fnana.2015.00142},
  keywords = {Connectomics,Electron microscopy,Image segmentation,Machine learning,Reconstruction},
  url      = {http://journal.frontiersin.org/Article/10.3389/fnana.2015.00142/abstract},
}

@Article{Cardona2010,
  author   = {Cardona, Albert and Hartenstein, Volker and Saalfeld, Stephan and Preibisch, Stephan and Schmid, Benjamin and Cheng, Anchi and Pulokas, Jim and Tomancak, Pavel},
  journal  = {PLoS Biology},
  title    = {{An integrated micro- and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy}},
  year     = {2010},
  issn     = {15457885},
  month    = {oct},
  number   = {10},
  pages    = {e1000502},
  volume   = {8},
  abstract = {The analysis of microcircuitry (the connectivity at the level of individual neuronal processes and synapses), which is indispensable for our understanding of brain function, is based on serial transmission electron microscopy (TEM) or one of its modern variants. Due to technical limitations, most previous studies that used serial TEM recorded relatively small stacks of individual neurons. As a result, our knowledge of microcircuitry in any nervous system is very limited. We applied the software package TrakEM2 to reconstruct neuronal microcircuitry from TEM sections of a small brain, the early larval brain of Drosophila melanogaster. TrakEM2 enables us to embed the analysis of the TEM image volumes at the microcircuit level into a light microscopically derived neuro-anatomical framework, by registering confocal stacks containing sparsely labeled neural structures with the TEM image volume. We imaged two sets of serial TEM sections of the Drosophila first instar larval brain neuropile and one ventral nerve cord segment, and here report our first results pertaining to Drosophila brain microcircuitry. Terminal neurites fall into a small number of generic classes termed globular, varicose, axiform, and dendritiform. Globular and varicose neurites have large diameter segments that carry almost exclusively presynaptic sites. Dendritiform neurites are thin, highly branched processes that are almost exclusively postsynaptic. Due to the high branching density of dendritiform fibers and the fact that synapses are polyadic, neurites are highly interconnected even within small neuropile volumes. We describe the network motifs most frequently encountered in the Drosophila neuropile. Our study introduces an approach towards a comprehensive anatomical reconstruction of neuronal microcircuitry and delivers microcircuitry comparisons between vertebrate and insect neuropile. {\textcopyright} 2010 Cardona et al.},
  doi      = {10.1371/journal.pbio.1000502},
  editor   = {Harris, Kristen M.},
  url      = {https://dx.plos.org/10.1371/journal.pbio.1000502},
}

@Article{Koenders2016,
  author   = {Koenders, Laura and Cousijn, Janna and Vingerhoets, Wilhelmina A.M. and {Van Den Brink}, Wim and Wiers, Reinout W. and Meijer, Carin J. and Machielsen, Marise W.J. and Veltman, Dick J. and Goudriaan, Anneke E. and {De Haan}, Lieuwe},
  journal  = {PLoS ONE},
  title    = {{Grey matter changes associated with heavy cannabis use: A longitudinal sMRI study}},
  year     = {2016},
  issn     = {19326203},
  month    = {may},
  number   = {5},
  pages    = {e0152482},
  volume   = {11},
  abstract = {Cannabis is the most frequently used illicit drug worldwide. Cross-sectional neuroimaging studies suggest that chronic cannabis exposure and the development of cannabis use disorders may affect brain morphology. However, cross-sectional studies cannot make a conclusive distinction between cause and consequence and longitudinal neuroimaging studies are lacking. In this prospective study we investigate whether continued cannabis use and higher levels of cannabis exposure in young adults are associated with grey matter reductions. Heavy cannabis users (N = 20, age baseline M = 20.5, SD = 2.1) and non-cannabis using healthy controls (N = 22, age baseline M = 21.6, SD = 2.45) underwent a comprehensive psychological assessment and a T1- structural MRI scan at baseline and 3 years follow-up. Grey matter volumes (orbitofrontal cortex, anterior cingulate cortex, insula, striatum, thalamus, amygdala, hippocampus and cerebellum) were estimated using the software package SPM (VBM-8 module). Continued cannabis use did not have an effect on GM volume change at follow-up. Cross-sectional analyses at baseline and follow-up revealed consistent negative correlations between cannabis related problems and cannabis use (in grams) and regional GM volume of the left hippocampus, amygdala and superior temporal gyrus. These results suggests that small GM volumes in the medial temporal lobe are a risk factor for heavy cannabis use or that the effect of cannabis on GM reductions is limited to adolescence with no further damage of continued use after early adulthood. Long-term prospective studies starting in early adolescence are needed to reach final conclusions.},
  doi      = {10.1371/journal.pone.0152482},
  editor   = {Chen, Kewei},
  pmid     = {27224247},
  url      = {https://dx.plos.org/10.1371/journal.pone.0152482},
}

@Article{Kasthuri2015,
  author   = {Kasthuri, Narayanan and Hayworth, Kenneth Jeffrey and Berger, Daniel Raimund and Schalek, Richard Lee and Conchello, Jos{\'{e}} Angel and Knowles-Barley, Seymour and Lee, Dongil and V{\'{a}}zquez-Reina, Amelio and Kaynig, Verena and Jones, Thouis Raymond and Roberts, Mike and Morgan, Josh Lyskowski and Tapia, Juan Carlos and Seung, H. Sebastian and Roncal, William Gray and Vogelstein, Joshua Tzvi and Burns, Randal and Sussman, Daniel Lewis and Priebe, Carey Eldin and Pfister, Hanspeter and Lichtman, Jeff William},
  journal  = {Cell},
  title    = {{Saturated Reconstruction of a Volume of Neocortex}},
  year     = {2015},
  issn     = {10974172},
  month    = {jul},
  number   = {3},
  pages    = {648--661},
  volume   = {162},
  abstract = {We describe automated technologies to probe the structure of neural tissue at nanometer resolution and use them to generate a saturated reconstruction of a sub-volume of mouse neocortex in which all cellular objects (axons, dendrites, and glia) and many sub-cellular components (synapses, synaptic vesicles, spines, spine apparati, postsynaptic densities, and mitochondria) are rendered and itemized in a database. We explore these data to study physical properties of brain tissue. For example, by tracing the trajectories of all excitatory axons and noting their juxtapositions, both synaptic and non-synaptic, with every dendritic spine we refute the idea that physical proximity is sufficient to predict synaptic connectivity (the so-called Peters' rule). This online minable database provides general access to the intrinsic complexity of the neocortex and enables further data-driven inquiries. Video Abstract},
  doi      = {10.1016/j.cell.2015.06.054},
  pmid     = {26232230},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0092867415008247},
}

@Article{Horikawa2017,
  author        = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
  journal       = {Nature Communications},
  title         = {{Generic decoding of seen and imagined objects using hierarchical visual features}},
  year          = {2017},
  issn          = {20411723},
  month         = {aug},
  number        = {1},
  pages         = {15037},
  volume        = {8},
  abstract      = {Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
  archiveprefix = {arXiv},
  arxivid       = {1510.06479},
  doi           = {10.1038/ncomms15037},
  eprint        = {1510.06479},
  pmid          = {28530228},
  url           = {http://www.nature.com/articles/ncomms15037},
}

@Book{Wallwork2016,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Presentations at International Conferences}},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-26328-1},
  abstract  = {This book is designed to help non-native English speakers to prepare and deliver effective presentations at international conferences. It will be the first book ever written on presentations specifically from the perspective of non-native English speakers. It will be written in an English that readers will be able to understand easily. This is not 'simple' English as a native speaker would interpret it, but a particular way of writing with minimal redundancy in which key points are highlighted clearly. This book will cover not only the typical difficulties of all presenters (structure, gaining.},
  booktitle = {English for Presentations at International Conferences},
  doi       = {10.1007/978-3-319-26330-4},
  url       = {http://link.springer.com/10.1007/978-3-319-26330-4},
}

@Book{Wallwork2013,
  author    = {Wallwork, Adrian},
  publisher = {Springer US},
  title     = {{English for Research: Grammar, Usage and Style}},
  year      = {2013},
  address   = {Boston, MA},
  isbn      = {978-1-4614-1592-3},
  doi       = {10.1007/978-1-4614-1593-0},
  url       = {http://link.springer.com/10.1007/978-1-4614-1593-0},
}

@Book{Wallwork2013a,
  author    = {Wallwork, Adrian},
  publisher = {Springer US},
  title     = {{English for Academic Research: Grammar Exercises}},
  year      = {2013},
  address   = {Boston, MA},
  isbn      = {978-1-4614-4288-2},
  abstract  = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  booktitle = {English for Academic Research: Grammar Exercises},
  doi       = {10.1007/978-1-4614-4289-9},
  url       = {http://link.springer.com/10.1007/978-1-4614-4289-9},
}

@Book{Wallwork2019,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Academic CVs, Resumes, and Online Profiles}},
  year      = {2019},
  address   = {Cham},
  isbn      = {978-3-030-11090-1},
  series    = {English for Academic Research},
  abstract  = {Are you a graduate, postgraduate or PhD student? Building a CV or profile can be difficult for anyone, but especially for those whose first language is not English. This book is essential for those looking to promote themselves in the academic community, and can be used both for self-study, as well as in an English for Academic Purposes (EAP) course. The book contains tips, do's and dont's, and discussion points that can be used by instructors. Based on interviews with recruiters and an analysis of hundreds of CVs from around 40 different countries, the book is structured as a series of FAQs. Topics covered include: how recruiters and HR people analyse a CV whether using a template is a good idea how to present your personal details and whether to include a photo how to write an Objective and a personal profile what to write in each section (Education, Work Experience, Skills, Personal Interests) how to highlight your language, communication and team skills how to get and write references The last chapter of the book contains a simple template to help you get the job of your dreams! Other books in this series include: English for Writing Research Papers English for Research: Usage, Style, and Grammar English for Presentations at International Conferences English for Academic Research: Grammar / Vocabulary / Writing Exercises English for Academic Correspondence English for Interacting on Campus Adrian Wallwork is the author of over 40 books aimed at helping non-native English speakers to communicate more effectively in English. He has published with SpringerNature, Oxford University Press, Cambridge University Press, Scholastic, BEP and the BBC.},
  booktitle = {English for Academic Research},
  doi       = {10.1007/978-3-030-11090-1},
  keywords  = {04 Teaching,Career Skills,Career education,Careers,Careers in Business and Management,English,English language,Grammar,Language Education,Language and education,Linguistics,PDF,Popular Science in Linguistics,Success in business},
  url       = {http://link.springer.com/10.1007/978-3-030-11090-1},
}

@Book{Wallwork2016a,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Academic Research: A Guide for Teachers}},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-32685-6},
  abstract  = {INTRODUCTION: Liver transplantation is an accepted and successful therapy for both acute and chronic liver diseases (CLDs), with good survival outcomes. Whilst the study of health-related quality of life (HRQoL) post transplantation for CLDs have been well documented, there is little data measuring HRQoL following liver transplantation for acute liver failure (ALF) patients. PATIENTS AND METHODS: Data were collected using between-method triangulation; however, only the quantitative element of the study is reported here. Measuring eight health domains, we distributed the short form 36 (SF-36) questionnaire by post to 96 acute and chronic transplant recipients. Differences between the groups were measured using both parametric and non-parametric t-tests. RESULTS: Overall, the patients showed a satisfactory HRQoL; there were no differences between either acute or chronic transplant groups in seven of the eight domains of quality of life. Among the patients transplanted for ALF, there were no differences in HRQoL between patients transplanted for paracetamol hepatotoxicity compared with other indications, and no variations in HRQoL related to recipient gender, employment or length of survival post transplantation. When compared with the UK SF-36 normal values to the ALF transplant recipients, there was a significantly lower physical functioning and role emotional scores. CONCLUSION: Regardless of aetiology, most of recipients transplanted for ALF have a HRQoL comparable with chronic transplant recipients.},
  booktitle = {English for Academic Research: A Guide for Teachers},
  doi       = {10.1007/978-3-319-32687-0},
  url       = {http://link.springer.com/10.1007/978-3-319-32687-0},
}

@Book{Parija2018,
  editor    = {Parija, Subhash Chandra and Kate, Vikram},
  publisher = {Springer Singapore},
  title     = {{Thesis Writing for Master's and Ph.D. Program}},
  year      = {2018},
  address   = {Singapore},
  isbn      = {978-981-13-0889-5},
  abstract  = {Thesis writing is one of the most crucial rites of passage in the postgraduate study period. Yet, for many, it remains the most arduous one as well. What is meant to be an exercise in gaining in-depth knowledge of research methodology often ends up as a slapdash production from the part of the harried graduate student. The differ- ence, we believe, lies in a proper and methodical approach to thesis/dissertation writing. Right from the beginning of the research project, one must be clear with the topic, the objectives, and the methods. While choosing a topic, one must keep in mind several factors such as the appropriateness, the feasibility, and, above all, one's interest in that particular area. The importance of securing clearances from the appropriate monitoring bodies at the correct time cannot be stressed enough. This exercise also gives the students an invaluable opportunity to learn various aspects of statistics, and this will go a long way in their future careers.},
  booktitle = {Thesis Writing for Master's and Ph.D. Program},
  doi       = {10.1007/978-981-13-0890-1},
  url       = {http://link.springer.com/10.1007/978-981-13-0890-1},
}

@Book{Wallwork2016b,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Writing Research Papers}},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-26092-1},
  abstract  = {Publishing your research in an international journal is key to your success in academia. This guide is based on a study of referees' reports and letters from journal editors on reasons why papers written by non-native researchers are rejected due to problems with English usage. It draws on English-related errors from around 5000 papers written by non-native authors, 500 abstrac..., (展开全部), Adrian Wallwork is the author of around 30 English Language Teaching textbooks published by: Springer Science, Oxford University Press, BBC, Cambridge University Press, Scholastic, De Agostini, and Vallardi., He lives and works in Pisa, Italy where he enjoys the sun, the wine, and teaching PhD students from all over the world., His dream book, which he hopes to have published in ..., (展开全部)},
  booktitle = {English for Writing Research Papers},
  doi       = {10.1007/978-3-319-26094-5},
  url       = {http://link.springer.com/10.1007/978-3-319-26094-5},
}

@Book{Baines2014,
  author    = {Baines, Lawrence},
  publisher = {SensePublishers},
  title     = {{Project-based writing in science}},
  year      = {2014},
  address   = {Rotterdam},
  isbn      = {9789462096714},
  abstract  = {Turn your students into scientists who use their knowledge and creativity to solve real-world problems. Each lesson features a step-by-step guide; a summary of recent research; and handouts that are classroom-ready. Learn about the three levels of writing, from a Level 1 quickwrite to a formal, multi-part, Level 3 research paper. Each writing assignment-narrative, persuasive, and informative-includes a detailed rubric that makes grading easy. Students collaborate to contain an outbreak of avian flu, lead a group of people trying to survive under harsh conditions, battle drought in a densely-populated city in the American southwest, research the behavior of animals in the local region, and calculate their own speed, velocity, and momentum. Engaging and demanding, Project-Based Writing in Science helps students to understand and improve the world.},
  booktitle = {Project-Based Writing in Science},
  doi       = {10.1007/978-94-6209-671-4},
  pages     = {1--108},
  url       = {http://link.springer.com/10.1007/978-94-6209-671-4},
}

@Book{Blackwell2011,
  author    = {Blackwell, John and Martin, Jan},
  publisher = {Springer New York},
  title     = {{A Scientific Approach to Scientific Writing}},
  year      = {2011},
  address   = {New York, NY},
  isbn      = {978-1-4419-9787-6},
  abstract  = {In the modern world, every scientist who wants to publish findings in an interna- tional, peer-reviewed journal must write in English. This can be very challenging for people who are not native speakers of English. Indeed, it can be challenging for people who are native speakers. However, whether you are writing papers in your first or any other language, the process can be greatly facilitated by approaching it in a logical, systematic manner. Writing a paper is not easy, especially if you are not writing in your first language, but approaching the task methodically can ease the process, by: Delimiting the study. Writing brief (subsequently expanded) statements describing the rationale and specific objective(s) of the study, what was done, the main findings, novel aspects and limitations (focus) of the analyses, and finally the implications of the findings. Using these statements to compose each section of the paper, which should be written clearly and simply, following the target journals Instructions for authors, providing clear figures and tables, where appropriate. This procedure will help to maximize the chances of your paper being accepted. However, do not be too dismayed if it is rejected this does not necessarily mean that your work has no merit.},
  booktitle = {A Scientific Approach to Scientific Writing},
  doi       = {10.1007/978-1-4419-9788-3},
  url       = {http://link.springer.com/10.1007/978-1-4419-9788-3},
}

@Book{Evans2013,
  author    = {Evans, Kate},
  publisher = {SensePublishers},
  title     = {{Pathways through writing blocks in the academic environment}},
  year      = {2013},
  address   = {Rotterdam},
  isbn      = {9789462092426},
  abstract  = {Writing blocks are likely to strike any writer, even experienced ones, at sometime or another. Academia has its own challenges which can provoke blocks particular to that environment. Drawing on her knowledge as writer, psychotherapeutic counsellor and university tutor, Kate Evans has put together a book which addresses many of the differing aspects of writing blocks, including looking at their emotional and psychological foundations. With discussion and practical exercises, this volume suggests that an infusion of creative techniques can offer pathways through writing blocks in the academic environment. The case studies provide an in-depth consideration of varying experiences of writing blocks. The book is aimed at students with essays, projects or reports to write, or theses to tackle; as well as academics who are working on articles and books. It will also offer insights for supervisors who wish to support those who are writing and guidance for people running writing groups within academia. Over-all the book encourages a creative, collaborative approach which aims to equip academics for writing within the context of the twenty-first century. "This book offers something for every academic writer, whether budding or experienced. Students struggling with essays and dissertations will find many practical exercises along with invaluable advice. More practised writers will encounter fresh insights{\ldots} I am confident that you, the reader, will enjoy this book, which is itself a model of good writing." Dr Linda Finlay, the Open University, UK.},
  booktitle = {Pathways through Writing Blocks in the Academic Environment},
  doi       = {10.1007/978-94-6209-242-6},
  pages     = {1--148},
  url       = {http://link.springer.com/10.1007/978-94-6209-242-6},
}

@Book{Koelsch2016,
  author    = {Koelsch, George},
  publisher = {Apress},
  title     = {{Requirements Writing for System Engineering}},
  year      = {2016},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-2098-6},
  doi       = {10.1007/978-1-4842-2099-3},
  url       = {http://link.springer.com/10.1007/978-1-4842-2099-3},
}

@Book{Parija2017,
  author    = {Parija, Subhash Chandra and Kate, Vikram},
  editor    = {Parija, Subhash Chandra and Kate, Vikram},
  publisher = {Springer Singapore},
  title     = {{Writing and publishing a scientific research paper}},
  year      = {2017},
  address   = {Singapore},
  isbn      = {9789811047206},
  abstract  = {This book covers all essential aspects of writing scientific research articles, presenting eighteen carefully selected titles that offer essential, "must-know" content on how to write high-quality articles. The book also addresses other, rarely discussed areas of scientific writing including dealing with rejected manuscripts, the reviewer's perspective as to what they expect in a scientific article, plagiarism, copyright issues, and ethical standards in publishing scientific papers. Simplicity is the book's hallmark, and it aims to provide an accessible, comprehensive and essential resource for those seeking guidance on how to publish their research work. The importance of publishing research work cannot be overemphasized. However, a major limitation in publishing work in a scientific journal is the lack of information on or experience with scientific writing and publishing. Young faculty and trainees who are starting their research career are in need of a comprehensive guide that provides all essential components of scientific writing and aids them in getting their research work published.},
  booktitle = {Writing and Publishing a Scientific Research Paper},
  doi       = {10.1007/978-981-10-4720-6},
  pages     = {1--195},
  url       = {http://link.springer.com/10.1007/978-981-10-4720-6},
}

@Book{Rogers2014,
  author    = {Rogers, Silvia M.},
  publisher = {Springer Berlin Heidelberg},
  title     = {{Mastering Scientific and Medical Writing}},
  year      = {2014},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-642-39445-4},
  doi       = {10.1007/978-3-642-39446-1},
  url       = {http://link.springer.com/10.1007/978-3-642-39446-1},
}

@Book{Alley2018,
  author    = {Alley, Michael},
  publisher = {Springer New York},
  title     = {{The craft of scientific writing: Fourth edition}},
  year      = {2018},
  address   = {New York, NY},
  isbn      = {9781441982889},
  abstract  = {The Craft of Scientific Writing is designed to help scientists and engineers - both professionals already active in the disciplines as well as students preparing to enter the professions - write about their work clearly and effectively. Written for use as a text in courses on scientific writing, the book includes many useful suggestions about approaching a wide variety of writing tasks from journal papers to grant proposals and from emails to formal reports, as well as a concise guide to style and usage appropriate for scientific writing. Also useful for self-study, the book will be an important reference for all scientists and engineers who need to write about their work. With this new and updated fourth edition, while most technical writing texts have gotten larger over the years, this one has streamlined, to provide busy readers with the essence of what distinguishes the style of the best scientific documents. With this new edition, readers will learn not just how to organize information, but how to emphasize the key details of that information. Also, readers will not just learn how to cast their ideas into precise and clear sentences, but how to connect these sentences in an energetic fashion. In the section on language, the new edition goes into much depth about how to make connections between ideas: an important issue that few technical writing texts address. Moreover, the new edition integrates the discussion of illustrations with language because those two aspects of style are so intertwined. Finally, the new edition does a better job of explaining how to make the process of writing more efficient. From a review of the first edition: "A refreshing addition to a genre dominated by English teacher-style textbooks. Instead of listing rules that constrain writers, the book uses examples to lay out the path to successful communication ... Especially helpful (and entertaining) is the chapter on the writing process. Anyone who has spent more time avoiding a writing task than actually doing it will appreciate Alley's tips." -Dr. Ellen Ochoa, Deputy Director of Flight Crew Operations, Johnson Space Center.},
  booktitle = {The Craft of Scientific Writing: Fourth Edition},
  doi       = {10.1007/978-1-4419-8288-9},
  pages     = {1--298},
  url       = {http://link.springer.com/10.1007/978-1-4419-8288-9},
}

@Book{Blair2016,
  author    = {Blair, Lorrie},
  publisher = {SensePublishers},
  title     = {{Writing a Graduate Thesis or Dissertation}},
  year      = {2016},
  address   = {Rotterdam},
  isbn      = {978-94-6300-426-8},
  abstract  = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  booktitle = {Writing a Graduate Thesis or Dissertation},
  doi       = {10.1007/978-94-6300-426-8},
  url       = {http://link.springer.com/10.1007/978-94-6300-426-8},
}

@Book{Carter2020,
  author    = {Carter, Susan and Guerin, Cally and Aitchison, Claire},
  publisher = {Springer Singapore},
  title     = {{Doctoral writing: Practices, processes and pleasures}},
  year      = {2020},
  address   = {Singapore},
  isbn      = {9789811518089},
  abstract  = {This book on doctoral writing offers a refreshingly new approach to help Ph.D. students and their supervisors overcome the host of writing challenges that can make—or break—the dissertation process. The book's unique contribution to the field of doctoral writing is its style of reflection on ongoing, lived practice; this is more readable than a simple how-to book, making it a welcome resource to support doctoral writing. The experiences and practices of research writing are explored through bite-sized vignettes, stories, and actionable ‘teachable' accounts. Doctoral Writing: Practices, Processes and Pleasures has its origins in a highly successful academic blog with an international following. Inspired by the popularity of the blog (which had more than 14, 800 followers as of October 2019) and a desire to make our six years' worth of posts more accessible, this book has been authored, reworked, and curated by the three editors of the blog and reconceived as a conveniently structured book.},
  booktitle = {Doctoral Writing: Practices, Processes and Pleasures},
  doi       = {10.1007/978-981-15-1808-9},
  keywords  = {Academic Writing for Graduate Students,Academic literacy for research students,Avoiding plagiarism in doctoral writing,Being and developing doctoral writers,Communicating practices in doctoral writing,Crafting doctoral writing,Disseminating research,Good writing habits for PhD students,Managing writing productivity,Overcoming writer's block,Publishing from a PhD thesis,Research communication and writing,Supervising and advising doctoral writing,Supporting doctoral writers,Thesis and Dissertation Writing,Voice and clarity in thesis writing,Writing Thesis Acknowledgments},
  pages     = {1--219},
  url       = {http://link.springer.com/10.1007/978-981-15-1808-9},
}

@Book{Myatt2016,
  author    = {Myatt, Alice Johnston and Gaillet, Lyn{\'{e}}e Lewis},
  editor    = {Myatt, Alice Johnston and Gaillet, Lyn{\'{e}}e Lewis},
  publisher = {Palgrave Macmillan US},
  title     = {{Writing program and writing center collaborations: Transcending boundaries}},
  year      = {2016},
  address   = {New York},
  isbn      = {9781137599322},
  abstract  = {This book demonstrates how to develop and engage in successful academic collaborations that are both practical and sustainable across campuses and within local communities. Authored by experienced writing program administrators, this edited collection includes a wide range of information addressing collaborative partnerships and projects, theoretical explorations of collaborative praxis, and strategies for sustaining collaborative initiatives. Contributors offer case studies of writing program collaborations and honestly address both the challenges of academic collaboration and the hallmarks of successful partnerships.},
  booktitle = {Writing Program and Writing Center Collaborations: Transcending Boundaries},
  doi       = {10.1057/978-1-137-59932-2},
  pages     = {1--275},
  url       = {http://link.springer.com/10.1057/978-1-137-59932-2},
}

@Book{Zobel2014,
  author    = {Zobel, Justin},
  publisher = {Springer London},
  title     = {{Writing for Computer Science}},
  year      = {2014},
  address   = {London},
  isbn      = {978-1-4471-6638-2},
  abstract  = {The elements of good writing are an essential part of success in science. With comprehensive practical help for students and experienced researchers, Writing for Computer Science: - Gives extensive guidance for writing style and editing; - Presents sound practice for graphs, figures, and tables; - Guides the presentation of mathematics, algorithms and experiments; - Shows how to assemble research materials into a technical paper; - Offers guidelines and advice on spoken presentations. This second edition contains detailed new material on research methods, the how-to of being a scientist, including: - Development of ideas into research programs; -Design and evaluation of experiments; - How to search for, read, evaluate, and referee other research; - Research ethics and the qualities that separate good and bad science. Writing for Computer Science is not only an introduction to the doing and describing of research, but is a handy reference for working scientists in computing and mathematical sciences.},
  booktitle = {Writing for Computer Science},
  doi       = {10.1007/978-1-4471-6639-9},
  url       = {http://link.springer.com/10.1007/978-1-4471-6639-9},
}

@Book{Datta2017,
  author    = {Datta, Dilip},
  publisher = {Springer International Publishing},
  title     = {{LaTeX in 24 Hours}},
  year      = {2017},
  address   = {Cham},
  isbn      = {978-3-319-47830-2},
  abstract  = {The need to largely reduce the amount of Carbon Dioxide (CO2) emissions in the coming years all over the world requires a large effort in decarbonising the economy. One of the sectors most in need of this effort is the transportation sector. In fact, only a large reduction of CO2 emissions in this sector will allow coping effectively with this problem. There are two ways to perform it (1) by increasing the amount of biofuels to be used by Internal Combustion Motors or (2) by making a shift towards electromobility. However, this shift towards the electrification of the transportation sector can only be well succeeded if one increases simultaneously the proportion of non-CO2-emitting power generation technologies, namely renewable based power sources. European Union (EU) is developing a large effort on these matters. In fact, the energy-related targets set by EU policy require careful exami- nation of potential solutions for the integration of renewable energy sources to meet the electricity demand. On the other side, the expected growing energy demand resulting from the introduction of electric-powered cars needs the development of innovative concepts to exploit the variable power supply. The application of dynamic techniques for prediction of electricity supply and demand, including electricity prices in the market, is expected to support the optimisation of the grid balance. The European wind markets predict an installed capacity that would provide 14 % of the electricity consumption in 2020. Today in Denmark and Portugal, the wind power accounts for more than 20 % of the power production.},
  booktitle = {LaTeX in 24 Hours},
  doi       = {10.1007/978-3-319-47831-9},
  url       = {http://link.springer.com/10.1007/978-3-319-47831-9},
}

@Book{Lantsoght2018,
  author    = {Lantsoght, Eva O L},
  publisher = {Springer International Publishing},
  title     = {{The A-Z of the PhD Trajectory}},
  year      = {2018},
  address   = {Cham},
  isbn      = {9783319774244},
  series    = {Springer Texts in Education},
  abstract  = {This textbook is a guide to success during the PhD trajectory. The first part of this book takes the reader through all steps of the PhD trajectory, and the second part contains a unique glossary of terms and explanation relevant for PhD candidates. Written in the accessible language of the PhD Talk blogs, the book contains a great deal of practical advice for carrying out research, and presenting one's work. It includes tips and advice from current and former PhD candidates, thus representing a broad range of opinions. The book includes exercises that help PhD candidates get their work kick-started. It covers all steps of a doctoral journey in STEM: getting started in a program, planning the work, the literature review, the research question, experimental work, writing, presenting, online tools, presenting at one's first conference, writing the first journal paper, writing and defending the thesis, and the career after the PhD. Since a PhD trajectory is a deeply personal journey, this book suggests methods PhD candidates can try out, and teaches them how to figure out for themselves which proposed methods work for them, and how to find their own way of doing things.},
  booktitle = {Springer Texts in Education},
  doi       = {10.1007/978-3-319-77425-1},
  issn      = {2366-7672},
  url       = {http://link.springer.com/10.1007/978-3-319-77425-1},
}

@Book{Pequegnat2011,
  author    = {Pequegnat, Willo and Stover, Ellen and Boyce, Cheryl Anne},
  editor    = {Pequegnat, Willo and Stover, Ellen and Boyce, Cheryl Anne},
  publisher = {Springer US},
  title     = {{How to write a successful research grant application: A guide for social and behavioral scientists: Second edition}},
  year      = {2011},
  address   = {Boston, MA},
  isbn      = {9781441914538},
  abstract  = {Over the last fifty years behavioral and medical research has been generously supported by the federal government, private foundations, and other philanthropic organizations contributing to the development of a vibrant public health system both in the United States and worldwide. However, these funds are dwindling and to stay competitive, investigators must understand the funding environment and know how to translate their hypotheses into research grant applications that reviewers evaluate as having scientific merit. The Second Edition of How to Write a Successful Research Grant Application is the only book of its kind written by federal research investigators which provides technical assistance for researchers applying for biobehavioral and psychosocial research funding and can give them an edge in this competitive environment. The book provides invaluable tips on all aspects of the art of grantsmanship, including: how to determine research opportunities and priorities, how to develop the different elements of an application, how to negotiate the electronic submission and review processes, and how to disseminate the findings. Charts, visual aids, Web links, an extensive real-world example of a research proposal with budget, and a "So You Were Awarded Your Grant-Now What?" chapter show prospective applicants how to: Formulate a testworthy-and interesting- hypothesis. Select the appropriate research mechanism. Avoid common pitfalls in proposal writing. Develop an adequate control group. Conduct a rigorous qualitative inquiry. Develop a budget justification of costs. Develop a human subjects of animal welfare plan. Write a data analytic plan. Design a quality control/assurance program. Read between the lines of a summary of the review of your application. Although its focus is on Public Health Service funding, How to Write a Successful Research Grant is equally useful for all research proposals, including graduate students preparing a thesis or dissertation proposal. Service providers in community-based organizations and public health agencies will also find this a useful resource in preparing a proposal to compete for grant funds from state and community resources, non-government organizations, and foundations. {\textcopyright} Springer Science+Business Media, LLC 2011. All rights reserved.},
  booktitle = {How to Write a Successful Research Grant Application: A Guide for Social and Behavioral Scientists: Second Edition},
  doi       = {10.1007/978-1-4419-1454-5},
  pages     = {1--386},
  url       = {http://link.springer.com/10.1007/978-1-4419-1454-5},
}

@Book{Roos2019,
  author    = {Roos, Cathryn and Roos, Gregory},
  publisher = {Springer Singapore},
  title     = {{Real Science in Clear English}},
  year      = {2019},
  address   = {Singapore},
  isbn      = {978-981-13-7819-5},
  series    = {SpringerBriefs in Education},
  doi       = {10.1007/978-981-13-7820-1},
  url       = {http://link.springer.com/10.1007/978-981-13-7820-1},
}

@Book{Hering2010,
  author    = {Hering, Lutz and Hering, Heike},
  publisher = {Springer Berlin Heidelberg},
  title     = {{How to Write Technical Reports}},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-540-69928-6},
  doi       = {10.1007/978-3-540-69929-3},
  url       = {http://link.springer.com/10.1007/978-3-540-69929-3},
}

@Book{Mardan2019,
  author    = {Mardan, Azat},
  publisher = {Apress},
  title     = {{Write Your Way To Success}},
  year      = {2019},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-3969-8},
  booktitle = {Write Your Way To Success},
  doi       = {10.1007/978-1-4842-3970-4},
  url       = {http://link.springer.com/10.1007/978-1-4842-3970-4},
}

@Book{Greenshields2017,
  author    = {Greenshields, Will},
  publisher = {Springer International Publishing},
  title     = {{Writing the Structures of the Subject}},
  year      = {2017},
  address   = {Cham},
  isbn      = {978-3-319-47532-5},
  abstract  = {W. Greenshields, Writing the Structures of the Subject,
The Palgrave Lacan Series, DOI 10.1007/978-3-319-47533-2_3

This book examines and explores Jacques Lacan's controversial topologisation of psychoanalysis, and seeks to persuade the reader that this enterprise was necessary and important. In providing both an introduction to a fundamental component of Lacan's theories, as well as readings of texts that have been largely ignored, it provides a thorough critical interpretation of his work. Will Greenshields argues that Lacan achieved his most pedagogically clear and successful presentations of his most essential and notoriously complex concepts – such as structure, the subject and the real – through the deployment of topology. The book will help readers to better understand Lacan, and also those concepts that have become prevalent in various intellectual discourses such as contemporary continental philosophy, politics and the study of ideology, and literary or cultural criticism.},
  booktitle = {Writing the Structures of the Subject},
  doi       = {10.1007/978-3-319-47533-2},
  url       = {http://link.springer.com/10.1007/978-3-319-47533-2},
}

@Book{Hering2019,
  author    = {Hering, Heike},
  publisher = {Springer Berlin Heidelberg},
  title     = {{How to Write Technical Reports}},
  year      = {2019},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-662-58105-6},
  doi       = {10.1007/978-3-662-58107-0},
  url       = {http://link.springer.com/10.1007/978-3-662-58107-0},
}

@Book{Haight1988,
  author    = {Haight, Kristina},
  publisher = {Springer International Publishing},
  title     = {{Writing for Publication}},
  year      = {1988},
  address   = {Cham},
  isbn      = {978-3-319-31648-2},
  number    = {3},
  series    = {Springer Texts in Education},
  volume    = {6},
  booktitle = {Home Healthcare Nurse},
  doi       = {10.1097/00004045-198805000-00015},
  issn      = {15390713},
  pages     = {39},
  url       = {http://link.springer.com/10.1007/978-3-319-31650-5},
}

@Book{Henderson2020,
  author    = {Henderson, Brad},
  publisher = {Springer International Publishing},
  title     = {{A Math-Based Writing System for Engineers}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-10754-3},
  booktitle = {A Math-Based Writing System for Engineers},
  doi       = {10.1007/978-3-030-10756-7},
  url       = {http://link.springer.com/10.1007/978-3-030-10756-7},
}

@Book{Mu2015,
  author    = {Mu, Congjun},
  publisher = {Springer Netherlands},
  title     = {{Writing and publishing science research papers in english: A global perspective}},
  year      = {2015},
  address   = {Dordrecht},
  isbn      = {9789400777149},
  series    = {SpringerBriefs in Education},
  volume    = {39},
  abstract  = {This book provides a comprehensive review of the current knowledge on writing and publishing scientific research papers and the social contexts. It deals with both English and non-Anglophone science writers, and presents a global perspective and an international focus. The book collects and synthesizes research from a range of disciplines, including applied linguistics, the sociology of science, sociolinguistics, bibliometrics, composition studies, and science education. This multidisciplinary approach helps the reader gain a solid understanding of the subject. Divided into three parts, the book considers the context of scientific papers, the text itself, and the people involved. It explains how the typical sections of scientific papers are structured. Standard English scientific writing style is also compared with science papers written in other languages. The book discusses the strengths and challenges faced by people with different degrees of science writing expertise and the role of journal editors and reviewers. The Rise of English as the Language of Science -- Measuring the Impact of Articles, Journals and Nations -- English Competence, Funds for Research, and Publishing Success -- Collaborations, Teams and Networks -- The Scientific Research Article and the Creation of Science -- Varieties of Science Texts -- Structure of the Research Article in the Creation of Knowledge -- Writing the Five Principal Sections: Abstract, Introduction, Methods, Results and Discussion -- Variations in Different Languages and Cultures -- Graduate Students Becoming Scientists -- Novice Scientists and Expert Scientists -- English-Speaking Scientists and Multilingual Scientists -- Gatekeepers, Guardians and Allies -- Afterword: Negotiating Research Article Writing and Publication.},
  booktitle = {English for Specific Purposes},
  doi       = {10.1016/j.esp.2014.12.004},
  issn      = {08894906},
  pages     = {76--78},
  url       = {http://link.springer.com/10.1007/978-94-007-7714-9},
}

@Book{Daepp2011,
  author    = {Daepp, Ulrich and Gorkin, Pamela},
  publisher = {Springer New York},
  title     = {{Reading, Writing, and Proving}},
  year      = {2011},
  address   = {New York, NY},
  isbn      = {978-1-4419-9478-3},
  series    = {Undergraduate Texts in Mathematics},
  doi       = {10.1007/978-1-4419-9479-0},
  url       = {http://link.springer.com/10.1007/978-1-4419-9479-0},
}

@Book{Wallwork2016c,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Academic Correspondence}},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-26433-2},
  abstract  = {Written specifically for researchers of all disciplines whose first language is not English, this guide presents easy-to-follow rules and tips, along with authentic examples taken from real emails, referees' reports and cover letters, will show you how to: write effective emails (subject lines, structure, requests, level of formality) review other people's manuscripts reply effectively and constructively to referees' reports correspond with editors write letters regarding summer schools, internships, and PhD and postdoc programs write reference letters This new edition contains over 40% new material, including stimulating factoids and discussion points both for self-study and in-class use, as well as suggestions for drafting proposals for research projects and writing research statements. EAP teachers will find this book to be a great source of tips for training students, and for providing both instructive and entertaining lessons.},
  booktitle = {English for Academic Correspondence},
  doi       = {10.1007/978-3-319-26435-6},
  url       = {http://link.springer.com/10.1007/978-3-319-26435-6},
}

@Book{Wallwork2016d,
  author    = {Wallwork, Adrian},
  publisher = {Springer International Publishing},
  title     = {{English for Interacting on Campus}},
  year      = {2016},
  address   = {Cham},
  isbn      = {978-3-319-28732-4},
  abstract  = {This volume covers the day-to-day activities of a non-native English speaking student carrying out research, attending lectures, socializing, and living in a foreign country. Whether on a US campus as a foreign student, or in a non-English speaking country where classes are given in English, this book will help students build confidence in interacting with professors and fellow students.},
  booktitle = {English for Interacting on Campus},
  doi       = {10.1007/978-3-319-28734-8},
  url       = {http://link.springer.com/10.1007/978-3-319-28734-8},
}

@Book{Wallwork2013b,
  author    = {Wallwork, Adrian},
  publisher = {Springer US},
  title     = {{English for Academic Research: Vocabulary Exercises}},
  year      = {2013},
  address   = {Boston, MA},
  isbn      = {978-1-4614-4267-7},
  abstract  = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  booktitle = {English for Academic Research: Vocabulary Exercises},
  doi       = {10.1007/978-1-4614-4268-4},
  url       = {http://link.springer.com/10.1007/978-1-4614-4268-4},
}

@Misc{LinuxFoundation2020,
  author    = {{The Linux Foundation}},
  title     = {{Production-Grade Container Orchestration - Kubernetes}},
  year      = {2020},
  booktitle = {Kubernetes.Io},
  url       = {https://kubernetes.io/},
}

@Misc{RyanOlsonJonathanCalmels2016,
  author   = {{Ryan Olson, Jonathan Calmels}, Felix Abecassis and Phil Rogers |},
  title    = {{NVIDIA Docker: GPU Server Application Deployment Made Easy}},
  year     = {2016},
  abstract = {Over the last few years there has been a dramatic rise in the use of containers for deploying data center applications at scale. The reason for this is simple: containers encapsulate an application's dependencies to provide reproducible and reliable execution of applications and services without the overhead of a full virtual machine. If you have ever spent a day provisioning a server with a multitude of packages for a scientific or deep learning application, or have put in weeks of effort to ensure your application can be built and deployed in multiple linux environments.},
  url      = {https://devblogs.nvidia.com/nvidia-docker-gpu-server-application-deployment-made-easy/},
}

@Misc{Hykes2013,
  author    = {Hykes, Solomon},
  title     = {{Empowering App Development for Developers | Docker}},
  year      = {2013},
  abstract  = {Learn how Docker helps developers bring their ideas to life by conquering the complexity of app development.},
  booktitle = {Docker, Inc},
  keywords  = {docker},
  url       = {https://www.docker.com/},
}

@Book{Turner2020,
  author    = {Turner, Phil},
  publisher = {Springer International Publishing},
  title     = {{Imagination + Technology}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-37347-4},
  series    = {Human–Computer Interaction Series},
  doi       = {10.1007/978-3-030-37348-1},
  url       = {http://link.springer.com/10.1007/978-3-030-37348-1},
}

@Book{Pan2020,
  editor    = {Pan, Linqiang and Liang, Jing and Qu, Boyang},
  publisher = {Springer Singapore},
  title     = {{Bio-inspired Computing: Theories and Applications}},
  year      = {2020},
  address   = {Singapore},
  isbn      = {978-981-15-3424-9},
  series    = {Communications in Computer and Information Science},
  volume    = {1159},
  doi       = {10.1007/978-981-15-3425-6},
  url       = {http://link.springer.com/10.1007/978-981-15-3425-6},
}

@Book{Dorronsoro2020,
  author    = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and Carlos, Juan and Torre, De and Urda, Daniel and Eds, El-ghazali Talbi},
  editor    = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and de la Torre, Juan Carlos and Urda, Daniel and Talbi, El-Ghazali},
  publisher = {Springer International Publishing},
  title     = {{Optimization and Learning}},
  year      = {2020},
  address   = {Cham},
  isbn      = {9783030419127},
  number    = {March},
  series    = {Communications in Computer and Information Science},
  volume    = {1173},
  doi       = {10.1007/978-3-030-41913-4},
  pages     = {2020},
  url       = {http://link.springer.com/10.1007/978-3-030-41913-4},
}

@Article{Miyawaki2008,
  author   = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and aki Sato, Masa and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
  journal  = {Neuron},
  title    = {{Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders}},
  year     = {2008},
  issn     = {08966273},
  month    = {dec},
  number   = {5},
  pages    = {915--929},
  volume   = {60},
  abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binary-contrast, 10 × 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
  doi      = {10.1016/j.neuron.2008.11.004},
  keywords = {SYSNEURO},
  pmid     = {19081384},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S0896627308009586},
}

@Book{Performance2013,
  author    = {Performance, High},
  editor    = {Bianchini, Calebe and Osthoff, Carla and Souza, Paulo and Ferreira, Renato},
  publisher = {Springer International Publishing},
  title     = {{High Performance Computing Systems}},
  year      = {2013},
  address   = {Cham},
  isbn      = {9783319102139},
  series    = {Communications in Computer and Information Science},
  volume    = {1171},
  doi       = {10.1007/978-3-030-41050-6},
  url       = {http://link.springer.com/10.1007/978-3-030-41050-6},
}

@Book{Libby2020,
  author    = {Libby, Alex},
  publisher = {Apress},
  title     = {{Introducing the HTML5 Web Speech API}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5734-0},
  booktitle = {Introducing the HTML5 Web Speech API},
  doi       = {10.1007/978-1-4842-5735-7},
  keywords  = {Speech Recognition,Web Speech API},
  url       = {http://www.sitepoint.com/introducing-web-speech-api/},
}

@Book{Srikant2020,
  author    = {Srikant, Y. N.},
  publisher = {Springer International Publishing},
  title     = {{Distributed Graph Analytics}},
  year      = {2020},
  address   = {Cham},
  isbn      = {9783030369866},
  volume    = {11969 LNCS},
  abstract  = {Graph Analytics is important in different domains: social networks, computer networks, and computational biology to name a few. This paper describes the challenges involved in programming the underlying graph algorithms for graph analytics for distributed systems with CPU, GPU, and multi-GPU machines and how to deal with them. It emphasizes how language abstractions and good compilation can ease programming graph analytics on such platforms without sacrificing implementation efficiency.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  doi       = {10.1007/978-3-030-36987-3_1},
  issn      = {16113349},
  keywords  = {GPU computation,Graph analytics,Graph frameworks,Graph processing languages,Multi-core processors,Parallel algorithms,Social networks},
  pages     = {3--20},
  url       = {http://link.springer.com/10.1007/978-3-030-41886-1},
}

@Book{BergHansen2020,
  author    = {{Berg Hansen}, Kim},
  publisher = {Apress},
  title     = {{Practical Oracle SQL}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5616-9},
  booktitle = {Practical Oracle SQL},
  doi       = {10.1007/978-1-4842-5617-6},
  url       = {http://link.springer.com/10.1007/978-1-4842-5617-6},
}

@Article{Veta2019,
  author        = {Veta, Mitko and Heng, Yujing J. and Stathonikos, Nikolas and Bejnordi, Babak Ehteshami and Beca, Francisco and Wollmann, Thomas and Rohr, Karl and Shah, Manan A. and Wang, Dayong and Rousson, Mikael and Hedlund, Martin and Tellez, David and Ciompi, Francesco and Zerhouni, Erwan and Lanyi, David and Viana, Matheus and Kovalev, Vassili and Liauchuk, Vitali and Phoulady, Hady Ahmady and Qaiser, Talha and Graham, Simon and Rajpoot, Nasir and Sj{\"{o}}blom, Erik and Molin, Jesper and Paeng, Kyunghyun and Hwang, Sangheum and Park, Sunggyun and Jia, Zhipeng and Chang, Eric I.Chao and Xu, Yan and Beck, Andrew H. and van Diest, Paul J. and Pluim, Josien P.W.},
  journal       = {Medical Image Analysis},
  title         = {{Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge}},
  year          = {2019},
  issn          = {13618423},
  month         = {jul},
  pages         = {111--121},
  volume        = {54},
  abstract      = {Tumor proliferation is an important biomarker indicative of the prognosis of breast cancer patients. Assessment of tumor proliferation in a clinical setting is a highly subjective and labor-intensive task. Previous efforts to automate tumor proliferation assessment by image analysis only focused on mitosis detection in predefined tumor regions. However, in a real-world scenario, automatic mitosis detection should be performed in whole-slide images (WSIs) and an automatic method should be able to produce a tumor proliferation score given a WSI as input. To address this, we organized the TUmor Proliferation Assessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores from WSIs. The challenge dataset consisted of 500 training and 321 testing breast cancer histopathology WSIs. In order to ensure fair and independent evaluation, only the ground truth for the training dataset was provided to the challenge participants. The first task of the challenge was to predict mitotic scores, i.e., to reproduce the manual method of assessing tumor proliferation by a pathologist. The second task was to predict the gene expression based PAM50 proliferation scores from the WSI. The best performing automatic method for the first task achieved a quadratic-weighted Cohen's kappa score of $\kappa$ = 0.567, 95% CI [0.464, 0.671] between the predicted scores and the ground truth. For the second task, the predictions of the top method had a Spearman's correlation coefficient of r = 0.617, 95% CI [0.581 0.651] with the ground truth. This was the first comparison study that investigated tumor proliferation assessment from WSIs. The achieved results are promising given the difficulty of the tasks and weakly-labeled nature of the ground truth. However, further research is needed to improve the practical utility of image analysis methods for this task.},
  archiveprefix = {arXiv},
  arxivid       = {1807.08284},
  doi           = {10.1016/j.media.2019.02.012},
  eprint        = {1807.08284},
  keywords      = {Breast cancer,Cancer prognostication,Deep learning,Tumor proliferation},
  pmid          = {30861443},
  url           = {http://arxiv.org/abs/1807.08284 http://dx.doi.org/10.1016/j.media.2019.02.012},
}

@Book{Norris2020,
  author    = {Norris, Donald J.},
  publisher = {Apress},
  title     = {{Machine Learning with the Raspberry Pi}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5173-7},
  booktitle = {Machine Learning with the Raspberry Pi},
  doi       = {10.1007/978-1-4842-5174-4},
  url       = {http://link.springer.com/10.1007/978-1-4842-5174-4},
}

@Book{Alloghani2020,
  author    = {Alloghani, Mohamed and Al-Jumeily, Dhiya and Aljaaf, Ahmed J and Khalaf, Mohammed and Mustafina, Jamila and Tan, Sin Y},
  editor    = {Khalaf, Mohammed I. and Al-Jumeily, Dhiya and Lisitsa, Alexei},
  publisher = {Springer International Publishing},
  title     = {{Applied Computing to Support Industry}},
  year      = {2020},
  address   = {Cham},
  isbn      = {9783030387518},
  series    = {Communications in Computer and Information Science},
  volume    = {1},
  abstract  = {In the recent years, the number of web logs, and the amount of opinionated data on the World Wide Web, have been grown substantially. The ability to determine the political orientation of an article automatically can be beneficial in many areas from academia to security. However, the sentiment classification of web log posts (political web log posts in particular), is apparently more complex than the sentiment classification of conventional text. In this paper, a supervised machine learning with two feature extraction techniques Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) are used for the classification process. For investigation, SVM with four kernels for supervised machine learning have been employed. Subsequent to testing, the results reveal that the linear with TF achieved the results in accuracy of 91.935% also with TF-IDF achieved the 95.161%. The linear kernel was deemed the most suitable for our model.},
  booktitle = {Applied Computing to Support Industry: Innovation and Technology},
  doi       = {10.1007/978-3-030-38752-5},
  keywords  = {AI medical research,Deep learning,artificial intelligence,machine learning},
  pages     = {248--261},
  url       = {http://dx.doi.org/10.1007/978-3-030-38752-5_7},
}

@Book{Magnor2020,
  author    = {Magnor, Marcus and Sorkine-Hornung, Alexander},
  editor    = {Magnor, Marcus and Sorkine-Hornung, Alexander},
  publisher = {Springer International Publishing},
  title     = {{Real VR – Immersive Digital Reality}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-41815-1},
  series    = {Lecture Notes in Computer Science},
  volume    = {11900},
  doi       = {10.1007/978-3-030-41816-8},
  url       = {http://link.springer.com/10.1007/978-3-030-41816-8},
}

@Book{Strauss2020,
  author    = {Strauss, Dirk},
  publisher = {Apress},
  title     = {{Getting Started with Visual Studio 2019}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5448-6},
  booktitle = {Getting Started with Visual Studio 2019},
  doi       = {10.1007/978-1-4842-5449-3},
  url       = {http://link.springer.com/10.1007/978-1-4842-5449-3},
}

@Book{Florentina2019,
  author    = {Florentina, Laura and Eds, Stoica},
  editor    = {Simian, Dana and Stoica, Laura Florentina},
  publisher = {Springer International Publishing},
  title     = {{Dana Simian Modelling and Development of Intelligent Systems}},
  year      = {2019},
  address   = {Cham},
  isbn      = {9783030392369},
  series    = {Communications in Computer and Information Science},
  volume    = {1126},
  doi       = {10.1007/978-3-030-39237-6},
  keywords  = {Adaptive le,Learning objects,Personalised learning,adaptive,digital learning resources,e-learning,learning objects,learning systems,personalised learning},
  pages     = {18--32},
  url       = {http://link.springer.com/10.1007/978-3-030-39237-6},
}

@Book{Clark2020,
  author    = {Clark, Dan},
  publisher = {Apress},
  title     = {{Beginning Microsoft Power BI}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5619-0},
  abstract  = {Analysieren Sie Unternehmensdaten schnell und einfach mit den leistungsstarken Datentools von Microsoft. Erfahren Sie, wie Sie skalierbare und robuste Datenmodelle erstellen, verschiedene Datenquellen effektiv bereinigen und kombinieren sowie {\"{u}}berzeugende und professionelle Grafiken erstellen. Beginning Power BI ist eine praktische, aktivit{\"{a}}tsbasierte Anleitung, die Sie durch den Prozess der Analyse Ihrer Daten mit den Tools f{\"{u}}hrt, die den Kern des Self-Service-BI-Angebots von Microsoft bilden. Beginnend mit Power Query erfahren Sie, wie Sie Daten aus verschiedenen Quellen abrufen und wie einfach es ist, die Daten vor dem Import in ein Datenmodell zu bereinigen und zu formen. Mithilfe der Tabelle Power BI und der Datenanalyse-Ausdr{\"{u}}cke (DAX) lernen Sie, robuste skalierbare Datenmodelle zu erstellen, die als Grundlage f{\"{u}}r Ihre Datenanalyse dienen. Von dort aus betreten Sie die Welt {\"{u}}berzeugender interaktiver Visualisierungen, um Ihre Daten zu analysieren und Einblicke in sie zu gewinnen. Sie schlie{\ss}en Ihre Power BI-Reise ab, indem Sie lernen, wie Sie Ihre Berichte und Dashboards verpacken und mit Ihren Kollegen teilen. Der Autor Dan Clark f{\"{u}}hrt Sie anhand von Schritt-f{\"{u}}r-Schritt-Aktivit{\"{a}}ten und zahlreichen Screenshots durch jedes Thema, um Sie mit den Tools vertraut zu machen. Diese dritte Ausgabe behandelt die neuen und sich weiterentwickelnden Funktionen der Power BI-Plattform sowie neue Kapitel zu Datenfl{\"{u}}ssen und zusammengesetzten Modellen. Dieses Buch ist Ihre praktische Anleitung f{\"{u}}r schnelle, zuverl{\"{a}}ssige und wertvolle Dateninformationen. Was du lernen wirst: Vereinfachen Sie das Erkennen, Zuordnen und Bereinigen von Daten Erstellen Sie solide analytische Datenmodelle Erstellen Sie robuste interaktive Datenpr{\"{a}}sentationen Kombinieren Sie analytische und geografische Daten in kartenbasierten Visualisierungen Ver{\"{o}}ffentlichen und teilen Sie Dashboards und Berichte Dan Clark ist ein Senior Business Intelligence (BI) und Programmierberater, der sich auf Microsoft-Technologien spezialisiert hat. Er konzentriert sich darauf, neue BI- und Datentechnologien zu erlernen und andere darin zu schulen, wie die Technologie am besten implementiert werden kann. Dan hat mehrere B{\"{u}}cher und zahlreiche Artikel {\"{u}}ber .NET-Programmierung und BI-Entwicklung ver{\"{o}}ffentlicht. Er h{\"{a}}lt regelm{\"{a}}{\ss}ig Vortr{\"{a}}ge bei verschiedenen Entwickler- und Datenbankkonferenzen sowie Benutzergruppentreffen und interagiert gerne mit den Microsoft-Communities. In einem fr{\"{u}}heren Leben war Dan Physiklehrer. Er ist immer noch inspiriert von dem Wunder und der Ehrfurcht, das Universum zu studieren und herauszufinden, warum sich die Dinge so verhalten, wie sie es tun.},
  booktitle = {Beginning Microsoft Power BI},
  doi       = {10.1007/978-1-4842-5620-6},
  url       = {http://link.springer.com/10.1007/978-1-4842-5620-6},
}

@Book{Langer2020,
  author    = {Langer, Arthur M.},
  publisher = {Springer International Publishing},
  title     = {{Analysis and Design of Next-Generation Software Architectures}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-36898-2},
  booktitle = {Analysis and Design of Next-Generation Software Architectures},
  doi       = {10.1007/978-3-030-36899-9},
  url       = {http://link.springer.com/10.1007/978-3-030-36899-9},
}

@Book{Taulli2020,
  author    = {Taulli, Tom},
  publisher = {Apress},
  title     = {{The Robotic Process Automation Handbook}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {9781484257289},
  booktitle = {The Robotic Process Automation Handbook},
  doi       = {10.1007/978-1-4842-5729-6},
  pages     = {2--3},
  url       = {https://doi.org/10.1007/978-1-4842-5729-6},
}

@Book{Staron2020,
  author    = {Staron, Miroslaw},
  publisher = {Springer International Publishing},
  title     = {{Action Research in Software Engineering}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-32609-8},
  abstract  = {This book addresses action research (AR), one of the main research methodologies used for academia-industry research collaborations. It elaborates on how to find the right research activities and how to distinguish them from non-significant ones. Further, it details how to glean lessons from the research results, no matter whether they are positive or negative. Lastly, it shows how companies can evolve and build talents while expanding their product portfolio.The book's structure is based on that of AR projects; it sequentially covers and discusses each phase of the project. Each chapter shares new insights into AR and provides the reader with a better understanding of how to apply it. In addition, each chapter includes a number of practical use cases or examples. Taken together, the chapters cover the entire software lifecycle: from problem diagnosis to project (or action) planning and execution, to documenting and disseminating results, including validity assessments for AR studies.The goal of this book is to help everyone interested in industry-academia collaborations to conduct joint research. It is for students of software engineering who need to learn about how to set up an evaluation, how to run a project, and how to document the results. It is for all academics who aren't afraid to step out of their comfort zone and enter industry. It is for industrial researchers who know that they want to do more than just develop software blindly. And finally, it is for stakeholders who want to learn how to manage industrial research projects and how to set up guidelines for their own role and expectations.},
  booktitle = {Action Research in Software Engineering},
  doi       = {10.1007/978-3-030-32610-4},
  url       = {http://link.springer.com/10.1007/978-3-030-32610-4},
}

@Book{Leordeanu2020,
  author    = {Leordeanu, Marius},
  publisher = {Springer International Publishing},
  title     = {{Unsupervised Learning in Space and Time}},
  year      = {2020},
  address   = {Cham},
  isbn      = {9783030421274},
  series    = {Advances in Computer Vision and Pattern Recognition},
  doi       = {10.1007/978-3-030-42128-1},
  url       = {http://www.springer.com/series/4205},
}

@Book{Singh2020,
  author    = {Singh, Himanshu and Lone, Yunis Ahmad},
  publisher = {Apress},
  title     = {{Deep Neuro-Fuzzy Systems with Python}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5360-1},
  booktitle = {Deep Neuro-Fuzzy Systems with Python},
  doi       = {10.1007/978-1-4842-5361-8},
  url       = {http://link.springer.com/10.1007/978-1-4842-5361-8},
}

@Book{Shaik2020,
  author    = {Shaik, Baji},
  publisher = {Apress},
  title     = {{PostgreSQL Configuration}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5662-6},
  booktitle = {PostgreSQL Configuration},
  doi       = {10.1007/978-1-4842-5663-3},
  url       = {http://link.springer.com/10.1007/978-1-4842-5663-3},
}

@Book{Sabharwal2020,
  author    = {Sabharwal, Navin and Edward, Shakuntala Gupta},
  publisher = {Apress},
  title     = {{Hands On Google Cloud SQL and Cloud Spanner}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5536-0},
  booktitle = {Hands On Google Cloud SQL and Cloud Spanner},
  doi       = {10.1007/978-1-4842-5537-7},
  url       = {http://link.springer.com/10.1007/978-1-4842-5537-7},
}

@Book{Guevarra2020,
  author    = {Guevarra, Ezra Thess Mendoza},
  publisher = {Apress},
  title     = {{Modeling and Animation Using Blender}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5339-7},
  abstract  = {Chapter 1: The Tour! -- Chapter 2: Blending with Blender: Getting Started -- Chapter 3: Blending with Blender: Modeling Workspace -- Chapter 4: Blending with Blender: Shading Workspace -- Chapter 5: Let's Animate -- Chapter 6: The Future of Game Engine. Discover the 3D-modeling and animation power of Blender 3D. This book starts with a brief introduction to Blender 3D including installation and the user interface. The following two chapters then introduce you to the upgraded tools in Blender 2.80 for 3D modeling, texturing, shading, and animation. The last chapter discusses the Blender game engine and all its core features. Along the way you'll see why Blender 3D has proved its competency in UV unwrapping, texturing, raster graphic editing, rigging, sculpting, animating, motion graphics, and video editing through the years. Modeling and Animation Using Blender gives a thorough tour of Blender Eevee, covering its new features and how to make best use of them. After reading this book you will have the confidence to choose Blender for your next project. You will: Master the features of Blender Eevee Work with modeling, animation, and much more using the updated software Understand important concepts such as physics and particles.},
  booktitle = {Modeling and Animation Using Blender},
  doi       = {10.1007/978-1-4842-5340-3},
  url       = {http://link.springer.com/10.1007/978-1-4842-5340-3},
}

@Book{Davis2020,
  author    = {Davis, Adam L.},
  publisher = {Apress},
  title     = {{Modern programming made easy: Using java, scala, groovy, and javascript, second edition}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {9781484255698},
  abstract  = {Introduction Get up and running fast with the basics of programming using Java as an example language. This short book gets you thinking like a programmer in an easy and entertaining way. Modern Programming Made Easy teaches you basic coding principles, including working with lists, sets, arrays, and maps; coding in the object-oriented style; and writing a web application. This book is largely language agnostic, but mainly covers the latest appropriate and relevant release of Java, with some updated references to Groovy, Scala, and JavaScript to give you a broad range of examples to consider. You will get a taste of what modern programming has to offer and set yourself up for further study and growth in your chosen language. You will: Write code using the functional programming style Build your code using the latest releases of Java, Groovy, and more Test your code Read and write from files Design user interfaces Deploy your app in the cloud.},
  booktitle = {Modern Programming made Easy: Using Java, Scala, Groovy, and JavaScript},
  doi       = {10.1007/978-1-4842-5569-8},
  keywords  = {Coding,Development,Groovy,Java,Javascript,Made simple,Modern,Programming,Scala,Software},
  pages     = {1--193},
  url       = {http://link.springer.com/10.1007/978-1-4842-5569-8},
}

@Book{Johannsen2011,
  author    = {Johannsen, Daniel},
  editor    = {Paquete, Lu{\'{i}}s and Zarges, Christine},
  publisher = {Springer International Publishing},
  title     = {{Evolutionary Computation in Combinatorial Optimization}},
  year      = {2011},
  address   = {Cham},
  isbn      = {978-3-030-43679-7},
  series    = {Lecture Notes in Computer Science},
  volume    = {12102},
  abstract  = {This paper presents a novel hybrid approach for solving the Container Loading (CL) problem based on the combination of Integer\n  Linear Programming (ILP) and Genetic Algorithms (GAs). More precisely, a GA engine works as a generator of reduced instances\n  for the original CL problem, which are formulated as ILP models. These instances, in turn, are solved by an exact optimization\n  technique (solver), and the performance measures accomplished by the respective models are interpreted as fitness values by\n  the genetic algorithm, thus guiding its evolutionary process. Computational experiments performed on standard benchmark problems,\n  as well as a practical case study developed in a metallurgic factory, are also reported and discussed here in a manner as\n  to testify the potentialities behind the novel approach.},
  doi       = {10.1142/9789814282673_0003},
  pages     = {53--99},
  url       = {http://link.springer.com/10.1007/978-3-030-43680-3},
}

@Book{Chowdhary2020,
  author    = {Chowdhary, K. R.},
  publisher = {Springer India},
  title     = {{Fundamentals of artificial intelligence}},
  year      = {2020},
  address   = {New Delhi},
  isbn      = {9788132239727},
  abstract  = {Fundamentals of Artificial Intelligence introduces the foundations of present day AI and provides coverage to recent developments in AI such as Constraint Satisfaction Problems, Adversarial Search and Game Theory, Statistical Learning Theory, Automated Planning, Intelligent Agents, Information Retrieval, Natural Language & Speech Processing, and Machine Vision. The book features a wealth of examples and illustrations, and practical approaches along with the theoretical concepts. It covers all major areas of AI in the domain of recent developments. The book is intended primarily for students who major in computer science at undergraduate and graduate level but will also be of interest as a foundation to researchers in the area of AI.},
  booktitle = {Fundamentals of Artificial Intelligence},
  doi       = {10.1007/978-81-322-3972-7},
  keywords  = {First-order Predicate Logic,Knowledge Representation,Non-monotonic Reasoning,Prolog,State-space Search},
  pages     = {1--716},
  url       = {http://link.springer.com/10.1007/978-81-322-3972-7},
}

@Book{Reinkemeyer2020,
  editor    = {Reinkemeyer, Lars},
  publisher = {Springer International Publishing},
  title     = {{Process Mining in Action}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-40171-9},
  abstract  = {This book describes process mining use cases and business impact along the value chain, from corporate to local applications, representing the state of the art in domain know-how. Providing a set of industrial case studies and best practices, it complements academic publications on the topic. Further the book reveals the challenges and failures in order to offer readers practical insights and guidance on how to avoid the pitfalls and ensure successful operational deployment. The book is divided into three parts: Part I provides an introduction to the topic from fundamental principles to key success factors, and an overview of operational use cases. As a holistic description of process mining in a business environment, this part is particularly useful for readers not yet familiar with the topic. Part II presents detailed use cases written by contributors from a variety of functions and industries. Lastly, Part III provides a brief overview of the future of process mining, both from academic and operational perspectives. Based on a solid academic foundation, process mining has received increasing interest from operational businesses, with many companies already reaping the benefits. As the first book to present an overview of successful industrial applications, it is of particular interest to professionals who want to learn more about the possibilities and opportunities this new technology offers. It is also a valuable resource for researchers looking for empirical results when considering requirements for enhancements and further developments.},
  booktitle = {Process Mining in Action},
  doi       = {10.1007/978-3-030-40172-6},
  url       = {http://link.springer.com/10.1007/978-3-030-40172-6},
}

@Book{Tsitoara2020,
  author    = {Tsitoara, Mariot},
  publisher = {Apress},
  title     = {{Beginning Git and GitHub}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {9781484253120},
  booktitle = {Beginning Git and GitHub},
  doi       = {10.1007/978-1-4842-5313-7},
  url       = {http://link.springer.com/10.1007/978-1-4842-5313-7},
}

@Book{Manelli2020,
  author    = {Manelli, Luciano},
  publisher = {Apress},
  title     = {{Introducing Algorithms in C}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5622-0},
  abstract  = {This book serves as a starting point for anyone who is beginning their study of computer science and information systems? Algorithms play an important role in programming, and they can be considered the cornerstone of computer science, because computer programs would not exist without algorithms? In fact, understanding a problem and getting a solution is a fundamental condition for software development and problem-solving strategies? Therefore, the aim of this book is to explain algorithms in different ways and then teach you how to analy?e new algorithms? In this book, we will use the C language to verify the correctness of the algorithms?},
  booktitle = {Introducing Algorithms in C},
  doi       = {10.1007/978-1-4842-5623-7},
  url       = {http://link.springer.com/10.1007/978-1-4842-5623-7},
}

@Book{Shargel1976,
  author    = {Shargel, L. and Dorrbecker, S. A. and Levitt, M.},
  editor    = {{De Marsico}, Maria and {Sanniti di Baja}, Gabriella and Fred, Ana},
  publisher = {Springer International Publishing},
  title     = {{Physiological disposition and metabolism of N t butylarterenol and its di p toluate ester (bitolterol) in the rat}},
  year      = {1976},
  address   = {Cham},
  isbn      = {978-3-642-36529-4},
  number    = {1},
  series    = {Lecture Notes in Computer Science},
  volume    = {4},
  abstract  = {The metabolism and disposition of the bronchodilator, N t butylarterenol (tBA) and its di p toluate ester (bitolterol) were compared in the rat. Radioactivity was preferentially retained in lungs of rats compared with heart and blood after iv medication with tritium labeled bitolterol, but was not retained in tissues after iv medication with [3H]tBA. After oral and iv medication with [3H]bitolterol, fecal radioactivity accounted for 24% of the dose and 65 and 79% of the radioactivity, respectively, was excreted in urine (0-72 hr). In comparison, urine radioactivity after oral and iv medication with [3H]tBA was 43 and 83% of the dose, respectively, and fecal radioactivity accounted for 43 and 23% of the dose, respectively (0-72 hr). Bitolterol was hydrolyzed in vitro to tBA esterases found in various tissues including small intestine, liver, and plasma. Moreover, tBA was a substrate for catecholamine O methyltransferase but not for monoamine oxidase. Similar metabolites were observed in urine samples of rats given either [3H]tBA or [3H]bitolterol. Urine metabolites were identified as free and conjugated forms of both tBA and 3 O methyl tBA.},
  booktitle = {Drug Metabolism and Disposition},
  doi       = {10.1007/978-3-642-36530-0},
  issn      = {00909556},
  keywords  = {combining classifier,electricity theft,optimum path forest,support vector machine,unbalance class problem,ute},
  pages     = {65--71},
  pmid      = {3403},
  url       = {http://link.springer.com/10.1007/978-3-642-36530-0},
}

@Book{Osterhage2020,
  author    = {Osterhage, Wolfgang W.},
  publisher = {Springer Berlin Heidelberg},
  title     = {{Mathematical Theory of Advanced Computing}},
  year      = {2020},
  address   = {Berlin, Heidelberg},
  isbn      = {978-3-662-60358-1},
  booktitle = {Mathematical Theory of Advanced Computing},
  doi       = {10.1007/978-3-662-60359-8},
  url       = {http://link.springer.com/10.1007/978-3-662-60359-8},
}

@InCollection{Kelly2014,
  author    = {Kelly, Alonzo and Kelly, Alonzo},
  booktitle = {Mobile Robotics},
  publisher = {Springer International Publishing},
  title     = {{Optimal Estimation}},
  year      = {2014},
  address   = {Cham},
  pages     = {270--369},
  doi       = {10.1017/cbo9781139381284.006},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_714-1},
}

@InCollection{Farid2020,
  author    = {Farid, Hany},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Image Forensics}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--10},
  abstract  = {From mainstream media outlets to social media and everything in between, doctored photographs are appearing with growing frequency and sophistication. The resulting lack of trust is impacting law enforcement, national security, the media, e-commerce, and more. While some types of manipulations can be detected with a careful visual examination, our visual system seems unable to reliably detect other types of manipulations. The field of image forensics has emerged to help return some trust in photography. I describe the perceptual limits of detecting manipulated images, as well as representative examples of computational techniques for authenticating images.},
  doi       = {10.1007/978-3-030-03243-2_877-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_877-1},
}

@InCollection{Richardt2020,
  author    = {Richardt, Christian},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Omnidirectional Stereo}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_808-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_808-1},
}

@InCollection{Wu2012,
  author    = {Wu, Jianxin and Rehg, James M.},
  booktitle = {Ensemble Machine Learning: Methods and Applications},
  publisher = {Springer International Publishing},
  title     = {{Object detection}},
  year      = {2012},
  address   = {Cham},
  isbn      = {9781441993267},
  pages     = {225--250},
  abstract  = {Over the past twenty years, data-driven methods have become a dominant paradigm for computer vision, with numerous practical successes. In difficult computer vision tasks, such as the detection of object categories (for example, the detection of faces of various gender, age, race, and pose, under various illumination and background conditions), researchers generally learn a classifier that can distinguish an image patch that contains the object of interest from all other image patches. Ensemble learning methods have been very successful in learning classifiers for object detection.},
  doi       = {10.1007/9781441993267_8},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_660-1},
}

@InCollection{Babadi2020,
  author    = {Babadi, Behtash},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Learning from a Neuroscience Perspective}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  abstract  = {{\ldots} is often summarized in the form of the following adage: The neurons that fire together, wire {\ldots} firing after the postsynaptic neuron) and promotes negative delays (ie, postsynaptic neuron firing after the {\ldots} TK, Lu E, Wen X, Poirazi P, Tracht- enberg JT et al (2018) Hotspots of dendritic {\ldots}},
  doi       = {10.1007/978-3-030-03243-2_823-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_823-1},
}

@InCollection{Govindu2020,
  author    = {Govindu, Venu Madhav},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Lie Algebra}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_871-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_871-1},
}

@InCollection{Brown2020,
  author    = {Brown, Matthew},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Image Stitching}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--3},
  abstract  = {Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Commonly performed through the use of computer software, most approaches to image stitching require nearly exact overlaps between images and identical exposures to produce seamless results, although some stitching algorithms actually benefit from differently exposed images by doing HDR (High Dynamic Range) imaging in regions of overlap. Some digital cameras can stitch their photos internally. Image stitching is widely used in today's world in applications such as\n“Image Stabilization” feature in camcorders which use frame-rate image alignment.\nHigh resolution photo mosaics in digital maps and satellite photos.\nMedical Imaging.\nMultiple image super-resolution.\nVideo Stitching.\nObject Insertion.},
  doi       = {10.1007/978-3-030-03243-2_13-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_13-1},
}

@InCollection{Gortler1996,
  author    = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
  booktitle = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
  publisher = {Springer International Publishing},
  title     = {{Lumigraph}},
  year      = {1996},
  address   = {Cham},
  pages     = {43--54},
  abstract  = {This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.},
  doi       = {10.1007/978-3-030-03243-2_8-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_8-1},
}

@InCollection{Xu2020,
  author    = {Xu, Chenyang and Prince, Jerry L.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Gradient Vector Flow}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--8},
  doi       = {10.1007/978-3-030-03243-2_712-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_712-1},
}

@InCollection{Sugihara2020,
  author    = {Sugihara, Kokichi},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Line Drawing Labeling}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--3},
  doi       = {10.1007/978-3-030-03243-2_390-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_390-1},
}

@InCollection{Zheng2009,
  author    = {Zheng, Nanning and Xue, Jianru},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Manifold Learning}},
  year      = {2009},
  address   = {Cham},
  pages     = {87--119},
  doi       = {10.1007/978-1-84882-312-9_4},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_824-1},
}

@InCollection{Murota1955,
  author    = {Murota, Kazuo},
  booktitle = {The Economic Studies Quarterly (Tokyo. 1950)},
  publisher = {Springer International Publishing},
  title     = {{Linear Programmingモデルの簡単な応用について}},
  year      = {1955},
  address   = {Cham},
  number    = {3-4},
  pages     = {156--163},
  volume    = {5},
  doi       = {10.11398/economics1950.5.3-4_156},
  issn      = {2185-4408},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_648-1},
}

@InCollection{Maybank2020,
  author    = {Maybank, Stephen J.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Fisher-Rao Metric}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--3},
  doi       = {10.1007/978-3-030-03243-2_657-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_657-1},
}

@InCollection{BolonCanedo2018,
  author    = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and Alonso-Betanzos, Amparo},
  booktitle = {Intelligent Systems Reference Library},
  publisher = {Springer International Publishing},
  title     = {{Feature selection}},
  year      = {2018},
  address   = {Cham},
  pages     = {13--37},
  volume    = {147},
  abstract  = {The advent of Big Data, and specially the advent of datasets with high dimensionality, has brought an important necessity to identify the relevant features of the data. In this scenario, the importance of feature selection is beyond doubt and different methods have been developed, although researchers do not agree on which one is the best method for any given setting. This chapter provides the reader with the foundations about feature selection (see Sect. 2.1) as well as a description of the state-of-the-art feature selection methods (Sect. 2.2). Then, these methods will be analyzed on several synthetic datasets (Sect. 2.3) trying to draw conclusions about their performance when dealing with a crescent number of irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Finally, in Sect. 2.4, some state-of-the-art methods will be analyzed to study their scalability, i.e. the impact of an increase in the training set on the computational performance of an algorithm in terms of accuracy, training time and stability.},
  doi       = {10.1007/978-3-319-90080-3_2},
  issn      = {18684408},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_299-1},
}

@InCollection{Ge2020,
  author    = {Ge, Liuhao and Yuan, Junsong},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Hand Pose Estimation}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  abstract  = {Real-time and accurate hand pose estimation can open new doors for making the entire world more interactive. The existing systems for hand pose estimation fail to produce an accurate and a physically valid pose in real-time. The approach in this project aims to tackle these problems by applying a discriminative model for real-time prediction of joint locations and, incorporates kinematic constraints for producing a geometrically valid pose, thus leading to accurate pose estimation. The discriminative model is a deep network consisting of convolutional, fully connected and dropout layers. The kinematic constraints are incorporated as a kinematic layer towards the end of the network, which acts as a prior for the hand pose. The results are evaluated on the NYU hand pose data-set and compared with state-of-the-art methods.},
  doi       = {10.1007/978-3-030-03243-2_875-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_875-1},
}

@InCollection{Larochelle2020,
  author    = {Larochelle, Hugo},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Few-Shot Learning}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_861-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_861-1},
}

@InCollection{Yu2020,
  author    = {Yu, Guoshen and Sapiro, Guillermo},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Image Enhancement and Restoration: Traditional Approaches}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_233-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_233-1},
}

@InCollection{Lepetit2020,
  author    = {Lepetit, Vincent},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Image Descriptors and Similarity Measures}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--8},
  doi       = {10.1007/978-3-030-03243-2_797-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_797-1},
}

@InCollection{Karaman2020,
  author    = {Karaman, Svebor and Chang, Shih-Fu},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Hashing for Face Search}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_817-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_817-1},
}

@InCollection{Yamamoto2020,
  author    = {Yamamoto, Masanobu},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Ego-Motion and EPI Analysis}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--7},
  doi       = {10.1007/978-3-030-03243-2_873-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_873-1},
}

@InCollection{Patel2020,
  author    = {Patel, Vishal M. and Nguyen, Hien Van},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Domain Adaptation Using Dictionaries}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_819-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_819-1},
}

@InCollection{Takahashi2020,
  author    = {Takahashi, Tomokazu and Murase, Hiroshi},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Eigenspace Methods}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_711-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_711-1},
}

@InCollection{Loy2020,
  author    = {Loy, Chen Change},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Face Detection}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  abstract  = {In recent years, face recognition has attracted much attention and its research has rapidly expanded by not only engineers but also neuroscientists, since it has many potential applications in computer vision},
  doi       = {10.1007/978-3-030-03243-2_798-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_798-1},
}

@InCollection{Blanchini2015,
  author    = {Blanchini, Franco and Miani, Stefano},
  booktitle = {Systems and Control: Foundations and Applications},
  publisher = {Springer International Publishing},
  title     = {{Dynamic programming}},
  year      = {2015},
  address   = {Cham},
  number    = {9783319179322},
  pages     = {193--234},
  abstract  = {In this section a jump back in the history of control is made and we consider problems which had been theoretically faced in the early 70s, and thereafter almost abandoned. The main reason is that the computational effort necessary to practically implement these techniques was not suitable for the computer technology of the time. Today, the situation is different, and many authors are reconsidering the approach. In this section, the main focus will be put on discrete-time systems, although it will also be shown, given the existing relation between continuous- and discrete-time invariant sets presented in Lemma 4.26, how the proposed algorithms can be used to deal with continuous-time systems as well.},
  doi       = {10.1007/978-3-319-17933-9_5},
  issn      = {23249757},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_690-1},
}

@InCollection{Thom2020,
  author    = {Thom, Nathan and Hand, Emily M.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Facial Attribute Recognition: A Survey}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--13},
  doi       = {10.1007/978-3-030-03243-2_815-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_815-1},
}

@InCollection{Flach2020,
  author    = {Flach, Boris and Hlavac, Vaclav},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Expectation-Maximization Algorithm}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  abstract  = {tep involves the computation of the expectation of the likelihood of allmodel parameters by including the hid- den variables as if they were observed. Eachmaximiza- tion step involves the computation of the maximum likelihood estimates of the parameters by maximizing the expected likelihood found during the expectation step. The parameters produced by the maximization step are then used to begin another expectation step, and the process is repeated. It can be shown that an EM iteration will not decrease the observed data likelihood function. How- ever, there is no guarantee that the iteration converges to amaximum likelihood estimator. “Expectation-maximization” has developed to be a general recipe and umbrella term for a class of algo- rithms that iterates between a type of expectation and maximization step. The Baum–Welch algorithm is an example of an EM algorithm specifically suited to HMMs.},
  doi       = {10.1007/978-3-030-03243-2_692-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_692-1},
}

@InCollection{Sankaranarayanan2020,
  author    = {Sankaranarayanan, Aswin C. and Baraniuk, Richard G.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Compressive Sensing}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_647-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_647-1},
}

@InCollection{Kumar2020,
  author    = {Kumar, Amit and Chellappa, Rama},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Face Alignment}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  abstract  = {https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/},
  doi       = {10.1007/978-3-030-03243-2_879-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_879-1},
}

@InCollection{Ebner2020,
  author    = {Ebner, Marc},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Color Constancy}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--9},
  doi       = {10.1007/978-3-030-03243-2_454-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_454-1},
}

@Book{Skalka2005,
  author    = {Skalka, Christian},
  editor    = {M{\"{u}}ller, Peter},
  publisher = {Springer International Publishing},
  title     = {{Programming languages and systems security}},
  year      = {2005},
  address   = {Cham},
  isbn      = {978-3-030-44913-1},
  number    = {3},
  series    = {Lecture Notes in Computer Science},
  volume    = {3},
  booktitle = {IEEE Security and Privacy},
  doi       = {10.1109/MSP.2005.77},
  issn      = {15407993},
  pages     = {80--83},
  url       = {http://link.springer.com/10.1007/978-3-030-44914-8},
}

@InCollection{Lempitsky2020,
  author    = {Lempitsky, Victor},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Autoencoder}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--6},
  doi       = {10.1007/978-3-030-03243-2_862-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_862-1},
}

@InCollection{GonzalezDiaz2020,
  author    = {Gonzalez-Diaz, Rocio and Stelldinger, Peer and Latecki, Longin Jan},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Digitization}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--8},
  doi       = {10.1007/978-3-030-03243-2_645-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_645-1},
}

@InCollection{Zaki2018,
  author    = {Zaki, Mohammed J. and {Meira, Jr}, Wagner},
  booktitle = {Data Mining and Analysis},
  publisher = {Springer International Publishing},
  title     = {{Dimensionality Reduction}},
  year      = {2018},
  address   = {Cham},
  pages     = {183--214},
  doi       = {10.1017/cbo9780511810114.008},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_652-1},
}

@InCollection{Viswanathan2020,
  author    = {Viswanathan, Ramanarayanan},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Data Fusion}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--3},
  doi       = {10.1007/978-3-030-03243-2_298-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_298-1},
}

@InCollection{Bansal2020,
  author    = {Bansal, Ankan and Ranjan, Rajeev and Castillo, Carlos D. and Chellappa, Rama},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Deep CNN-Based Face Recognition}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--9},
  doi       = {10.1007/978-3-030-03243-2_880-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_880-1},
}

@Book{Olsson2020,
  author    = {Olsson, Mikael},
  publisher = {Apress},
  title     = {{C# 8 Quick Syntax Reference}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5576-6},
  booktitle = {C# 8 Quick Syntax Reference},
  doi       = {10.1007/978-1-4842-5577-3},
  url       = {http://link.springer.com/10.1007/978-1-4842-5577-3},
}

@Book{Fiadeiro2015,
  author    = {Fiadeiro, Jos{\'{e}} Luiz and Liu, Zhiming},
  editor    = {Arbab, Farhad and Jongmans, Sung-Shik},
  publisher = {Springer International Publishing},
  title     = {{Formal Aspects of Component Software (FACS 2013)}},
  year      = {2015},
  address   = {Cham},
  isbn      = {978-3-030-40913-5},
  series    = {Lecture Notes in Computer Science},
  volume    = {113},
  booktitle = {Science of Computer Programming},
  doi       = {10.1016/j.scico.2015.11.001},
  issn      = {01676423},
  pages     = {221--222},
  url       = {http://link.springer.com/10.1007/978-3-030-40914-2},
}

@Book{Bors2020,
  author    = {Bors, Luc and Samajdwer, Ardhendu and van Oosterhout, Mascha},
  publisher = {Apress},
  title     = {{Oracle Digital Assistant}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5421-9},
  booktitle = {Oracle Digital Assistant},
  doi       = {10.1007/978-1-4842-5422-6},
  url       = {http://link.springer.com/10.1007/978-1-4842-5422-6},
}

@Book{Skansi2020,
  editor    = {Skansi, Sandro},
  publisher = {Springer International Publishing},
  title     = {{Guide to Deep Learning Basics}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-37590-4},
  booktitle = {Guide to Deep Learning Basics},
  doi       = {10.1007/978-3-030-37591-1},
  url       = {http://link.springer.com/10.1007/978-3-030-37591-1},
}

@Book{Rojas2020,
  author    = {Rojas, Carlos},
  publisher = {Apress},
  title     = {{Building Progressive Web Applications with Vue.js}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5333-5},
  booktitle = {Building Progressive Web Applications with Vue.js},
  doi       = {10.1007/978-1-4842-5334-2},
  url       = {http://link.springer.com/10.1007/978-1-4842-5334-2},
}

@Book{MargretAnouncia2020,
  author    = {{Margret Anouncia}, S. and Gohel, Hardik A. and Vairamuthu, Subbiah},
  editor    = {Anouncia, S. Margret and Gohel, Hardik A. and Vairamuthu, Subbiah},
  publisher = {Springer Singapore},
  title     = {{Data visualization: Trends and challenges toward multidisciplinary perception}},
  year      = {2020},
  address   = {Singapore},
  isbn      = {9789811522826},
  abstract  = {This book discusses the recent trends and developments in the fields of information processing and information visualization. In view of the increasing amount of data, there is a need to develop visualization techniques to make that data easily understandable. Presenting such approaches from various disciplines, this book serves as a useful resource for graduates.},
  booktitle = {Data Visualization: Trends and Challenges Toward Multidisciplinary Perception},
  doi       = {10.1007/978-981-15-2282-6},
  pages     = {1--179},
  url       = {https://link.springer.com/book/10.1007%2F978-981-15-2282-6},
}

@Book{Bucchiarone2020,
  editor    = {Bucchiarone, Antonio and Dragoni, Nicola and Dustdar, Schahram and Lago, Patricia and Mazzara, Manuel and Rivera, Victor and Sadovykh, Andrey},
  publisher = {Springer International Publishing},
  title     = {{Microservices}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-31645-7},
  booktitle = {Microservices},
  doi       = {10.1007/978-3-030-31646-4},
  url       = {http://link.springer.com/10.1007/978-3-030-31646-4},
}

@InCollection{Fukui2020,
  author    = {Fukui, Kazuhiro},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Subspace Methods}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_708-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_708-1},
}

@Book{Streib2011,
  author    = {Streib, James T.},
  publisher = {Springer International Publishing},
  title     = {{Guide to Assembly Language}},
  year      = {2011},
  address   = {Cham},
  isbn      = {978-3-030-35638-5},
  series    = {Undergraduate Topics in Computer Science},
  booktitle = {Guide to Assembly Language},
  doi       = {10.1007/978-0-85729-271-1},
  url       = {http://link.springer.com/10.1007/978-3-030-35639-2},
}

@Book{Milliken2020,
  author    = {Milliken, Connor P.},
  publisher = {Apress},
  title     = {{Python Projects for Beginners}},
  year      = {2020},
  address   = {Berkeley, CA},
  isbn      = {978-1-4842-5354-0},
  abstract  = {A Ten-Week Bootcamp Approach to Python Programming},
  booktitle = {Python Projects for Beginners},
  doi       = {10.1007/978-1-4842-5355-7},
  url       = {http://link.springer.com/10.1007/978-1-4842-5355-7},
}

@Book{Raposo2020,
  author    = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
  editor    = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
  publisher = {Springer International Publishing},
  title     = {{Correction to: Computational Intelligence Methods for Bioinformatics and Biostatistics}},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-34584-6},
  series    = {Lecture Notes in Computer Science},
  volume    = {11925},
  doi       = {10.1007/978-3-030-34585-3_31},
  pages     = {C1--C1},
  url       = {http://link.springer.com/10.1007/978-3-030-34585-3},
}

@InCollection{Fisher2020,
  author    = {Fisher, Robert B.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Subpixel Estimation}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  abstract  = {Sub-pixel estimation is the process of estimating the value of a geometric quantity to better than pixel accuracy, even though the data was originally sampled on an integer pixel quantized space.},
  doi       = {10.1007/978-3-030-03243-2_189-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_189-1},
}

@InCollection{Kang2020,
  author    = {Kang, Sing Bing},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Shiftable Windows for Stereo}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_795-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_795-1},
}

@InCollection{Zhang2016,
  author    = {Zhang, Yanchun and Xu, Guandong},
  booktitle = {Encyclopedia of Database Systems},
  publisher = {Springer International Publishing},
  title     = {{Singular Value Decomposition}},
  year      = {2016},
  address   = {Cham},
  pages     = {1--3},
  abstract  = {Photoacoustic imaging is a non-ionizing imaging modality that provides contrast consistent with optical imaging techniques while the resolution and penetration depth is similar to ultrasound techniques. In a previous publication Opt. Express 18, 11406 (2010), a technique was introduced to experimentally acquire the imaging operator for a photoacoustic imaging system. While this was an important foundation for future work, we have recently improved the experimental procedure allowing for a more densely populated imaging operator to be acquired. Subsets of the imaging operator were produced by varying the transducer count as well as the measurement space temporal sampling rate. Examination of the matrix rank and the effect of contributing object space singular vectors to image reconstruction were performed. For a PAI system collecting only limited data projections, matrix rank increased linearly with transducer count and measurement space temporal sampling rate. Image reconstruction using a regularized pseudoinverse of the imaging operator was performed on photoacoustic signals from a point source, line source, and an array of point sources derived from the imaging operator. As expected, image quality increased for each object with increasing transducer count and measurement space temporal sampling rate. Using the same approach, but on experimentally sampled photoacoustic signals from a moving point-like source, acquisition, data transfer, reconstruction and image display took 1.4 s using one laser pulse per 3D frame. With relatively simple hardware improvements to data transfer and computation speed, our current imaging results imply that acquisition and display of 3D photoacoustic images at laser repetition rates of 10Hz is easily achieved.},
  doi       = {10.1007/978-1-4899-7993-3_538-2},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_802-1},
}

@InCollection{Lampert2020,
  author    = {Lampert, Christoph H.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Zero-Shot Learning}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--3},
  doi       = {10.1007/978-3-030-03243-2_874-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_874-1},
}

@InCollection{Yang2020,
  author    = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
  booktitle = {Transfer Learning},
  publisher = {Springer International Publishing},
  title     = {{Transfer Learning}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  abstract  = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21% with default Glide SP settings to 58% with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63% success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40% of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  doi       = {10.1017/9781139061773},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_837-1},
}

@InCollection{Deguchi2020,
  author    = {Deguchi, Koichiro},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Regularization}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_828-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_828-1},
}

@InCollection{Rusinkiewicz2020,
  author    = {Rusinkiewicz, Szymon},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Reflectance Models}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_537-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_537-1},
}

@Book{Ramage2020,
  author    = {Ramage, Magnus and Shipp, Karen},
  publisher = {Springer London},
  title     = {{Systems Thinkers}},
  year      = {2020},
  address   = {London},
  isbn      = {978-1-4471-7474-5},
  abstract  = {Systems Thinkers presents a biographical history of the field of systems thinking, by examining the life and work of thirty of its major thinkers. It discusses each thinker's key contributions, the way this contribution was expressed in practice and the relationship between their life and ideas. This discussion is supported by an extract from the thinker's own writing, to give a flavour of their work and to give readers a sense of which thinkers are most relevant to their own interests. Systems thinking is necessarily interdisciplinary, so that the thinkers selected come from a wide range of areas - biology, management, physiology, anthropology, chemistry, public policy, sociology and environmental studies among others. Some are core innovators in systems ideas; some have been primarily practitioners who also advanced and popularised systems ideas; others are well-known figures who drew heavily upon systems thinking although it was not their primary discipline. A significant aim of the book is to broaden and deepen the reader's interest in systems writers, providing an appetising 'taster' for each of the 30 thinkers, so that the reader is encouraged to go on to study the published works of the thinkers themselves. Copyright {\textcopyright} 2009 The Open University. All rights reserved.},
  booktitle = {Systems Thinkers},
  doi       = {10.1007/978-1-4471-7475-2},
  pages     = {1--324},
  url       = {http://link.springer.com/10.1007/978-1-4471-7475-2},
}

@InCollection{Lin2020,
  author    = {Lin, Tong and Zha, Hongbin},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Riemannian Manifold}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--6},
  doi       = {10.1007/978-3-030-03243-2_801-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_801-1},
}

@InCollection{Matsushita2020,
  author    = {Matsushita, Yasuyuki},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Shape from Shading}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_829-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_829-1},
}

@InCollection{Panda2020,
  author    = {Panda, Rameswar and Roy-Chowdhury, Amit K.},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Person Re-identification: Current Approaches and Future Challenges}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_825-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_825-1},
}

@InCollection{Shotton2020,
  author    = {Shotton, Jamie and Kohli, Pushmeet},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Semantic Image Segmentation: Traditional Approach}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_251-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_251-1},
}

@InCollection{Picasso2002,
  author    = {Picasso, Princeton},
  booktitle = {Ieee Signal Processing Letters},
  publisher = {Springer International Publishing},
  title     = {{Principal Component Analysis Why Principal Component Analysis ?}},
  year      = {2002},
  address   = {Cham},
  isbn      = {9781439802847},
  number    = {2},
  pages     = {40--42},
  volume    = {9},
  doi       = {10.1201/b17700-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_649-1},
}

@InCollection{Meer2020,
  author    = {Meer, Peter},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Robust Estimation Techniques}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--8},
  doi       = {10.1007/978-3-030-03243-2_882-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_882-1},
}

@InCollection{Zhou2020,
  author    = {Zhou, Bolei},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Scene Classification}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  abstract  = {{\textcopyright} Springer International Publishing Switzerland 2015. There are a large number of small and inexpensive single-board computers with Linux operating systems available on the market today. Most of these aim for the consumer and enthusiast market, but can also be used in research and commercial applications. This paper builds on several years of experience with using such computers in student projects, as well as the development of cyber-physical and embedded control systems. A summary of the properties that are key for dependability for selected boards is given in tabulated form. These boards have interesting properties for many embedded and cyber-physical systems, e.g. high-performance, small size and low cost. The use of Linux for operating system means a development environment that is familiar to many developers, and the availability of many libraries and applications. While not suitable for applications were formally proven dependability is necessary, we argue that by actively mitigating some of the potential problems identified in this paper such computers can be used in many applications where high dependability is desirable, especially in combination with low-cost. A solution with redundant single-board computers is presented as a strategy for achieving high dependability. Due to the low cost and small size, this is feasible for applications were redundancy traditionally would be prohibitively too large or costly.},
  doi       = {10.1007/978-3-030-03243-2_799-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_799-1},
}

@InCollection{Luo2020,
  author    = {Luo, Chong and Zeng, Wenjun},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Monocular and Binocular People Tracking}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--4},
  doi       = {10.1007/978-3-030-03243-2_872-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_872-1},
}

@InCollection{Boyarski2020,
  author    = {Boyarski, Amit and Bronstein, Alex},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Multidimensional Scaling}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--14},
  doi       = {10.1007/978-3-030-03243-2_827-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_827-1},
}

@InCollection{Kemp2019,
  author    = {Kemp, Jonathan and Kemp, Jonathan},
  booktitle = {Film on Video},
  publisher = {Springer International Publishing},
  title     = {{Motion blur}},
  year      = {2019},
  address   = {Cham},
  pages     = {45--54},
  doi       = {10.4324/9780429468872-5},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_512-1},
}

@InCollection{Brox2020,
  author    = {Brox, Thomas},
  booktitle = {Computer Vision},
  publisher = {Springer International Publishing},
  title     = {{Optical Flow: Traditional Approaches}},
  year      = {2020},
  address   = {Cham},
  pages     = {1--5},
  doi       = {10.1007/978-3-030-03243-2_600-1},
  url       = {http://link.springer.com/10.1007/978-3-030-03243-2_600-1},
}

@Article{Krishnamurthy2018,
  author        = {Krishnamurthy, Gangeshwar and Majumder, Navonil and Poria, Soujanya and Cambria, Erik},
  journal       = {arXiv},
  title         = {{A deep learning approach for multimodal deception detection}},
  year          = {2018},
  issn          = {23318422},
  month         = {mar},
  abstract      = {Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14% and ROC-AUC of 0.9799.},
  archiveprefix = {arXiv},
  arxivid       = {1803.00344},
  eprint        = {1803.00344},
  url           = {http://arxiv.org/abs/1803.00344},
}

@Misc{Ikutani2020,
  author    = {Ikutani, Yoshiharu and {Takatomi Kubo} and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
  title     = {{Decoding functional category of source code from the brain (fMRI on Java program comprehension)}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002411.V1.1.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002411/versions/1.1.0},
}

@Misc{Zhang2020,
  author    = {Zhang, S and Yoshida, W and Mano, H and Yanagisawa, T and Shibata, K and Kawato, M and Seymour, B},
  title     = {{Cognitive control of sensory pain encoding in the pregenual anterior cingulate cortex. d1 - decoder construction in day 1, d2 - adaptive control in day 2.}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS002596.V1.0.0},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds002596/versions/1.0.0},
}

@Misc{GuohuaShen2020,
  author    = {{Guohua Shen} and {Tomoyasu Horikawa} and Majima, Kei and {Yukiyasu Kamitani}},
  title     = {{Deep Image Reconstruction}},
  year      = {2020},
  doi       = {10.18112/OPENNEURO.DS001506.V1.3.1},
  publisher = {Openneuro},
  url       = {https://openneuro.org/datasets/ds001506/versions/1.3.1},
}

@Article{Ikutani2020a,
  author    = {Ikutani, Yoshiharu and Kubo, Takatomi and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
  journal   = {bioRxiv},
  title     = {{Expert programmers have fine-tuned cortical representations of source code}},
  year      = {2020},
  issn      = {2373-2822},
  pages     = {2020.01.28.923953},
  abstract  = {Expertise enables humans to achieve outstanding performance on domain-specific tasks, and programming is no exception. Many have shown that expert programmers exhibit remarkable differences from novices in behavioral performance, knowledge structure, and selective attention. However, the underlying differences in the brain are still unclear. We here address this issue by associating the cortical representation of source code with individual programming expertise using a data-driven decoding approach. This approach enabled us to identify seven brain regions, widely distributed in the frontal, parietal, and temporal cortices, that have a tight relationship with programming expertise. In these brain regions, functional categories of source code could be decoded from brain activity and the decoding accuracies were significantly correlated with individual behavioral performances on source-code categorization. Our results suggest that programming expertise is built up on fine-tuned cortical representations specialized for the domain of programming.},
  doi       = {10.1101/2020.01.28.923953},
  publisher = {Cold Spring Harbor Laboratory},
  url       = {https://www.biorxiv.org/content/10.1101/2020.01.28.923953v1},
}

@Article{Shen2019,
  author   = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
  journal  = {PLoS Computational Biology},
  title    = {{Deep image reconstruction from human brain activity}},
  year     = {2019},
  issn     = {15537358},
  month    = {jan},
  number   = {1},
  pages    = {e1006633},
  volume   = {15},
  abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
  doi      = {10.1371/journal.pcbi.1006633},
  editor   = {O'Reilly, Jill},
  pmid     = {30640910},
  url      = {http://dx.plos.org/10.1371/journal.pcbi.1006633},
}

@Article{Chang2018,
  author        = {Chang, Nadine and Pyles, John A. and Gupta, Abhinav and Tarr, Michael J. and Aminoff, Elissa M.},
  journal       = {arXiv},
  title         = {{BOLD5000 A public fMRI dataset of 5000 images}},
  year          = {2018},
  issn          = {23318422},
  month         = {sep},
  number        = {1},
  pages         = {49},
  volume        = {6},
  abstract      = {Vision science, particularly machine vision, has been revolutionized by introducing large-scale image datasets and statistical learning approaches. Yet, human neuroimaging studies of visual perception still rely on small numbers of images (around 100) due to time-constrained experimental procedures. To apply statistical learning approaches that integrate neuroscience, the number of images used in neuroimaging must be significantly increased. We present BOLD5000, a human functional MRI (fMRI) study that includes almost 5,000 distinct images depicting real-world scenes. Beyond dramatically increasing image dataset size relative to prior fMRI studies, BOLD5000 also accounts for image diversity, overlapping with standard computer vision datasets by incorporating images from the Scene UNderstanding (SUN), Common Objects in Context (COCO), and ImageNet datasets. The scale and diversity of these image datasets, combined with a slow event-related fMRI design, enable fine-grained exploration into the neural representation of a wide range of visual features, categories, and semantics. Concurrently, BOLD5000 brings us closer to realizing Marr's dream of a singular vision science - the intertwined study of biological and computer vision.},
  archiveprefix = {arXiv},
  arxivid       = {1809.01281},
  doi           = {10.1038/s41597-019-0052-3},
  eprint        = {1809.01281},
  url           = {http://dx.doi.org/10.1038/s41597-019-0052-3},
}

@Article{Lepping2016a,
  author   = {Lepping, Rebecca J. and Atchley, Ruth Ann and Savage, Cary R.},
  journal  = {Psychology of Music},
  title    = {{Development of a validated emotionally provocative musical stimulus set for research}},
  year     = {2016},
  issn     = {17413087},
  month    = {sep},
  number   = {5},
  pages    = {1012--1028},
  volume   = {44},
  abstract = {Music is a strong emotional stimulus; however, it is difficult to differentiate the effects of arousal and valence. While emotional stimuli sets have been created from words and pictures, a normed set of musical stimuli is unavailable. The goal of this project was to identify a set of ecologically valid musical stimuli for use in research studies of emotion and mood that differ on valence but are matched for arousal, and are also matched with emotionally evocative non-musical stimuli. Three rating experiments were conducted. In the first Stimulus Selection experiment, participants rated short music clips for valence and arousal, and the most evocative clips were selected. In two follow-up Stimulus Validation experiments, two additional groups of participants rated the selected musical and non-musical stimuli, with Experiment 3 confirming that these stimuli work effectively in a noisy environment (within an MRI scanner). The result of these three studies is a set of emotionally evocative, positive and negative musical stimuli, matched for valence and arousal with a subset of previously validated non-musical stimuli.},
  doi      = {10.1177/0305735615604509},
  keywords = {arousal,auditory perception/cognition,emotion,experimental aesthetics,negative emotions,valence},
  url      = {http://journals.sagepub.com/doi/10.1177/0305735615604509},
}

@Article{Lepping2016,
  author   = {Lepping, Rebecca J. and Atchley, Ruth Ann and Chrysikou, Evangelia and Martin, Laura E. and Clair, Alicia A. and Ingram, Rick E. and Simmons, W. Kyle and Savage, Cary R.},
  journal  = {PLoS ONE},
  title    = {{Neural processing of emotional musical and nonmusical stimuli in depression}},
  year     = {2016},
  issn     = {19326203},
  month    = {jun},
  number   = {6},
  pages    = {e0156859},
  volume   = {11},
  abstract = {Background Anterior cingulate cortex (ACC) and striatum are part of the emotional neural circuitry implicated in major depressive disorder (MDD). Music is often used for emotion regulation, and pleasurable music listening activates the dopaminergic system in the brain, including the ACC. The present study uses functional MRI (fMRI) and an emotional nonmusical and musical stimuli paradigm to examine how neural processing of emotionally provocative auditory stimuli is altered within the ACC and striatum in depression. Method Nineteen MDD and 20 never-depressed (ND) control participants listened to standardized positive and negative emotional musical and nonmusical stimuli during fMRI scanning and gave subjective ratings of valence and arousal following scanning. Results ND participants exhibited greater activation to positive versus negative stimuli in ventral ACC. When compared with ND participants, MDD participants showed a different pattern of activation in ACC. In the rostral part of the ACC, ND participants showed greater activation for positive information, while MDD participants showed greater activation to negative information. In dorsal ACC, the pattern of activation distinguished between the types of stimuli, with ND participants showing greater activation to music compared to nonmusical stimuli, while MDD participants showed greater activation to nonmusical stimuli, with the greatest response to negative nonmusical stimuli. No group differences were found in striatum. Conclusions These results suggest that people with depression may process emotional auditory stimuli differently based on both the type of stimulation and the emotional content of that stimulation. This raises the possibility that music may be useful in retraining ACC function, potentially leading to more effective and targeted treatments.},
  doi      = {10.1371/journal.pone.0156859},
  editor   = {Kotz, Sonja},
  pmid     = {27284693},
  url      = {http://dx.plos.org/10.1371/journal.pone.0156859},
}

@Article{Dugre2019,
  author   = {Dugr{\'{e}}, Jules R. and Bitar, Nathalie and Dumais, Alexandre and Potvin, St{\'{e}}phane},
  journal  = {American Journal of Psychiatry},
  title    = {{Limbic hyperactivity in response to emotionally neutral stimuli in schizophrenia: A neuroimaging meta-analysis of the hypervigilant mind}},
  year     = {2019},
  issn     = {15357228},
  month    = {dec},
  number   = {12},
  pages    = {1021--1029},
  volume   = {176},
  abstract = {Objective: It has long been assumed that paranoid ideation may stem from an aberrant limbic response to threatening stimuli. However, results from functional neuroimaging studies using negative emotional stimuli have failed to confirm this assumption. One of the potential reasons for the lack of effect is that study participants with psychosis may display aberrant brain responses to neutral material rather than to threatening stimuli. The authors conducted a functional neuroimaging meta-analysis to test this hypothesis. Methods: A literature search was performed with PubMed, Google Scholar, and Embase to identify functional neuroimaging studies examining brain responses to neutral material in patients with psychosis. A total of 23 studies involving schizophrenia patients were retrieved. Using t-maps of peak coordinates to calculate effect sizes, a random-effects model meta-analysis was performed with the anisotropic effect-size version of Seed-based d Mapping software. Results: In schizophrenia patients relative to healthy control subjects, increased activations were observed in the left and right amygdala and parahippocampus and the left putamen, hippocampus, and insula in response to neutral stimuli. Conclusions: Given that several limbic regions were found to be more activated in schizophrenia patients than in control subjects, the results of this meta-analysis strongly suggest that these patients confer aberrant emotional significance to nonthreatening stimuli. In theory, this abnormal brain reactivity may fuel delusional thoughts. Studies are needed in individuals at risk of psychosis to determine whether aberrant limbic reactivity to neutral stimuli is an early neurofunctional marker of psychosis vulnerability.},
  doi      = {10.1176/appi.ajp.2019.19030247},
  pmid     = {31509006},
  url      = {http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2019.19030247},
}

@Article{BrendanMcMahan2017,
  author        = {{Brendan McMahan}, H. and Moore, Eider and Ramage, Daniel and Hampson, Seth and {Ag{\"{u}}era y Arcas}, Blaise},
  journal       = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
  title         = {{Communication-efficient learning of deep networks from decentralized data}},
  year          = {2017},
  month         = {feb},
  abstract      = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100x as compared to synchronized stochastic gradient descent.},
  archiveprefix = {arXiv},
  arxivid       = {1602.05629},
  eprint        = {1602.05629},
  url           = {http://arxiv.org/abs/1602.05629},
}

@InCollection{Sheller2019,
  author        = {Sheller, Micah J. and Reina, G. Anthony and Edwards, Brandon and Martin, Jason and Bakas, Spyridon},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation}},
  year          = {2019},
  isbn          = {9783030117221},
  pages         = {92--104},
  volume        = {11383 LNCS},
  abstract      = {Deep learning models for semantic segmentation of images require large amounts of data. In the medical imaging domain, acquiring sufficient data is a significant challenge. Labeling medical image data requires expert knowledge. Collaboration between institutions could address this challenge, but sharing medical data to a centralized location faces various legal, privacy, technical, and data-ownership challenges, especially among international institutions. In this study, we introduce the first use of federated learning for multi-institutional collaboration, enabling deep learning modeling without sharing patient data. Our quantitative results demonstrate that the performance of federated semantic segmentation models (Dice = 0.852) on multimodal brain scans is similar to that of models trained by sharing data (Dice = 0.862). We compare federated learning with two alternative collaborative learning methods and find that they fail to match the performance of federated learning.},
  archiveprefix = {arXiv},
  arxivid       = {1810.04304},
  doi           = {10.1007/978-3-030-11723-8_9},
  eprint        = {1810.04304},
  issn          = {16113349},
  keywords      = {BraTS,Deep learning,Federated,Glioma,Incremental,Machine learning,Segmentation},
  url           = {http://link.springer.com/10.1007/978-3-030-11723-8_9},
}

@Article{Finlayson2018,
  author        = {Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.},
  journal       = {arXiv},
  title         = {{Adversarial Attacks Against Medical Deep Learning Systems}},
  year          = {2018},
  month         = {apr},
  abstract      = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.},
  archiveprefix = {arXiv},
  arxivid       = {1804.05296},
  eprint        = {1804.05296},
  keywords      = {Adversarial examples,Deep learning,Healthcare,Neural networks,Security},
  url           = {http://arxiv.org/abs/1804.05296},
}

@Article{art/KonecnyJ_201610,
  author        = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  title         = {{Federated Learning: Strategies for Improving Communication Efficiency}},
  year          = {2016},
  month         = {oct},
  abstract      = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
  archiveprefix = {arXiv},
  arxivid       = {1610.05492},
  eprint        = {1610.05492},
  url           = {http://arxiv.org/abs/1610.05492},
}

@Article{Mirsky2019,
  author        = {Mirsky, Yisroel and Mahler, Tom and Shelef, Ilan and Elovici, Yuval},
  journal       = {Proceedings of the 28th USENIX Security Symposium},
  title         = {{CT-GAN: Malicious tampering of 3D medical imagery using deep learning}},
  year          = {2019},
  month         = {jan},
  pages         = {461--478},
  abstract      = {In 2018, clinics and hospitals were hit with numerous attacks leading to significant data breaches and interruptions in medical services. An attacker with access to medical records can do much more than hold the data for ransom or sell it on the black market. In this paper, we show how an attacker can use deep-learning to add or remove evidence of medical conditions from volumetric (3D) medical scans. An attacker may perform this act in order to stop a political candidate, sabotage research, commit insurance fraud, perform an act of terrorism, or even commit murder. We implement the attack using a 3D conditional GAN and show how the framework (CT-GAN) can be automated. Although the body is complex and 3D medical scans are very large, CT-GAN achieves realistic results which can be executed in milliseconds. To evaluate the attack, we focused on injecting and removing lung cancer from CT scans. We show how three expert radiologists and a state-of-the-art deep learning AI are highly susceptible to the attack. We also explore the attack surface of a modern radiology network and demonstrate one attack vector: we intercepted and manipulated CT scans in an active hospital network with a covert penetration test.},
  archiveprefix = {arXiv},
  arxivid       = {1901.03597},
  eprint        = {1901.03597},
  isbn          = {9781939133069},
  url           = {http://arxiv.org/abs/1901.03597},
}

@Article{Zbontar2018,
  author        = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
  journal       = {arXiv},
  title         = {{fastMRI: An open dataset and benchmarks for accelerated MRI}},
  year          = {2018},
  issn          = {23318422},
  month         = {nov},
  abstract      = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
  archiveprefix = {arXiv},
  arxivid       = {1811.08839},
  eprint        = {1811.08839},
  url           = {http://arxiv.org/abs/1811.08839},
}

@Article{Kuijf2019,
  author        = {Kuijf, Hugo J. and Casamitjana, Adri{\`{a}} and Collins, D. Louis and Dadar, Mahsa and Georgiou, Achilleas and Ghafoorian, Mohsen and Jin, Dakai and Khademi, April and Knight, Jesse and Li, Hongwei and Llad{\'{o}}, Xavier and Biesbroek, J. Matthijs and Luna, Miguel and Mahmood, Qaiser and Mckinley, Richard and Mehrtash, Alireza and Ourselin, Sebastien and Park, Bo Yong and Park, Hyunjin and Park, Sang Hyun and Pezold, Simon and Puybareau, Elodie and {De Bresser}, Jeroen and Rittner, Leticia and Sudre, Carole H. and Valverde, Sergi and Vilaplana, Veronica and Wiest, Roland and Xu, Yongchao and Xu, Ziyue and Zeng, Guodong and Zhang, Jianguo and Zheng, Guoyan and Heinen, Rutger and Chen, Christopher and {Van Der Flier}, Wiesje and Barkhof, Frederik and Viergever, Max A. and Biessels, Geert Jan and Andermatt, Simon and Bento, Mariana and Berseth, Matt and Belyaev, Mikhail and Cardoso, M. Jorge},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge}},
  year          = {2019},
  issn          = {1558254X},
  month         = {nov},
  number        = {11},
  pages         = {2556--2568},
  volume        = {38},
  abstract      = {Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. The automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their methods on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge. Sixty T1 + FLAIR images from three MR scanners were released with the manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. The segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: 1) Dice similarity coefficient; 2) modified Hausdorff distance (95th percentile); 3) absolute log-transformed volume difference; 4) sensitivity for detecting individual lesions; and 5) F1-score for individual lesions. In addition, the methods were ranked on their inter-scanner robustness; 20 participants submitted their methods for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all the methods generalize to unseen scanners. The challenge remains open for future submissions and provides a public platform for method evaluation.},
  archiveprefix = {arXiv},
  arxivid       = {1904.00682},
  doi           = {10.1109/TMI.2019.2905770},
  eprint        = {1904.00682},
  keywords      = {Magnetic resonance imaging (MRI),brain,evaluation and performance,segmentation},
  pmid          = {30908194},
  url           = {https://ieeexplore.ieee.org/document/8669968/},
}

@InProceedings{Dolph2017,
  author    = {Dolph, C. V. and Alam, M. and Shboul, Z. and Samad, M. D. and Iftekharuddin, K. M.},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks},
  title     = {{Deep learning of texture and structural features for multiclass Alzheimer's disease classification}},
  year      = {2017},
  month     = {may},
  pages     = {2259--2266},
  publisher = {IEEE},
  volume    = {2017-May},
  abstract  = {This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6% and 58.0% tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4% and 56.8%. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2% in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.},
  doi       = {10.1109/IJCNN.2017.7966129},
  isbn      = {9781509061815},
  keywords  = {ADNI,Alzheimer's disease,Biomarkers,Deep learning,Dropout learning,Hippocampus,Neuroimaging classification},
  url       = {http://ieeexplore.ieee.org/document/7966129/},
}

@Article{art/SoerensenL_2017,
  author   = {S{\o}rensen, Lauge and Igel, Christian and Pai, Akshay and Balas, Ioana and Anker, Cecilie and Lillholm, Martin and Nielsen, Mads},
  journal  = {NeuroImage: Clinical},
  title    = {{Differential diagnosis of mild cognitive impairment and Alzheimer's disease using structural MRI cortical thickness, hippocampal shape, hippocampal texture, and volumetry}},
  year     = {2017},
  issn     = {22131582},
  pages    = {470--482},
  volume   = {13},
  abstract = {This paper presents a brain T1-weighted structural magnetic resonance imaging (MRI) biomarker that combines several individual MRI biomarkers (cortical thickness measurements, volumetric measurements, hippocampal shape, and hippocampal texture). The method was developed, trained, and evaluated using two publicly available reference datasets: a standardized dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the imaging arm of the Australian Imaging Biomarkers and Lifestyle flagship study of ageing (AIBL). In addition, the method was evaluated by participation in the Computer-Aided Diagnosis of Dementia (CADDementia) challenge. Cross-validation using ADNI and AIBL data resulted in a multi-class classification accuracy of 62.7% for the discrimination of healthy normal controls (NC), subjects with mild cognitive impairment (MCI), and patients with Alzheimer's disease (AD). This performance generalized to the CADDementia challenge where the method, trained using the ADNI and AIBL data, achieved a classification accuracy 63.0%. The obtained classification accuracy resulted in a first place in the challenge, and the method was significantly better (McNemar's test) than the bottom 24 methods out of the total of 29 methods contributed by 15 different teams in the challenge. The method was further investigated with learning curve and feature selection experiments using ADNI and AIBL data. The learning curve experiments suggested that neither more training data nor a more complex classifier would have improved the obtained results. The feature selection experiment showed that both common and uncommon individual MRI biomarkers contributed to the performance; hippocampal volume, ventricular volume, hippocampal texture, and parietal lobe thickness were the most important. This study highlights the need for both subtle, localized measurements and global measurements in order to discriminate NC, MCI, and AD simultaneously based on a single structural MRI scan. It is likely that additional non-structural MRI features are needed to further improve the obtained performance, especially to improve the discrimination between NC and MCI.},
  doi      = {10.1016/j.nicl.2016.11.025},
  keywords = {Alzheimer's disease,Biomarker,Classification,Machine learning,Mild cognitive impairment,Structural MRI},
  pmid     = {28119818},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302340},
}

@Article{Wachinger2016,
  author   = {Wachinger, Christian and Reuter, Martin},
  journal  = {NeuroImage},
  title    = {{Domain adaptation for Alzheimer's disease diagnostics}},
  year     = {2016},
  issn     = {10959572},
  month    = {oct},
  pages    = {470--479},
  volume   = {139},
  abstract = {With the increasing prevalence of Alzheimer's disease, research focuses on the early computer-aided diagnosis of dementia with the goal to understand the disease process, determine risk and preserving factors, and explore preventive therapies. By now, large amounts of data from multi-site studies have been made available for developing, training, and evaluating automated classifiers. Yet, their translation to the clinic remains challenging, in part due to their limited generalizability across different datasets. In this work, we describe a compact classification approach that mitigates overfitting by regularizing the multinomial regression with the mixed ℓ1/ℓ2 norm. We combine volume, thickness, and anatomical shape features from MRI scans to characterize neuroanatomy for the three-class classification of Alzheimer's disease, mild cognitive impairment and healthy controls. We demonstrate high classification accuracy via independent evaluation within the scope of the CADDementia challenge. We, furthermore, demonstrate that variations between source and target datasets can substantially influence classification accuracy. The main contribution of this work addresses this problem by proposing an approach for supervised domain adaptation based on instance weighting. Integration of this method into our classifier allows us to assess different strategies for domain adaptation. Our results demonstrate (i) that training on only the target training set yields better results than the na{\"{i}}ve combination (union) of source and target training sets, and (ii) that domain adaptation with instance weighting yields the best classification results, especially if only a small training component of the target dataset is available. These insights imply that successful deployment of systems for computer-aided diagnostics to the clinic depends not only on accurate classifiers that avoid overfitting, but also on a dedicated domain adaptation strategy.},
  doi      = {10.1016/j.neuroimage.2016.05.053},
  keywords = {Alzheimer's disease,Classification,Computer-aided diagnosis,Domain adaptation},
  pmid     = {27262241},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916301732},
}

@Article{Paper2015a,
  author   = {Paper, Scientific and Bron, E and Smits, M and Barkhof, F and Niessen, W and Klein, S},
  title    = {{Large-scale objective comparison of 29 novel algorithms for computer-aided diagnosis of dementia based on structural}},
  year     = {2015},
  pages    = {1--18},
  keywords = {applications-detection,cad,comparative studies,computer,computer applications,diagnosis,mr,neuroradiology brain},
}

@Article{Akkus2017,
  author   = {Akkus, Zeynettin and Ali, Issa and Sedl{\'{a}}ř, Jiř{\'{i}} and Agrawal, Jay P. and Parney, Ian F. and Giannini, Caterina and Erickson, Bradley J.},
  journal  = {Journal of Digital Imaging},
  title    = {{Predicting Deletion of Chromosomal Arms 1p/19q in Low-Grade Gliomas from MR Images Using Machine Intelligence}},
  year     = {2017},
  issn     = {1618727X},
  month    = {aug},
  number   = {4},
  pages    = {469--476},
  volume   = {30},
  abstract = {Several studies have linked codeletion of chromosome arms 1p/19q in low-grade gliomas (LGG) with positive response to treatment and longer progression-free survival. Hence, predicting 1p/19q status is crucial for effective treatment planning of LGG. In this study, we predict the 1p/19q status from MR images using convolutional neural networks (CNN), which could be a non-invasive alternative to surgical biopsy and histopathological analysis. Our method consists of three main steps: image registration, tumor segmentation, and classification of 1p/19q status using CNN. We included a total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status (57 non-deleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2 images. We divided our data into training, validation, and test sets. The training data was balanced for equal class probability and was then augmented with iterations of random translational shift, rotation, and horizontal and vertical flips to increase the size of the training set. We shuffled and augmented the training data to counter overfitting in each epoch. Finally, we evaluated several configurations of a multi-scale CNN architecture until training and validation accuracies became consistent. The results of the best performing configuration on the unseen test set were 93.3% (sensitivity), 82.22% (specificity), and 87.7% (accuracy). Multi-scale CNN with their self-learning capability provides promising results for predicting 1p/19q status non-invasively based on T1C and T2 images. Predicting 1p/19q status non-invasively from MR images would allow selecting effective treatment strategies for LGG patients without the need for surgical biopsy.},
  doi      = {10.1007/s10278-017-9984-3},
  keywords = {1p/19q codeletion,Convolutional neural networks,Low grade gliomas,Therapy response},
  pmid     = {28600641},
  url      = {http://link.springer.com/10.1007/s10278-017-9984-3},
}

@Article{Commowick2018,
  author   = {Commowick, Olivier and Istace, Audrey and Kain, Micha{\"{e}}l and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Pop, Sorina Camarasu and Girard, Pascal and Am{\'{e}}li, Roxana and Ferr{\'{e}}, Jean Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and McKinley, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\'{o}}, Xavier and Santos, Michel M. and Santos, Wellington P. and Silva-Filho, Abel G. and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K. and Cotton, Fran{\c{c}}ois and Barillot, Christian},
  journal  = {Scientific Reports},
  title    = {{Objective Evaluation of Multiple Sclerosis Lesion Segmentation using a Data Management and Processing Infrastructure}},
  year     = {2018},
  issn     = {20452322},
  month    = {dec},
  number   = {1},
  pages    = {13650},
  volume   = {8},
  abstract = {We present a study of multiple sclerosis segmentation algorithms conducted at the international MICCAI 2016 challenge. This challenge was operated using a new open-science computing infrastructure. This allowed for the automatic and independent evaluation of a large range of algorithms in a fair and completely automatic manner. This computing infrastructure was used to evaluate thirteen methods of MS lesions segmentation, exploring a broad range of state-of-theart algorithms, against a high-quality database of 53 MS cases coming from four centers following a common definition of the acquisition protocol. Each case was annotated manually by an unprecedented number of seven different experts. Results of the challenge highlighted that automatic algorithms, including the recent machine learning methods (random forests, deep learning, {\ldots}), are still trailing human expertise on both detection and delineation criteria. In addition, we demonstrate that computing a statistically robust consensus of the algorithms performs closer to human expertise on one score (segmentation) although still trailing on detection scores.},
  doi      = {10.1038/s41598-018-31911-7},
  pmid     = {30209345},
  url      = {http://www.nature.com/articles/s41598-018-31911-7},
}

@Misc{Commowick2018a,
  author    = {Commowick, Olivier and Istace, Audrey and Kain, Michael and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Camarasu-Pop, Sorina and Girard, Pascal and Ameli, Roxana and Ferr{\'{e}}, Jean-Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and {Mc Kinley}, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\`{o}}, Xavier and Santos, Michel M and Santos, Wellington P and Silva-Filho, Abel G and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K and Cotton, Fran{\c{c}}ois and Barillot, Christian},
  title     = {{MICCAI 2016 MS lesion segmentation challenge: supplementary results}},
  year      = {2018},
  doi       = {10.5281/zenodo.1307653},
  publisher = {Zenodo},
  url       = {https://doi.org/10.5281/zenodo.1307653},
}

@Article{Vallieres2017,
  author        = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Aerts, Hugo J.W.L. and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang Shu and Sultanem, Khalil and Seuntjens, Jan and {El Naqa}, Issam},
  journal       = {Scientific Reports},
  title         = {{Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer}},
  year          = {2017},
  issn          = {20452322},
  month         = {dec},
  number        = {1},
  pages         = {10117},
  volume        = {7},
  abstract      = {Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.},
  archiveprefix = {arXiv},
  arxivid       = {1703.08516},
  doi           = {10.1038/s41598-017-10371-5},
  eprint        = {1703.08516},
  pmid          = {28860628},
  url           = {http://www.nature.com/articles/s41598-017-10371-5},
}

@Article{Carass2017,
  author   = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Prados, Ferran and Sudre, Carole H. and {Jorge Cardoso}, Manuel and Cawley, Niamh and Ciccarelli, Olga and Wheeler-Kingshott, Claudia A.M. and Ourselin, S{\'{e}}bastien and Catanese, Laurence and Deshpande, Hrishikesh and Maurel, Pierre and Commowick, Olivier and Barillot, Christian and Tomas-Fernandez, Xavier and Warfield, Simon K. and Vaidya, Suthirth and Chunduru, Abhijith and Muthuganapathy, Ramanathan and Krishnamurthi, Ganapathy and Jesson, Andrew and Arbel, Tal and Maier, Oskar and Handels, Heinz and Iheme, Leonardo O. and Unay, Devrim and Jain, Saurabh and Sima, Diana M. and Smeets, Dirk and Ghafoorian, Mohsen and Platel, Bram and Birenbaum, Ariel and Greenspan, Hayit and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
  journal  = {NeuroImage},
  title    = {{Longitudinal multiple sclerosis lesion segmentation: Resource and challenge}},
  year     = {2017},
  issn     = {10959572},
  month    = {mar},
  pages    = {77--102},
  volume   = {148},
  abstract = {In conjunction with the ISBI 2015 conference, we organized a longitudinal lesion segmentation challenge providing training and test data to registered participants. The training data consisted of five subjects with a mean of 4.4 time-points, and test data of fourteen subjects with a mean of 4.4 time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. Eleven teams submitted results using state-of-the-art lesion segmentation algorithms to the challenge, with ten teams presenting their results at the conference. We present a quantitative evaluation comparing the consistency of the two raters as well as exploring the performance of the eleven submitted results in addition to three other lesion segmentation algorithms. The challenge presented three unique opportunities: (1) the sharing of a rich data set; (2) collaboration and comparison of the various avenues of research being pursued in the community; and (3) a review and refinement of the evaluation metrics currently in use. We report on the performance of the challenge participants, as well as the construction and evaluation of a consensus delineation. The image data and manual delineations will continue to be available for download, through an evaluation website2 The Challenge Evaluation Website is: http://smart-stats-tools.org/lesion-challenge-2015 as a resource for future researchers in the area. This data resource provides a platform to compare existing methods in a fair and consistent manner to each other and multiple manual raters.},
  doi      = {10.1016/j.neuroimage.2016.12.064},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916307819},
}

@InProceedings{Wegmayr2018,
  author    = {Wegmayr, Viktor and Aitharaju, Sai and Buhmann, Joachim},
  booktitle = {Medical Imaging 2018: Computer-Aided Diagnosis},
  title     = {{Classification of brain MRI with big data and deep 3D convolutional neural networks}},
  year      = {2018},
  editor    = {Mori, Kensaku and Petrick, Nicholas},
  month     = {feb},
  pages     = {63},
  publisher = {SPIE},
  abstract  = {Our ever-aging society faces the growing problem of neurodegenerative diseases, in particular dementia. Magnetic Resonance Imaging provides a unique tool for non-invasive investigation of these brain diseases. However, it is extremely difficult for neurologists to identify complex disease patterns from large amounts of three-dimensional images. In contrast, machine learning excels at automatic pattern recognition from large amounts of data. In particular, deep learning has achieved impressive results in image classification. Unfortunately, its application to medical image classification remains difficult. We consider two reasons for this difficulty: First, volumetric medical image data is considerably scarcer than natural images. Second, the complexity of 3D medical images is much higher compared to common 2D images. To address the problem of small data set size, we assemble the largest dataset ever used for training a deep 3D convolutional neural network to classify brain images as healthy (HC), mild cognitive impairment (MCI) or Alzheimers disease (AD). We use more than 20.000 images from subjects of these three classes, which is almost 9x the size of the previously largest data set. The problem of high dimensionality is addressed by using a deep 3D convolutional neural network, which is state-of-the-art in large-scale image classification. We exploit its ability to process the images directly, only with standard preprocessing, but without the need for elaborate feature engineering. Compared to other work, our workflow is considerably simpler, which increases clinical applicability. Accuracy is measured on the ADNI+AIBL data sets, and the independent CADDementia benchmark.},
  doi       = {10.1117/12.2293719},
  isbn      = {9781510616394},
  issn      = {16057422},
  url       = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10575/2293719/Classification-of-brain-MRI-with-big-data-and-deep-3D/10.1117/12.2293719.full},
}

@Article{Paper2015,
  author = {Paper, Scientific and Lillholm, M and Pai, A and Balas, I and Anker, C and Igel, C and Nielsen, M},
  title  = {{Improved Alzheimer ' s disease diagnostic performance using structural MRI : validation of the MRI combination biomarker that won the CADDementia challenge}},
  year   = {2015},
}

@Article{Carass2017a,
  author   = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
  journal  = {Data in Brief},
  title    = {{Longitudinal multiple sclerosis lesion segmentation data resource}},
  year     = {2017},
  issn     = {23523409},
  month    = {jun},
  pages    = {346--350},
  volume   = {12},
  abstract = {The data presented in this article is related to the research article entitled “Longitudinal multiple sclerosis lesion segmentation: Resource and challenge” (Carass et al., 2017) [1]. In conjunction with the 2015 International Symposium on Biomedical Imaging, we organized a longitudinal multiple sclerosis (MS) lesion segmentation challenge providing training and test data to registered participants. The training data consists of five subjects with a mean of 4.4 (±0.55) time-points, and test data of fourteen subjects with a mean of 4.4 (±0.67) time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. The training data including multi-modal scans and manually delineated lesion masks is available for download.1 In addition, the testing data is also being made available in conjunction with a website for evaluating the automated analysis of the testing data.},
  doi      = {10.1016/j.dib.2017.04.004},
  keywords = {Magnetic resonance imaging,Multiple sclerosis},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2352340917301361},
}

@Article{Bakas2017,
  author   = {Bakas, Spyridon and Akbari, Hamed and Sotiras, Aristeidis and Bilello, Michel and Rozycki, Martin and Kirby, Justin S. and Freymann, John B. and Farahani, Keyvan and Davatzikos, Christos},
  journal  = {Scientific Data},
  title    = {{Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features}},
  year     = {2017},
  issn     = {20524463},
  month    = {dec},
  number   = {1},
  pages    = {170117},
  volume   = {4},
  abstract = {Gliomas belong to a group of central nervous system tumors, and consist of various sub-regions. Gold standard labeling of these sub-regions in radiographic imaging is essential for both clinical and computational studies, including radiomic and radiogenomic analyses. Towards this end, we release segmentation labels and radiomic features for all pre-operative multimodal magnetic resonance imaging (MRI) (n=243) of the multi-institutional glioma collections of The Cancer Genome Atlas (TCGA), publicly available in The Cancer Imaging Archive (TCIA). Pre-operative scans were identified in both glioblastoma (TCGA-GBM, n=135) and low-grade-glioma (TCGA-LGG, n=108) collections via radiological assessment. The glioma sub-region labels were produced by an automated state-of-the-art method and manually revised by an expert board-certified neuroradiologist. An extensive panel of radiomic features was extracted based on the manually-revised labels. This set of labels and features should enable i) direct utilization of the TCGA/TCIA glioma collections towards repeatable, reproducible and comparative quantitative studies leading to new predictive, prognostic, and diagnostic assessments, as well as ii) performance evaluation of computer-aided segmentation methods, and comparison to our state-of-the-art method.},
  doi      = {10.1038/sdata.2017.117},
  pmid     = {28872634},
  url      = {http://www.nature.com/articles/sdata2017117},
}

@Article{Mascalchi2018,
  author   = {Mascalchi, Mario and Marzi, Chiara and Giannelli, Marco and Ciulli, Stefano and Bianchi, Andrea and Ginestroni, Andrea and Tessa, Carlo and Nicolai, Emanuele and Aiello, Marco and Salvatore, Elena and Soricelli, Andrea and Diciotti, Stefano},
  journal  = {PLoS ONE},
  title    = {{Histogram analysis of dti-derived indices reveals pontocerebellar degeneration and its progression in SCA2}},
  year     = {2018},
  issn     = {19326203},
  month    = {jul},
  number   = {7},
  pages    = {e0200258},
  volume   = {13},
  abstract = {Purpose To assess the potential of histogram metrics of diffusion-tensor imaging (DTI)-derived indices in revealing neurodegeneration and its progression in spinocerebellar ataxia type 2 (SCA2). Materials and methods Nine SCA2 patients and 16 age-matched healthy controls, were examined twice (SCA2 patients 3.6±0.7 years and controls 3.3±1.0 years apart) on the same 1.5T scanner by acquiring T1-weighted and diffusion-weighted (b-value = 1000 s/mm2) images. Cerebrum and brainstem-cerebellum regions were segmented using FreeSurfer suite. Histogram analysis of DTI-derived indices, including mean diffusivity (MD), fractional anisotropy (FA), axial (AD) / radial (RD) diffusivity and mode of anisotropy (MO), was performed. Results At baseline, significant differences between SCA2 patients and controls were confined to brainstem-cerebellum. Median values of MD/AD/RD and FA/MO were significantly (p<0.001) higher and lower, respectively, in SCA2 patients (1.11/1.30/1.03×10−3 mm2/s and 0.14/0.19) than in controls (0.80/1.00/0.70×10−3 mm2/s and 0.20/0.41). Also, peak location values of MD/AD/RD and FA were significantly (p<0.001) higher and lower, respectively, in SCA2 patients (0.91/1.11/0.81×10−3 mm2/s and 0.12) than in controls (0.71/0.91/0.63×10−3 mm2/s and 0.18). Peak height values of FA and MD/AD/RD/MO were significantly (p<0.001) higher and lower, respectively, in SCA2 patients (0.20 and 0.07/0.06/0.07×10−3 mm2/s/year /0.07) than in controls (0.15 and 0.14/0.11/0.12/×10−3 mm2/s/year /0.09). The rate of change of MD median values was significantly (p<0.001) higher (i.e., increased) in SCA2 patients (0.010×10−3 mm2/s/year) than in controls (-0.003×10−3 mm2/s/year) in the brainstem-cerebellum, whereas no significant difference was found for other indices and in the cerebrum. Conclusion Histogram analysis of DTI-derived indices is a relatively straightforward approach which reveals microstructural changes associated with pontocerebellar degeneration in SCA2 and the median value of MD is capable to track its progression.},
  doi      = {10.1371/journal.pone.0200258},
  editor   = {Lenglet, Christophe},
  pmid     = {30001379},
  url      = {https://dx.plos.org/10.1371/journal.pone.0200258},
}

@Article{Bakas2018,
  author        = {Bakas, Spyridon and Reyes, Mauricio and Jakab, Andras and Bauer, Stefan and Rempfler, Markus and Crimi, Alessandro and Shinohara, Russell Takeshi and Berger, Christoph and Ha, Sung Min and Rozycki, Martin and Prastawa, Marcel and Alberts, Esther and Lipkova, Jana and Freymann, John and Kirby, Justin and Bilello, Michel and Fathallah-Shaykh, Hassan M. and Wiest, Roland and Kirschke, Jan and Wiestler, Benedikt and Colen, Rivka and Kotrotsou, Aikaterini and Lamontagne, Pamela and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Weber, Marc Andr and Mahajan, Abhishek and Baid, Ujjwal and Gerstner, Elizabeth and Kwon, Dongjin and Acharya, Gagan and Agarwal, Manu and Alam, Mahbubul and Albiol, Alberto and Albiol, Antonio and Albiol, Francisco J. and Alex, Varghese and Allinson, Nigel and Amorim, Pedro H.A. and Amrutkar, Abhijit and Anand, Ganesh and Andermatt, Simon and Arbel, Tal and Arbelaez, Pablo and Avery, Aaron and Azmat, Muneeza and Pranjal, B. and Bai, Wenjia and Banerjee, Subhashis and Barth, Bill and Batchelder, Thomas and Batmanghelich, Kayhan and Battistella, Enzo and Beers, Andrew and Belyaev, Mikhail and Bendszus, Martin and Benson, Eze and Bernal, Jose and Bharath, Halandur Nagaraja and Biros, George and Bisdas, Sotirios and Brown, James and Cabezas, Mariano and Cao, Shilei and Cardoso, Jorge M. and Carver, Eric N. and Casamitjana, Adri and Castillo, Laura Silvana and Cat, Marcel and Cattin, Philippe and C{\'{e}}rigues, Albert and Chagas, Vinicius S. and Chandra, Siddhartha and Chang, Yi Ju and Chang, Shiyu and Chang, Ken and Chazalon, Joseph and Chen, Shengcong and Chen, Wei and Chen, Jefferson W. and Chen, Zhaolin and Cheng, Kun and Choudhury, Ahana Roy and Chylla, Roger and Clrigues, Albert and Colleman, Steven and Colmeiro, Ramiro German Rodriguez and Combalia, Marc and Costa, Anthony and Cui, Xiaomeng and Dai, Zhenzhen and Dai, Lutao and Daza, Laura Alexandra and Deutsch, Eric and Ding, Changxing and Dong, Chao and Dong, Shidu and Dudzik, Wojciech and Eaton-Rosen, Zach and Egan, Gary and Escudero, Guilherme and Estienne, Tho and Everson, Richard and Fabrizio, Jonathan and Fan, Yong and Fang, Longwei and Feng, Xue and Ferrante, Enzo and Fidon, Lucas and Fischer, Martin and French, Andrew P. and Fridman, Naomi and Fu, Huan and Fuentes, David and Gao, Yaozong and Gates, Evan and Gering, David and Gholami, Amir and Gierke, Willi and Glocker, Ben and Gong, Mingming and Gonzlez-Vill, Sandra and Grosges, T. and Guan, Yuanfang and Guo, Sheng and Gupta, Sudeep and Han, Woo Sup and Han, Il Song and Harmuth, Konstantin and He, Huiguang and Hernndez-Sabat, Aura and Herrmann, Evelyn and Himthani, Naveen and Hsu, Winston and Hsu, Cheyu and Hu, Xiaojun and Hu, Xiaobin and Hu, Yan and Hu, Yifan and Hua, Rui and Huang, Teng Yi and Huang, Weilin and van Huffel, Sabine and Huo, Quan and Vivek, H. V. and Iftekharuddin, Khan M. and Isensee, Fabian and Islam, Mobarakol and Jackson, Aaron S. and Jambawalikar, Sachin R. and Jesson, Andrew and Jian, Weijian and Jin, Peter and {Jeya Maria Jose}, V. and Jungo, Alain and Kainz, Bernhard and Kamnitsas, Konstantinos and Kao, Po Yu and Karnawat, Ayush and Kellermeier, Thomas and Kermi, Adel and Keutzer, Kurt and Khadir, Mohamed Tarek and Khened, Mahendra and Kickingereder, Philipp and Kim, Geena and King, Nik and Knapp, Haley and Knecht, Urspeter and Kohli, Lisa and Kong, Deren and Kong, Xiangmao and Koppers, Simon and Kori, Avinash and Krishnamurthi, Ganapathy and Krivov, Egor and Kumar, Piyush and Kushibar, Kaisar and Lachinov, Dmitrii and Lambrou, Tryphon and Lee, Joon and Lee, Chengen and Lee, Yuehchou and Lee, Matthew Chung Hai and Lefkovits, Szidonia and Lefkovits, Laszlo and Levitt, James and Li, Tengfei and Li, Hongwei and Li, Wenqi and Li, Hongyang and Li, Xiaochuan and Li, Yuexiang and Li, Heng and Li, Zhenye and Li, Xiaoyu and Li, Zeju and Li, Xiao Gang and Li, Wenqi and Lin, Zheng Shen and Lin, Fengming and Lio, Pietro and Liu, Chang and Liu, Boqiang and Liu, Xiang and Liu, Mingyuan and Liu, Ju and Liu, Luyan and Llad{\'{o}}, Xavier and Lopez, Marc Moreno and Lorenzo, Pablo Ribalta and Lu, Zhentai and Luo, Lin and Luo, Zhigang and Ma, Jun and Ma, Kai and Mackie, Thomas and Madabhushi, Anant and Mahmoudi, Issam and Maier-Hein, Klaus H. and Maji, Pradipta and Mammen, C. P. and Mang, Andreas and Manjunath, B. S. and Marcinkiewicz, Michal and McDonagh, Steven and McKenna, Stephen and McKinley, Richard and Mehl, Miriam and Mehta, Sachin and Mehta, Raghav and Meier, Raphael and Meinel, Christoph and Merhof, Dorit and Meyer, Craig and Miller, Robert and Mitra, Sushmita and Moiyadi, Aliasgar and Molina-Garcia, David and Monteiro, Miguel A.B. and Mrukwa, Grzegorz and Myronenko, Andriy and Nalepa, Jakub and Ngo, Thuyen and Nie, Dong and Ning, Holly and Niu, Chen and Nuechterlein, Nicholas K. and Oermann, Eric and Oliveira, Arlindo and Oliveira, Diego D.C. and Oliver, Arnau and Osman, Alexander F.I. and Ou, Yu Nian and Ourselin, Sebastien and Paragios, Nikos and Park, Moo Sung and Paschke, Brad and {Gregory Pauloski}, J. and Pawar, Kamlesh and Pawlowski, Nick and Pei, Linmin and Peng, Suting and Pereira, Silvio M. and Perez-Beteta, Julian and Perez-Garcia, Victor M. and Pezold, Simon and Pham, Bao and Phophalia, Ashish and Piella, Gemma and Pillai, G. N. and Piraud, Marie and Pisov, Maxim and Popli, Anmol and Pound, Michael P. and Pourreza, Reza and Prasanna, Prateek and Pr, Vesnakovska and Pridmore, Tony P. and Puch, Santi and Puybareau, Lodie and Qian, Buyue and Qiao, Xu and Rajchl, Martin and Rane, Swapnil and Rebsamen, Michael and Ren, Hongliang and Ren, Xuhua and Revanuru, Karthik and Rezaei, Mina and Rippel, Oliver and Rivera, Luis Carlos and Robert, Charlotte and Rosen, Bruce and Rueckert, Daniel and Safwan, Mohammed and Salem, Mostafa and Salvi, Joaquim and Sanchez, Irina and Snchez, Irina and Santos, Heitor M. and Sartor, Emmett and Schellingerhout, Dawid and Scheufele, Klaudius and Scott, Matthew R. and Scussel, Artur A. and Sedlar, Sara and Serrano-Rubio, Juan Pablo and {Jon Shah}, N. and Shah, Nameetha and Shaikh, Mazhar and {Uma Shankar}, B. and Shboul, Zeina and Shen, Haipeng and Shen, Dinggang and Shen, Linlin and Shen, Haocheng and Shenoy, Varun and Shi, Feng and Shin, Hyung Eun and Shu, Hai and Sima, Diana and Sinclair, Matthew and Smedby, Orjan and Snyder, James M. and Soltaninejad, Mohammadreza and Song, Guidong and Soni, Mehul and Stawiaski, Jean and Subramanian, Shashank and Sun, Li and Sun, Roger and Sun, Jiawei and Sun, Kay and Sun, Yu and Sun, Guoxia and Sun, Shuang and Suter, Yannick R. and Szilagyi, Laszlo and Talbar, Sanjay and Tao, Dacheng and Tao, Dacheng and Teng, Zhongzhao and Thakur, Siddhesh and Thakur, Meenakshi H. and Tharakan, Sameer and Tiwari, Pallavi and Tochon, Guillaume and Tran, Tuan and Tsai, Yuhsiang M. and Tseng, Kuan Lun and Tuan, Tran Anh and Turlapov, Vadim and Tustison, Nicholas and Vakalopoulou, Maria and Valverde, Sergi and Vanguri, Rami and Vasiliev, Evgeny and Ventura, Jonathan and Vera, Luis and Vercauteren, Tom and Verrastro, C. A. and Vidyaratne, Lasitha and Vilaplana, Veronica and Vivekanandan, Ajeet and Wang, Guotai and Wang, Qian and Wang, Chiatse J. and Wang, Weichung and Wang, Duo and Wang, Ruixuan and Wang, Yuanyuan and Wang, Chunliang and Wang, Guotai and Wen, Ning and Wen, Xin and Weninger, Leon and Wick, Wolfgang and Wu, Shaocheng and Wu, Qiang and Wu, Yihong and Xia, Yong and Xu, Yanwu and Xu, Xiaowen and Xu, Peiyuan and Yang, Tsai Ling and Yang, Xiaoping and Yang, Hao Yu and Yang, Junlin and Yang, Haojin and Yang, Guang and Yao, Hongdou and Ye, Xujiong and Yin, Changchang and Young-Moxon, Brett and Yu, Jinhua and Yue, Xiangyu and Zhang, Songtao and Zhang, Angela and Zhang, Kun and Zhang, Xuejie and Zhang, Lichi and Zhang, Xiaoyue and Zhang, Yazhuo and Zhang, Lei and Zhang, Jianguo and Zhang, Xiang and Zhang, Tianhao and Zhao, Sicheng and Zhao, Yu and Zhao, Xiaomei and Zhao, Liang and Zheng, Yefeng and Zhong, Liming and Zhou, Chenhong and Zhou, Xiaobing and Zhou, Fan and Zhu, Hongtu and Zhu, Jin and Zhuge, Ying and Zong, Weiwei and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Davatzikos, Christos and van Leemput, Koen and Menze, Bjoern},
  journal       = {arXiv},
  title         = {{Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the BRATS challenge}},
  year          = {2018},
  issn          = {23318422},
  month         = {nov},
  abstract      = {Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1811.02629},
  eprint        = {1811.02629},
  keywords      = {BraTS,Brain,Challenge,Glioblastoma,Glioma,Machine learning,Progression,RANO,RECIST,Radiomics,Segmentation,Survival,Tumor},
  url           = {http://arxiv.org/abs/1811.02629},
}

@Article{Madhyastha2015,
  author   = {Madhyastha, Tara M. and Askren, Mary K. and Boord, Peter and Grabowski, Thomas J.},
  journal  = {Brain Connectivity},
  title    = {{Dynamic connectivity at rest predicts attention task performance}},
  year     = {2015},
  issn     = {21580022},
  month    = {feb},
  number   = {1},
  pages    = {45--59},
  volume   = {5},
  abstract = {Consistent spatial patterns of coherent activity, representing large-scale networks, have been reliably identified in multiple populations. Most often, these studies have examined "stationary" connectivity. However, there is a growing recognition that there is a wealth of information in the time-varying dynamics of networks which has neural underpinnings, which changes with age and disease and that supports behavior. Using factor analysis of overlapping sliding windows across 25 participants with Parkinson disease (PD) and 21 controls (ages 41-86), we identify factors describing the covarying correlations of regions (dynamic connectivity) within attention networks and the default mode network, during two baseline resting-state and task runs. Cortical regions that support attention networks are affected early in PD, motivating the potential utility of dynamic connectivity as a sensitive way to characterize physiological disruption to these networks. We show that measures of dynamic connectivity are more reliable than comparable measures of stationary connectivity. Factors in the dorsal attention network (DAN) and fronto-parietal task control network, obtained at rest, are consistently related to the alerting and orienting reaction time effects in the subsequent Attention Network Task. In addition, the same relationship between the same DAN factor and the alerting effect was present during tasks. Although reliable, dynamic connectivity was not invariant, and changes between factor scores across sessions were related to changes in accuracy. In summary, patterns of time-varying correlations among nodes in an intrinsic network have a stability that has functional relevance.},
  doi      = {10.1089/brain.2014.0248},
  keywords = {Parkinson disease,attention network task,dynamic functional connectivity,resting-state connectivity,task connectivity},
  pmid     = {25014419},
  url      = {http://www.liebertpub.com/doi/10.1089/brain.2014.0248},
}

@Article{Isgum2015,
  author   = {I{\v{s}}gum, Ivana and Benders, Manon J.N.L. and Avants, Brian and Cardoso, M. Jorge and Counsell, Serena J. and Gomez, Elda Fischi and Gui, Laura and Huppi, Petra S. and Kersbergen, Karina J. and Makropoulos, Antonios and Melbourne, Andrew and Moeskops, Pim and Mol, Christian P. and Kuklisova-Murgasova, Maria and Rueckert, Daniel and Schnabel, Julia A. and Srhoj-Egekher, Vedran and Wu, Jue and Wang, Siying and de Vries, Linda S. and Viergever, Max A.},
  journal  = {Medical Image Analysis},
  title    = {{Evaluation of automatic neonatal brain segmentation algorithms: The NeoBrainS12 challenge}},
  year     = {2015},
  issn     = {13618423},
  month    = {feb},
  number   = {1},
  pages    = {135--151},
  volume   = {20},
  abstract = {A number of algorithms for brain segmentation in preterm born infants have been published, but a reliable comparison of their performance is lacking. The NeoBrainS12 study (http://neobrains12.isi.uu.nl), providing three different image sets of preterm born infants, was set up to provide such a comparison. These sets are (i) axial scans acquired at 40. weeks corrected age, (ii) coronal scans acquired at 30. weeks corrected age and (iii) coronal scans acquired at 40. weeks corrected age. Each of these three sets consists of three T1- and T2-weighted MR images of the brain acquired with a 3T MRI scanner. The task was to segment cortical grey matter, non-myelinated and myelinated white matter, brainstem, basal ganglia and thalami, cerebellum, and cerebrospinal fluid in the ventricles and in the extracerebral space separately. Any team could upload the results and all segmentations were evaluated in the same way. This paper presents the results of eight participating teams. The results demonstrate that the participating methods were able to segment all tissue classes well, except myelinated white matter.},
  doi      = {10.1016/j.media.2014.11.001},
  keywords = {Brain segmentation,MRI,Neonatal brain,Segmentation comparison,Segmentation evaluation},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841514001583},
}

@Article{Hssayeni2020,
  author        = {Hssayeni, Murtadha D. and Croock, Muayad S. and Salman, Aymen D. and Al-Khafaji, Hassan Falah and Yahya, Zakaria A. and Ghoraani, Behnaz},
  journal       = {Data},
  title         = {{Intracranial hemorrhage segmentation using a deep convolutional model}},
  year          = {2020},
  issn          = {23065729},
  month         = {feb},
  number        = {1},
  pages         = {14},
  volume        = {5},
  abstract      = {Traumatic brain injuries may cause intracranial hemorrhages (ICH). ICH could lead to disability or death if it is not accurately diagnosed and treated in a time-sensitive procedure. The current clinical protocol to diagnose ICH is examining Computerized Tomography (CT) scans by radiologists to detect ICH and localize its regions. However, this process relies heavily on the availability of an experienced radiologist. In this paper, we designed a study protocol to collect a dataset of 82 CT scans of subjects with a traumatic brain injury. Next, the ICH regions were manually delineated in each slice by a consensus decision of two radiologists. The dataset is publicly available online at the PhysioNet repository for future analysis and comparisons. In addition to publishing the dataset, which is the main purpose of this manuscript, we implemented a deep Fully Convolutional Networks (FCNs), known as U-Net, to segment the ICH regions from the CT scans in a fully-automated manner. The method as a proof of concept achieved a Dice coefficient of 0.31 for the ICH segmentation based on 5-fold cross-validation.},
  archiveprefix = {arXiv},
  arxivid       = {1910.08643},
  doi           = {10.3390/data5010014},
  eprint        = {1910.08643},
  keywords      = {CT scans dataset,Fully convolutional network,ICH detection,Intracranial hemorrhage segmentation,U-Net},
  url           = {https://www.mdpi.com/2306-5729/5/1/14},
}

@Article{Goldberger2000,
  author   = {Goldberger, A. L. and Amaral, L. A. and Glass, L. and Hausdorff, J. M. and Ivanov, P. C. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C. K. and Stanley, H. E.},
  journal  = {Circulation},
  title    = {{PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals.}},
  year     = {2000},
  issn     = {15244539},
  month    = {jun},
  number   = {23},
  volume   = {101},
  abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet. org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
  doi      = {10.1161/01.cir.101.23.e215},
  pmid     = {10851218},
  url      = {https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215},
}

@Article{art/KaurA_201501,
  author   = {Kaur, Amandeep and Singh, Chandan},
  journal  = {Signal, Image and Video Processing},
  title    = {{Automatic cephalometric landmark detection using Zernike moments and template matching}},
  year     = {2015},
  issn     = {18631711},
  month    = {jan},
  number   = {1},
  pages    = {117--132},
  volume   = {9},
  abstract = {Cephalometry is an essential clinical and research tool in orthodontics. It has been used for decades to obtain absolute and relative measures of the craniofacial skeleton. Since manual identification of predefined anatomical landmarks is a very tedious approach, there is a strong need for automated methods. This paper explores the use of Zernike moment-based global features for initial landmark estimation and computing small expectation window for each landmark. Using this expectation window and local template matching based on ring and central projection method, a closer approximation of landmark position is obtained. A smaller search window based on this approximation is used to find the exact location of landmark positions based on template matching using a combination of sum of squared distance and normalized cross-correlation. The system was tested on 18 commonly used landmarks using a dataset of 85 randomly selected cephalograms. A total of 89.5 % of the localization of 18 selected landmarks are within a window of $$\le \!\!\pm 2\text{ mm}$$. The average mean error for the 18 landmarks is 1.84 mm and average SD of mean error is 1.24.},
  doi      = {10.1007/s11760-013-0432-7},
  keywords = {Central projections,Cephalometry,Landmarks,Template matching,Zernike moments},
  url      = {http://link.springer.com/10.1007/s11760-013-0432-7},
}

@Article{Mendrik2015,
  author   = {Mendrik, Adri{\"{e}}nne M. and Vincken, Koen L. and Kuijf, Hugo J. and Breeuwer, Marcel and Bouvy, Willem H. and {De Bresser}, Jeroen and Alansary, Amir and {De Bruijne}, Marleen and Carass, Aaron and El-Baz, Ayman and Jog, Amod and Katyal, Ranveer and Khan, Ali R. and {Van Der Lijn}, Fedde and Mahmood, Qaiser and Mukherjee, Ryan and {Van Opbroek}, Annegreet and Paneri, Sahil and Pereira, S{\'{e}}rgio and Persson, Mikael and Rajchl, Martin and Sarikaya, Duygu and Smedby, {\"{O}}rjan and Silva, Carlos A. and Vrooman, Henri A. and Vyas, Saurabh and Wang, Chunliang and Zhao, Liang and Biessels, Geert Jan and Viergever, Max A.},
  journal  = {Computational Intelligence and Neuroscience},
  title    = {{MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans}},
  year     = {2015},
  issn     = {16875273},
  pages    = {1--16},
  volume   = {2015},
  abstract = {Many methods have been proposed for tissue segmentation in brain MRI scans. The multitude of methods proposed complicates the choice of one method above others. We have therefore established the MRBrainS online evaluation framework for evaluating (semi)automatic algorithms that segment gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) on 3T brain MRI scans of elderly subjects (65-80 y). Participants apply their algorithms to the provided data, after which their results are evaluated and ranked. Full manual segmentations of GM, WM, and CSF are available for all scans and used as the reference standard. Five datasets are provided for training and fifteen for testing. The evaluated methods are ranked based on their overall performance to segment GM, WM, and CSF and evaluated using three evaluation metrics (Dice, H95, and AVD) and the results are published on the MRBrainS13 website. We present the results of eleven segmentation algorithms that participated in the MRBrainS13 challenge workshop at MICCAI, where the framework was launched, and three commonly used freeware packages: FreeSurfer, FSL, and SPM. The MRBrainS evaluation framework provides an objective and direct comparison of all evaluated algorithms and can aid in selecting the best performing method for the segmentation goal at hand.},
  doi      = {10.1155/2015/813696},
  pmid     = {26759553},
  url      = {http://www.hindawi.com/journals/cin/2015/813696/},
}

@Article{Klein2012,
  author   = {Klein, Arno and Tourville, Jason},
  journal  = {Frontiers in Neuroscience},
  title    = {{101 Labeled Brain Images and a Consistent Human Cortical Labeling Protocol}},
  year     = {2012},
  issn     = {16624548},
  number   = {DEC},
  volume   = {6},
  abstract = {We introduce the Mindboggle-101 dataset, the largest and most complete set of free, publicly accessible, manually labeled human brain images. To manually label the macroscopic anatomy in magnetic resonance images of 101 healthy participants, we created a new cortical labeling protocol that relies on robust anatomical landmarks and minimal manual edits after initialization with automated labels. The "Desikan-Killiany-Tourville" (DKT) protocol is intended to improve the ease, consistency, and accuracy of labeling human cortical areas. Given how difficult it is to label brains, the Mindboggle-101 dataset is intended to serve as brain atlases for use in labeling other brains, as a normative dataset to establish mor-phometric variation in a healthy population for comparison against clinical populations, and contribute to the development, training, testing, and evaluation of automated registration and labeling algorithms. To this end, we also introduce benchmarks for the evaluation of such algorithms by comparing our manual labels with labels automatically generated by probabilistic and multi-atlas registration-based approaches. All data and related software and updated information are available on the http://mindboggle.info/data website. Copy; 2012 Kleinand Tourville.},
  doi      = {10.3389/fnins.2012.00171},
  keywords = {Anatomy,Cerebral cortex,Human brain,Labeling,Mri,Parcellation,Segmentation},
  url      = {http://journal.frontiersin.org/article/10.3389/fnins.2012.00171/abstract},
}

@Article{Yue2006,
  author   = {Yue, Weining and Yin, Dali and Li, Chengjun and Wang, Guoping and Xu, Tianmin},
  journal  = {IEEE Transactions on Biomedical Engineering},
  title    = {{Automated 2-D cephalometric analysis on X-ray images by a model-based approach}},
  year     = {2006},
  issn     = {00189294},
  month    = {aug},
  number   = {8},
  pages    = {1615--1623},
  volume   = {53},
  abstract = {Craniofacial landmark localization and anatomical structure tracing on cephalograms are two important ways to obtain the cephalometric analysis. In order to computerize them in parallel, a model-based approach is proposed to locate 262 craniofacial feature points, including 90 landmarks and 172 auxiliary points. In model training, 12 landmarks are selected as reference points and used to divide every training shape to 10 regions according to the anatomical knowledge; principle components analysis is employed to characterize the region shape variations and the statistical grey profile of every feature point. Locating feature points on an input image is a two-stage procedure. First, we identify the reference landmarks by image processing and pattern matching techniques, so that the shape partition is performed on the input image. Then, for each region, its feature points are located by a modified active shape model. All craniofacial anatomical structures can be traced out by connecting the located points with subdivision curves according to the prior knowledge. Users are permitted to modify the results interactively in many different ways. Experimental results show the advantage and reliability of the proposed method. {\textcopyright} 2006 IEEE.},
  doi      = {10.1109/TBME.2006.876638},
  keywords = {ASM,Cephalometry,Edge detection,Landmarks,PCA,Pattern matching,Structure tracing,Superimposition},
  pmid     = {16916096},
  url      = {http://ieeexplore.ieee.org/document/1658156/},
}

@InCollection{Kafieh2008,
  author    = {Kafieh, Rahele and Sadri, Saeed and Mehri, Alireza and Raji, Hamid},
  booktitle = {Communications in Computer and Information Science},
  title     = {{Discrimination of bony structures in cephalograms for automatic landmark detection}},
  year      = {2008},
  isbn      = {3540899847},
  pages     = {609--620},
  volume    = {6 CCIS},
  abstract  = {This paper introduces a new method for automatic landmark detection in cephalometry. In first step, some feature points of bony structures are extracted to model the size, rotation, and translation of skull, we propose two different methods for bony structure discrimination in cephalograms. The first method is using bit slices of a gray level image to create a layered version of the same image and the second method is to make use of a SUSAN edge detector and discriminate the pixels with enough thickness as bony structures. Then a neural network is used to classify images according to their geometrical specifications. Using NN for every new image, the possible coordinates of landmarks are estimated. Then a modified ASM is applied to locate the exact location of landmarks.On average the first method can discriminate feature points of bony structures in 78% of cephalograms and the second method can do it in 94% of them. {\textcopyright} 2008 Springer-Verlag.},
  doi       = {10.1007/978-3-540-89985-3_75},
  issn      = {18650929},
  keywords  = {Active Shape Model,Bit slicing,Learning Vector Quantization,Susan edge detector,cephalometry},
  url       = {http://link.springer.com/10.1007/978-3-540-89985-3_75},
}

@Article{Boord2017,
  author   = {Boord, Peter and Madhyastha, Tara M. and Askren, Mary K. and Grabowski, Thomas J.},
  journal  = {NeuroImage: Clinical},
  title    = {{Executive attention networks show altered relationship with default mode network in PD}},
  year     = {2017},
  issn     = {22131582},
  pages    = {1--8},
  volume   = {13},
  abstract = {Attention dysfunction is a common but often undiagnosed cognitive impairment in Parkinson's disease that significantly reduces quality of life. We sought to increase understanding of the mechanisms underlying attention dysfunction using functional neuroimaging. Functional MRI was acquired at two repeated sessions in the resting state and during the Attention Network Test, for 25 non-demented subjects with Parkinson's disease and 21 healthy controls. Behavioral and MRI contrasts were calculated for alerting, orienting, and executive control components of attention. Brain regions showing group differences in attention processing were used as seeds in a functional connectivity analysis of a separate resting state run. Parkinson's disease subjects showed more activation during increased executive challenge in four regions of the dorsal attention and frontoparietal networks, namely right frontal eye field, left and right intraparietal sulcus, and precuneus. In three regions we saw reduced resting state connectivity to the default mode network. Further, whereas higher task activation in the right intraparietal sulcus correlated with reduced resting state connectivity between right intraparietal sulcus and the precuneus in healthy controls, this relationship was absent in Parkinson's disease subjects. Our results suggest that a weakened interaction between the default mode and task positive networks might alter the way in which the executive response is processed in PD.},
  doi      = {10.1016/j.nicl.2016.11.004},
  keywords = {Attention network test,Default mode network,Executive attention,Functional connectivity,Parkinson's disease},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302108},
}

@Article{Liew2018,
  author   = {Liew, Sook Lei and Anglin, Julia M. and Banks, Nick W. and Sondag, Matt and Ito, Kaori L. and Kim, Hosung and Chan, Jennifer and Ito, Joyce and Jung, Connie and Khoshab, Nima and Lefebvre, Stephanie and Nakamura, William and Saldana, David and Schmiesing, Allie and Tran, Cathy and Vo, Danny and Ard, Tyler and Heydari, Panthea and Kim, Bokkyu and Aziz-Zadeh, Lisa and Cramer, Steven C. and Liu, Jingchun and Soekadar, Surjo and Nordvik, Jan Egil and Westlye, Lars T. and Wang, Junping and Winstein, Carolee and Yu, Chunshui and Ai, Lei and Koo, Bonhwang and Craddock, R. Cameron and Milham, Michael and Lakich, Matthew and Pienta, Amy and Stroud, Alison},
  journal  = {Scientific Data},
  title    = {{A large, open source dataset of stroke anatomical brain images and manual lesion segmentations}},
  year     = {2018},
  issn     = {20524463},
  month    = {dec},
  number   = {1},
  pages    = {180011},
  volume   = {5},
  abstract = {Stroke is the leading cause of adult disability worldwide, with up to two-thirds of individuals experiencing long-term disabilities. Large-scale neuroimaging studies have shown promise in identifying robust biomarkers (e.g., measures of brain structure) of long-term stroke recovery following rehabilitation. However, analyzing large rehabilitation-related datasets is problematic due to barriers in accurate stroke lesion segmentation. Manually-traced lesions are currently the gold standard for lesion segmentation on T1-weighted MRIs, but are labor intensive and require anatomical expertise. While algorithms have been developed to automate this process, the results often lack accuracy. Newer algorithms that employ machine-learning techniques are promising, yet these require large training datasets to optimize performance. Here we present ATLAS (Anatomical Tracings of Lesions After Stroke), an open-source dataset of 304 T1-weighted MRIs with manually segmented lesions and metadata. This large, diverse dataset can be used to train and test lesion segmentation algorithms and provides a standardized dataset for comparing the performance of different segmentation methods. We hope ATLAS release 1.1 will be a useful resource to assess and improve the accuracy of current lesion segmentation methods.},
  doi      = {10.1038/sdata.2018.11},
  pmid     = {29461514},
  url      = {http://www.nature.com/articles/sdata201811},
}

@Article{VanRullen2019,
  author        = {VanRullen, Rufin and Reddy, Leila},
  journal       = {Communications Biology},
  title         = {{Reconstructing faces from fMRI patterns using deep generative neural networks}},
  year          = {2019},
  issn          = {23993642},
  month         = {oct},
  number        = {1},
  volume        = {2},
  abstract      = {Although distinct categories are reliably decoded from fMRI brain responses, it has proved more difficult to distinguish visually similar inputs, such as different faces. Here, we apply a recently developed deep learning system to reconstruct face images from human fMRI. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised procedure over a large data set of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand faces to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, translating fMRI patterns into VAE latent codes, and codes into face reconstructions. The system not only performed robust pairwise decoding (>95% correct), but also accurate gender classification, and even decoded which face was imagined, rather than seen.},
  archiveprefix = {arXiv},
  arxivid       = {1810.03856},
  doi           = {10.1038/s42003-019-0438-y},
  eprint        = {1810.03856},
  pmid          = {31925027},
  url           = {http://arxiv.org/abs/1810.03856},
}

@Article{Neher2014,
  author   = {Neher, Peter F. and Laun, Frederik B. and Stieltjes, Bram and Maier-Hein, Klaus H.},
  journal  = {Magnetic Resonance in Medicine},
  title    = {{Fiberfox: Facilitating the creation of realistic white matter software phantoms}},
  year     = {2014},
  issn     = {15222594},
  month    = {nov},
  number   = {5},
  pages    = {1460--1470},
  volume   = {72},
  abstract = {Purpose: Phantom-based validation of diffusion-weighted image processing techniques is an important key to innovation in the field and is widely used. Openly available and user friendly tools for the flexible generation of tailor-made datasets for the specific tasks at hand can greatly facilitate the work of researchers around the world.
Methods: We present an open-source framework, Fiberfox, that enables (1) the intuitive definition of arbitrary artificial white matter fiber tracts, (2) signal generation from those fibers by means of the most recent multi-compartment modeling techniques, and (3) simulation of the actual MR acquisition that allows for the introduction of realistic MRI-related effects into the final image.
Results: We show that real acquisitions can be closely approximated by simulating the acquisition of the well-known FiberCup phantom. We further demonstrate the advantages of our framework by evaluating the effects of imaging artifacts and acquisition settings on the outcome of 12 tractography algorithms.
Conclusion: Our findings suggest that experiments on a realistic software phantom might change the conclusions drawn from earlier hardware phantom experiments. Fiberfox may find application in validating and further developing methods such as tractography, super-resolution, diffusion modeling or artifact correction.},
  doi      = {10.1002/mrm.25045},
  keywords = {Artifact simulation,Diffusion-weighted imaging,Open-source software,S synthetic white matter fibers,Software phantoms},
  pmid     = {24323973},
  url      = {http://www.ncbi.nlm.nih.gov/pubmed/24323973},
}

@Article{Cote2013,
  author   = {C{\^{o}}t{\'{e}}, Marc Alexandre and Girard, Gabriel and Bor{\'{e}}, Arnaud and Garyfallidis, Eleftherios and Houde, Jean Christophe and Descoteaux, Maxime},
  journal  = {Medical Image Analysis},
  title    = {{Tractometer: Towards validation of tractography pipelines}},
  year     = {2013},
  issn     = {13618415},
  month    = {oct},
  number   = {7},
  pages    = {844--857},
  volume   = {17},
  abstract = {We have developed the Tractometer: an online evaluation and validation system for tractography processing pipelines. One can now evaluate the results of more than 57,000 fiber tracking outputs using different acquisition settings (b-value, averaging), different local estimation techniques (tensor, q-ball, spherical deconvolution) and different tracking parameters (masking, seeding, maximum curvature, step size). At this stage, the system is solely based on a revised FiberCup analysis, but we hope that the community will get involved and provide us with new phantoms, new algorithms, third party libraries and new geometrical metrics, to name a few. We believe that the new connectivity analysis and tractography characteristics proposed can highlight limits of the algorithms and contribute in solving open questions in fiber tracking: from raw data to connectivity analysis. Overall, we show that (i) averaging improves quality of tractography, (ii) sharp angular ODF profiles helps tractography, (iii) seeding and multi-seeding has a large impact on tractography outputs and must be used with care, and (iv) deterministic tractography produces less invalid tracts which leads to better connectivity results than probabilistic tractography. {\textcopyright} 2013 Elsevier B.V.},
  doi      = {10.1016/j.media.2013.03.009},
  keywords = {Connectivity analysis,Diffusion MRI,Tractography,Validation},
  pmid     = {23706753},
  url      = {http://www.ncbi.nlm.nih.gov/pubmed/23706753},
}

@Article{Wang2019,
  author   = {Wang, Li and Nie, Dong and Li, Guannan and Puybareau, {\'{E}}lodie and Dolz, Jose and Zhang, Qian and Wang, Fan and Xia, Jing and Wu, Zhengwang and Chen, Jia Wei and Thung, Kim Han and Bui, Toan Duc and Shin, Jitae and Zeng, Guodong and Zheng, Guoyan and Fonov, Vladimir S. and Doyle, Andrew and Xu, Yongchao and Moeskops, Pim and Pluim, Josien P.W. and Desrosiers, Christian and Ayed, Ismail Ben and Sanroma, Gerard and Benkarim, Oualid M. and Casamitjana, Adri{\`{a}} and Vilaplana, Ver{\'{o}}nica and Lin, Weili and Li, Gang and Shen, Dinggang},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Benchmark on automatic six-month-old infant brain segmentation algorithms: The iSeg-2017 challenge}},
  year     = {2019},
  issn     = {1558254X},
  month    = {sep},
  number   = {9},
  pages    = {2219--2230},
  volume   = {38},
  abstract = {Accurate segmentation of infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid is an indispensable foundation for early studying of brain growth patterns and morphological changes in neurodevelopmental disorders. Nevertheless, in the isointense phase (approximately 6-9months of age), due to inherentmyelination andmaturation process, WM and GM exhibit similar levels of intensity in both T1-weighted and T2-weighted MR images, making tissue segmentation very challenging. Although many efforts were devoted to brain segmentation, only a few studies have focused on the segmentation of six-month infant brain images. With the idea of boosting methodological development in the community, iSeg-2017 challenge (http://iseg2017.web.unc.edu) provides a set of six-month infant subjects with manual labels for training and testing the participating methods. Among the 21 automatic segmentation methods participating in iSeg-2017, we review the eight top-ranked teams, in terms of Dice ratio, modified Hausdorff distance, and average surface distance, and introduce their pipelines, implementations, as well as source codes. We further discuss the limitations and possible future directions. We hope the dataset in iSeg-2017, and this paper could provide insights into methodological development for the community.},
  doi      = {10.1109/TMI.2019.2901712},
  keywords = {Brain,Challenge,Infant,Isointense phase,Segmentation},
  pmid     = {30835215},
  url      = {https://ieeexplore.ieee.org/document/8654000/},
}

@Article{Alkhasli2019,
  author   = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M. and Binkofski, Ferdinand},
  journal  = {Frontiers in Human Neuroscience},
  title    = {{Modulation of fronto-striatal functional connectivity using transcranial magnetic stimulation}},
  year     = {2019},
  issn     = {16625161},
  month    = {jun},
  volume   = {13},
  abstract = {Background: The fronto-striatal network is involved in various motor, cognitive, and emotional processes, such as spatial attention, working memory, decision-making, and emotion regulation. Intermittent theta burst transcranial magnetic stimulation (iTBS) has been shown to modulate functional connectivity of brain networks. Long stimulation intervals, as well as high stimulation intensities are typically applied in transcranial magnetic stimulation (TMS) therapy for mood disorders. The role of stimulation intensity on network function and homeostasis has not been explored systematically yet. Objective: In this pilot study, we aimed to modulate fronto-striatal connectivity by applying iTBS at different intensities to the left dorso-lateral prefrontal cortex (DLPFC). We measured individual and group changes by comparing resting state functional magnetic resonance imaging (rsfMRI) both pre-iTBS and post-iTBS. Differential effects of individual sub- vs. supra-resting motor-threshold stimulation intensities were assessed. Methods: Sixteen healthy subjects underwent excitatory iTBS at two intensities [90% and 120% of individual resting motor threshold (rMT)] on separate days. Six-hundred pulses (2 s trains, 8 s pauses, duration of 3 min, 20 s) were applied over the left DLPFC. Directly before and 7 min after stimulation, task-free rsfMRI sessions, lasting 10 min each, were conducted. Individual seed-to-seed functional connectivity changes were calculated for 10 fronto-striatal and amygdala regions of interest with the SPM toolbox DPABI. Results: Sub-threshold-iTBS increased functional connectivity directly between the left DLPFC and the left and right caudate, respectively. Supra-threshold stimulation did not change fronto-striatal functional connectivity but increased functional connectivity between the right amygdala and the right caudate. Conclusion: A short iTBS protocol applied at sub-threshold intensities was not only sufficient, but favorable, in order to increase bilateral fronto-striatal functional connectivity, while minimizing side effects. The absence of an increase in functional connectivity after supra-threshold stimulation was possibly caused by network homeostatic effects.},
  doi      = {10.3389/fnhum.2019.00190},
  keywords = {DLPFC,Fronto-striatal network,Functional connectivity,Intermittent theta burst stimulation (iTBS),Prefrontal cortex,Resting state,Striatum},
  url      = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00190/full},
}

@Misc{Schmainda2018,
  author    = {Schmainda, K.M. and Prah, M.},
  title     = {{Brain-Tumor-Progression}},
  year      = {2018},
  abstract  = {This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images). The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent. All of the series are co-registered with the T1+C images. The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.},
  booktitle = {Cancer Imaging Archive},
  doi       = {10.7937/K9/TCIA.2018.15quzvnb},
  url       = {http://doi.org/10.7937/K9/TCIA.2018.15quzvnb},
}

@Article{Bontempi2020,
  author        = {Bontempi, Dennis and Benini, Sergio and Signoroni, Alberto and Svanera, Michele and Muckli, Lars},
  journal       = {Medical Image Analysis},
  title         = {{CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI}},
  year          = {2020},
  issn          = {13618423},
  month         = {sep},
  volume        = {62},
  abstract      = {Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Alternatively, recent methods deploying solutions based on Convolutional Neural Networks (CNNs) are enabling the direct analysis of out-of-the-scanner data. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This process entails a loss of global contextual information, thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed of 947 out-of-the-scanner (3 Tesla T1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only a few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. Moreover, through a randomised survey involving expert neuroscientists, we show that subjective judgements favour our solution with respect to widely adopted atlas-based software.},
  archiveprefix = {arXiv},
  arxivid       = {1909.05085},
  doi           = {10.1016/j.media.2020.101688},
  eprint        = {1909.05085},
  keywords      = {3D Image analysis,Brain MRI segmentation,Convolutional neural networks,Weakly supervised learning},
  pmid          = {32272345},
  url           = {http://arxiv.org/abs/1909.05085},
}

@Article{Ning2015,
  author   = {Ning, Lipeng and Laun, Frederik and Gur, Yaniv and DiBella, Edward V.R. and Deslauriers-Gauthier, Samuel and Megherbi, Thinhinane and Ghosh, Aurobrata and Zucchelli, Mauro and Menegaz, Gloria and Fick, Rutger and St-Jean, Samuel and Paquette, Michael and Aranda, Ramon and Descoteaux, Maxime and Deriche, Rachid and O'Donnell, Lauren and Rathi, Yogesh},
  journal  = {Medical Image Analysis},
  title    = {{Sparse Reconstruction Challenge for diffusion MRI: Validation on a physical phantom to determine which acquisition scheme and analysis method to use?}},
  year     = {2015},
  issn     = {13618423},
  month    = {dec},
  number   = {1},
  pages    = {316--331},
  volume   = {26},
  abstract = {Diffusion magnetic resonance imaging (dMRI) is the modality of choice for investigating in-vivo white matter connectivity and neural tissue architecture of the brain. The diffusion-weighted signal in dMRI reflects the diffusivity of water molecules in brain tissue and can be utilized to produce image-based biomarkers for clinical research. Due to the constraints on scanning time, a limited number of measurements can be acquired within a clinically feasible scan time. In order to reconstruct the dMRI signal from a discrete set of measurements, a large number of algorithms have been proposed in recent years in conjunction with varying sampling schemes, i.e., with varying b-values and gradient directions. Thus, it is imperative to compare the performance of these reconstruction methods on a single data set to provide appropriate guidelines to neuroscientists on making an informed decision while designing their acquisition protocols. For this purpose, the SPArse Reconstruction Challenge (SPARC) was held along with the workshop on Computational Diffusion MRI (at MICCAI 2014) to validate the performance of multiple reconstruction methods using data acquired from a physical phantom. A total of 16 reconstruction algorithms (9 teams) participated in this community challenge. The goal was to reconstruct single b-value and/or multiple b-value data from a sparse set of measurements. In particular, the aim was to determine an appropriate acquisition protocol (in terms of the number of measurements, b-values) and the analysis method to use for a neuroimaging study. The challenge did not delve on the accuracy of these methods in estimating model specific measures such as fractional anisotropy (FA) or mean diffusivity, but on the accuracy of these methods to fit the data. This paper presents several quantitative results pertaining to each reconstruction algorithm. The conclusions in this paper provide a valuable guideline for choosing a suitable algorithm and the corresponding data-sampling scheme for clinical neuroscience applications.},
  doi      = {10.1016/j.media.2015.10.012},
  keywords = {Angular error,Diffusion MRI,Normalized mean square error,Physical phantom},
  pmid     = {26606457},
  url      = {https://linkinghub.elsevier.com/retrieve/pii/S1361841515001541},
}

@Article{Litjens2016,
  author   = {Litjens, Geert and S{\'{a}}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen-Van De Kaa}, Christina and Bult, Peter and {Van Ginneken}, Bram and {Van Der Laak}, Jeroen},
  journal  = {Scientific Reports},
  title    = {{Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis}},
  year     = {2016},
  issn     = {20452322},
  month    = {sep},
  number   = {1},
  pages    = {26286},
  volume   = {6},
  abstract = {Pathologists face a substantial increase in workload and complexity of histopathologic cancer diagnosis due to the advent of personalized medicine. Therefore, diagnostic protocols have to focus equally on efficiency and accuracy. In this paper we introduce 'deep learning' as a technique to improve the objectivity and efficiency of histopathologic slide analysis. Through two examples, prostate cancer identification in biopsy specimens and breast cancer metastasis detection in sentinel lymph nodes, we show the potential of this new methodology to reduce the workload for pathologists, while at the same time increasing objectivity of diagnoses. We found that all slides containing prostate cancer and micro- and macro-metastases of breast cancer could be identified automatically while 30-40% of the slides containing benign and normal tissue could be excluded without the use of any additional immunohistochemical markers or human intervention. We conclude that 'deep learning' holds great promise to improve the efficacy of prostate cancer diagnosis and breast cancer staging.},
  doi      = {10.1038/srep26286},
  pmid     = {27212078},
  url      = {http://www.nature.com/articles/srep26286},
}

@Article{Hindy2020,
  author        = {Hindy, Hanan and Brosset, David and Bayne, Ethan and Seeam, Amar Kumar and Tachtatzis, Christos and Atkinson, Robert and Bellekens, Xavier},
  journal       = {IEEE Access},
  title         = {{A Taxonomy of Network Threats and the Effect of Current Datasets on Intrusion Detection Systems}},
  year          = {2020},
  issn          = {21693536},
  month         = {jun},
  pages         = {104650--104675},
  volume        = {8},
  abstract      = {As the world moves towards being increasingly dependent on computers and automation, building secure applications, systems and networks are some of the main challenges faced in the current decade. The number of threats that individuals and businesses face is rising exponentially due to the increasing complexity of networks and services of modern networks. To alleviate the impact of these threats, researchers have proposed numerous solutions for anomaly detection; however, current tools often fail to adapt to ever-changing architectures, associated threats and zero-day attacks. This manuscript aims to pinpoint research gaps and shortcomings of current datasets, their impact on building Network Intrusion Detection Systems (NIDS) and the growing number of sophisticated threats. To this end, this manuscript provides researchers with two key pieces of information; a survey of prominent datasets, analyzing their use and impact on the development of the past decade's Intrusion Detection Systems (IDS) and a taxonomy of network threats and associated tools to carry out these attacks. The manuscript highlights that current IDS research covers only 33.3% of our threat taxonomy. Current datasets demonstrate a clear lack of real-network threats, attack representation and include a large number of deprecated threats, which together limit the detection accuracy of current machine learning IDS approaches. The unique combination of the taxonomy and the analysis of the datasets provided in this manuscript aims to improve the creation of datasets and the collection of real-world data. As a result, this will improve the efficiency of the next generation IDS and reflect network threats more accurately within new datasets.},
  archiveprefix = {arXiv},
  arxivid       = {1806.03517},
  doi           = {10.1109/ACCESS.2020.3000179},
  eprint        = {1806.03517},
  keywords      = {Anomaly detection,datasets,intrusion detection systems,network attacks,network security,security threats,survey,taxonomy},
  url           = {http://arxiv.org/abs/1806.03517},
}

@Article{Corneanu2016,
  author        = {Corneanu, Ciprian Adrian and Sim{\'{o}}n, Marc Oliu and Cohn, Jeffrey F. and Guerrero, Sergio Escalera},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications}},
  year          = {2016},
  issn          = {01628828},
  month         = {aug},
  number        = {8},
  pages         = {1548--1568},
  volume        = {38},
  abstract      = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
  archiveprefix = {arXiv},
  arxivid       = {1606.03237},
  doi           = {10.1109/TPAMI.2016.2515606},
  eprint        = {1606.03237},
  keywords      = {3D,Facial expression,RGB,affect,emotion recognition,multimodal,thermal},
  pmid          = {26761193},
  publisher     = {IEEE Computer Society},
}

@Article{Wu2017,
  author        = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
  journal       = {Computer Vision and Image Understanding},
  title         = {{Visual question answering: A survey of methods and datasets}},
  year          = {2017},
  issn          = {1090235X},
  month         = {jul},
  pages         = {21--40},
  volume        = {163},
  abstract      = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
  archiveprefix = {arXiv},
  arxivid       = {1607.05910},
  doi           = {10.1016/j.cviu.2017.05.001},
  eprint        = {1607.05910},
  keywords      = {Knowledge bases,Natural language processing,Recurrent neural networks,Visual question answering},
  url           = {http://arxiv.org/abs/1607.05910},
}

@Article{Chapman2020,
  author        = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib{\'{a}}{\~{n}}ez, Luis Daniel and Kacprzak, Emilia and Groth, Paul},
  journal       = {VLDB Journal},
  title         = {{Dataset search: a survey}},
  year          = {2020},
  issn          = {0949877X},
  month         = {jan},
  number        = {1},
  pages         = {251--272},
  volume        = {29},
  abstract      = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
  archiveprefix = {arXiv},
  arxivid       = {1901.00735},
  doi           = {10.1007/s00778-019-00564-x},
  eprint        = {1901.00735},
  keywords      = {Dataset,Dataset retrieval,Dataset search,Information search and retrieval},
  url           = {http://arxiv.org/abs/1901.00735},
}

@Article{Mogadala2019,
  author        = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
  journal       = {arXiv},
  title         = {{Trends in integration of vision and language research: A survey of tasks, datasets, and methods}},
  year          = {2019},
  issn          = {23318422},
  month         = {jul},
  abstract      = {Integration of vision and language tasks has seen a significant growth in the recent times due to surge of interest from multi-disciplinary communities such as deep learning, computer vision, and natural language processing. In this survey, we focus on ten different vision and language integration tasks in terms of their problem formulation, methods, existing datasets, evaluation measures, and comparison of results achieved with the corresponding state-of-the-art methods. This goes beyond earlier surveys which are either task-specific or concentrate only on one type of visual content i.e., image or video. We then conclude the survey by discussing some possible future directions for integration of vision and language research.},
  archiveprefix = {arXiv},
  arxivid       = {1907.09358},
  eprint        = {1907.09358},
  url           = {http://arxiv.org/abs/1907.09358},
}

@Article{Smeulders2014,
  author    = {Smeulders, Arnold W.M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {{Visual tracking: An experimental survey}},
  year      = {2014},
  issn      = {01628828},
  number    = {7},
  pages     = {1442--1468},
  volume    = {36},
  abstract  = {There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. {\textcopyright} 2014 IEEE.},
  doi       = {10.1109/TPAMI.2013.230},
  keywords  = {Camera surveillance,Computer vision,Image processing,Object tracking,Tracking dataset,Tracking evaluation,Video understanding},
  publisher = {IEEE Computer Society},
}

@Misc{art/ZhengL_201805,
  author        = {Zheng, Liang and Yang, Yi and Tian, Qi},
  month         = {may},
  title         = {{SIFT Meets CNN: A Decade Survey of Instance Retrieval}},
  year          = {2018},
  abstract      = {In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.},
  archiveprefix = {arXiv},
  arxivid       = {1608.01807},
  booktitle     = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi           = {10.1109/TPAMI.2017.2709749},
  eprint        = {1608.01807},
  issn          = {01628828},
  keywords      = {Instance retrieval,SIFT,convolutional neural network,literature survey},
  number        = {5},
  pages         = {1224--1244},
  pmid          = {29610107},
  publisher     = {IEEE Computer Society},
  volume        = {40},
}

@Article{Ye2015,
  author    = {Ye, Qixiang and Doermann, David},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title     = {{Text Detection and Recognition in Imagery: A Survey}},
  year      = {2015},
  issn      = {01628828},
  month     = {jul},
  number    = {7},
  pages     = {1480--1500},
  volume    = {37},
  abstract  = {This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field.},
  doi       = {10.1109/TPAMI.2014.2366765},
  keywords  = {Survey,Text Detection,Text Localization,Text Recognition},
  publisher = {IEEE Computer Society},
}

@Misc{Kalsotra2019,
  author    = {Kalsotra, Rudrika and Arora, Sakshi},
  title     = {{A Comprehensive Survey of Video Datasets for Background Subtraction}},
  year      = {2019},
  abstract  = {Background subtraction is an effective method of choice when it comes to detection of moving objects in videos and has been recognized as a breakthrough for the wide range of applications of intelligent video analytics (IVA). In recent years, a number of video datasets intended for background subtraction have been created to address the problem of large realistic datasets with accurate ground truth. The use of these datasets enables qualitative as well as quantitative comparisons and allows benchmarking of different algorithms. Finding the appropriate dataset is generally a cumbersome task for an exhaustive evaluation of algorithms. Therefore, we systematically survey standard video datasets and list their applicability for different applications. This paper presents a comprehensive account of public video datasets for background subtraction and attempts to cover the lack of a detailed description of each dataset. The video datasets are presented in chronological order of their appearance. Current trends of deep learning in background subtraction along with top-ranked background subtraction methods are also discussed in this paper. The survey introduced in this paper will assist researchers of the computer vision community in the selection of appropriate video dataset to evaluate their algorithms on the basis of challenging scenarios that exist in both indoor and outdoor environments.},
  booktitle = {IEEE Access},
  doi       = {10.1109/ACCESS.2019.2914961},
  issn      = {21693536},
  keywords  = {Background model,background subtraction,challenges,datasets,deep neural networks,foreground,intelligent video analytics (IVA),video frames},
  pages     = {59143--59171},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  volume    = {7},
}

@InProceedings{Wei2016a,
  author    = {Wei, Yi and Liu, Shijun and Sun, Jiao and Cui, Lizhen and Pan, Li and Wu, Lei},
  booktitle = {Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016},
  title     = {{Big datasets for research: A survey on flagship conferences}},
  year      = {2016},
  month     = {oct},
  pages     = {394--401},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  abstract  = {It is obvious that big data can bring us new opportunities to discover valuable information. Apparently, corresponding big datasets are powerful tools for scholars, which connect theoretical studies to reality. They can help scholars to evaluate their achievements and find new problems. In recent years, there has been a significant growth in research data repositories and registries. However, these infrastructures are fragmented across institutions, countries and research domains. As such, finding research datasets is not a trivial task for many researchers. Thus we investigated 195 papers regarding big data on some notable international conferences in recent 3 years, and also gathered 285 datasets mentioned in them. In this paper, we present and analyze our survey results in terms of the status quo of big data research and datasets from different aspects. In particular, we propose two different taxonomies of big datasets and classify our surveyed datasets into them. In addition, we also give a brief introduction about 7 widely accepted data collections online. Finally, some basic principles for scholars in choosing and using big datasets are given.},
  doi       = {10.1109/BigDataCongress.2016.62},
  isbn      = {9781509026227},
  keywords  = {Big data,Datasets,Survey},
}

@Article{Hosny2018,
  author   = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J.W.L.},
  journal  = {PLoS Medicine},
  title    = {{Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study}},
  year     = {2018},
  issn     = {15491676},
  number   = {11},
  volume   = {15},
  abstract = {Background: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. Methods and findings: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5–93.3], survival median = 1.7 years [range 0.0–11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5–93.3], survival median = 1.3 years [range 0.0–11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2–88.0], survival median = 3.1 years [range 0.0–8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95% CI 0.63–0.78], p < 0.001) and surgery (AUC = 0.71 [95% CI 0.60–0.82], p < 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p < 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters—including age, sex, and tumor node metastasis stage—as well as demonstrate high robustness against test–retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. Conclusions: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.},
  annote   = {From Duplicate 2 (Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study - Hosny, Ahmed; Parmar, Chintan; Coroller, Thibaud P; Grossmann, Patrick; Zeleznik, Roman; Kumar, Avnish; Bussink, Johan; Gillies, Robert J; Mak, Raymond H; Aerts, Hugo J.W.L.) Publisher: Public Library of Science},
  doi      = {10.1371/journal.pmed.1002711},
  pmid     = {30500819},
  url      = {https://doi.org/10.1371/journal.pmed.1002711},
}

@InProceedings{Xie2016,
  author        = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks}},
  year          = {2016},
  pages         = {842--857},
  volume        = {9908 LNCS},
  abstract      = {As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2Dvideos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained endto-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.},
  annote        = {From Duplicate 2 (Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks - Xie, Junyuan; Girshick, Ross; Farhadi, Ali) _eprint: 1604.03650},
  archiveprefix = {arXiv},
  arxivid       = {1604.03650},
  doi           = {10.1007/978-3-319-46493-0_51},
  eprint        = {1604.03650},
  isbn          = {9783319464923},
  issn          = {16113349},
  keywords      = {Deep convolutional neural networks,Monocular stereo reconstruction},
  url           = {http://arxiv.org/abs/1604.03650},
}

@InProceedings{Wu2015,
  author        = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{3D ShapeNets: A deep representation for volumetric shapes}},
  year          = {2015},
  pages         = {1912--1920},
  volume        = {07-12-June},
  abstract      = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
  annote        = {_eprint: 1406.5670},
  archiveprefix = {arXiv},
  arxivid       = {1406.5670},
  doi           = {10.1109/CVPR.2015.7298801},
  eprint        = {1406.5670},
  isbn          = {9781467369640},
  issn          = {10636919},
  keywords      = {Computer Science - Computer Vision and Pattern Rec},
  url           = {http://arxiv.org/abs/1406.5670},
}

@Article{Hohman2019,
  author        = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title         = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
  year          = {2019},
  issn          = {19410506},
  number        = {8},
  pages         = {2674--2693},
  volume        = {25},
  abstract      = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
  archiveprefix = {arXiv},
  arxivid       = {1801.06889},
  doi           = {10.1109/TVCG.2018.2843369},
  eprint        = {1801.06889},
  keywords      = {Deep learning,information visualization,neural networks,visual analytics},
  url           = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
}

@Article{Dice1945,
  author   = {Dice, Lee R.},
  journal  = {Ecology},
  title    = {{Measures of the Amount of Ecologic Association Between Species}},
  year     = {1945},
  issn     = {0012-9658},
  number   = {3},
  pages    = {297--302},
  volume   = {26},
  abstract = {The coefficient of association of Forbes indicates the amount of association be- tween two given species compared to the amount of association between them expected by chance. In order to provide a simple direct measure of the amount of association of one species with another the association index is proposed. If a is the number of random samples of a given series in which species A occurs and h is the number of samples in which another species B occurs together with A, then the association index B/A = h/a. Similarly, if b is the number of samples in which species B occurs, then the associa- tion index A/B = h/b. There is also proposed a coincidence index, 2h/(a + b), whose value is intermediate between the two reciprocal association indices. As a measure of the statistical reliability of the deviation shown by the samples of a given series from the amount of associa- tion expected by chance, the chi-square test may be used.},
  doi      = {10.2307/1932409},
}

@Book{Learning2017,
  author    = {Learning, Machine},
  publisher = {McGraw-Hill Education},
  title     = {{Machine learning 분야 소개 및 주요 방법론 학습 기본 machine learning 알고리즘에 대한 이해 및 응용 관련 최신 연구 동향 습득}},
  year      = {2017},
  isbn      = {026201243X},
  number    = {13},
  volume    = {45},
  abstract  = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
  booktitle = {Machine Learning},
  pages     = {40--48},
  url       = {https://books.google.ca/books?id=EoYBngEACAAJ&dq=mitchell+machine+learning+1997&hl=en&sa=X&ved=0ahUKEwiomdqfj8TkAhWGslkKHRCbAtoQ6AEIKjAA},
}

@InCollection{LeCun1989,
  author    = {LeCun, Yann and Others},
  booktitle = {Connectionism in perspective},
  publisher = {Elsevier},
  title     = {{Generalization and network design strategies}},
  year      = {1989},
  editor    = {Pfeifer, R and Schreter, Z and Fogelman, F and Steels, L},
  isbn      = {978-0444880611},
  pages     = {143--155},
  abstract  = {An interestmg property of connectiomst systems is their ability to learn from examples. Although most recent work in the field concentrates on reducing learning times, the most important feature of a learning ma-chine is its generalization performance. It is usually accepted that good generalization performance on real-world problems cannot be achieved unless some a pnon knowledge about the task is butlt Into the system. Back-propagation networks provide a way of specifymg such knowledge by imposing constraints both on the architecture of the network and on its weights. In general, such constramts can be considered as particular transformations of the parameter space Building a constramed network for image recogmtton appears to be a feasible task. We descnbe a small handwritten digit recogmtion problem and show that, even though the problem is linearly separable, single layer networks exhibit poor generalizatton performance. Multtlayer constrained networks perform very well on this task when orgamzed in a hierarchical structure with shift invariant feature detectors. These results confirm the idea that minimizing the number of free parameters in the network enhances generalization.},
}

@Article{Su2015,
  author        = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Multi-view convolutional neural networks for 3D shape recognition}},
  year          = {2015},
  issn          = {15505499},
  pages         = {945--953},
  volume        = {2015 Inter},
  abstract      = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
  annote        = {From Duplicate 1 (Multi-view Convolutional Neural Networks for 3D Shape Recognition - Su, Hang; Maji, Subhransu; Kalogerakis, Evangelos; Learned-Miller, Erik G) _eprint: 1505.00880},
  archiveprefix = {arXiv},
  arxivid       = {1505.00880},
  doi           = {10.1109/ICCV.2015.114},
  eprint        = {1505.00880},
  isbn          = {9781467383912},
  url           = {http://arxiv.org/abs/1505.00880},
}

@Article{Ren2017,
  author        = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  year          = {2017},
  issn          = {01628828},
  number        = {6},
  pages         = {1137--1149},
  volume        = {39},
  abstract      = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  annote        = {From Duplicate 1 (Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - Ren, Shaoqing; He, Kaiming; Girshick, Ross B; Sun, Jian) _eprint: 1506.01497},
  archiveprefix = {arXiv},
  arxivid       = {1506.01497},
  doi           = {10.1109/TPAMI.2016.2577031},
  eprint        = {1506.01497},
  keywords      = {Object detection,convolutional neural network,region proposal},
  pmid          = {27295650},
  url           = {http://arxiv.org/abs/1506.01497},
}

@Article{Ghiasi2016,
  author        = {Ghiasi, Golnaz and Fowlkes, Charless C.},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Laplacian pyramid reconstruction and refinement for semantic segmentation}},
  year          = {2016},
  issn          = {16113349},
  pages         = {519--534},
  volume        = {9907 LNCS},
  abstract      = {CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them to tasks that require dense, pixel-accurate labeling. This paper makes two contributions: (1) We demonstrate that while the apparent spatial resolution of convolutional feature maps is low, the high-dimensional feature representation contains significant sub-pixel localization information. (2) We describe a multi-resolution reconstruction architecture based on a Laplacian pyramid that uses skip connections from higher resolution feature maps and multiplicative gating to successively refine segment boundaries reconstructed from lower-resolution maps. This approach yields state-of-the-art semantic segmentation results on the PASCAL VOC and Cityscapes segmentation benchmarks without resorting to more complex random-field inference or instance detection driven architectures.},
  annote        = {From Duplicate 1 (Laplacian Reconstruction and Refinement for Semantic Segmentation - Ghiasi, Golnaz; Fowlkes, Charless C) _eprint: 1605.02264},
  archiveprefix = {arXiv},
  arxivid       = {1605.02264},
  doi           = {10.1007/978-3-319-46487-9_32},
  eprint        = {1605.02264},
  isbn          = {9783319464862},
  keywords      = {Convolutional neural networks,Semantic segmentation},
  url           = {http://arxiv.org/abs/1605.02264},
}

@Article{Zamir2016,
  author        = {Zamir, Amir R. and Wekel, Tilman and Agrawal, Pulkit and Wei, Colin and Malik, Jitendra and Savarese, Silvio},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Generic 3D representation via pose estimation and matching}},
  year          = {2016},
  issn          = {16113349},
  pages         = {535--553},
  volume        = {9907 LNCS},
  abstract      = {Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learnt features).We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.},
  annote        = {From Duplicate 1 (Generic 3D Representation via Pose Estimation and Matching - Zamir, Amir Roshan; Wekel, Tilman; Agrawal, Pulkit; Wei, Colin; Malik, Jitendra; Savarese, Silvio) _eprint: 1710.08247},
  archiveprefix = {arXiv},
  arxivid       = {1710.08247},
  doi           = {10.1007/978-3-319-46487-9_33},
  eprint        = {1710.08247},
  isbn          = {9783319464862},
  keywords      = {Descriptor learning,Generic vision,Pose estimation,Representation,Street view,Wide-baseline matching},
  url           = {http://arxiv.org/abs/1710.08247},
}

@Article{Flynn2016,
  author        = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Deep stereo: Learning to predict new views from the world's imagery}},
  year          = {2016},
  issn          = {10636919},
  pages         = {5515--5524},
  volume        = {2016-Decem},
  abstract      = {Deep networks have recently enjoyed enormous success when applied to recognition and classification problems in computer vision [22, 33], but their use in graphics problems has been limited ([23, 7] are notable recent exceptions). In this work, we present a novel deep architecture that performs new view synthesis directly from pixels, trained from a large number of posed image sets. In contrast to traditional approaches, which consist of multiple complex stages of processing, each of which requires careful tuning and can fail in unexpected ways, our system is trained end-to-end. The pixels from neighboring views of a scene are presented to the network, which then directly produces the pixels of the unseen view. The benefits of our approach include generality (we only require posed image sets and can easily apply our method to different domains), and high quality results on traditionally difficult scenes. We believe this is due to the end-to-end nature of our system, which is able to plausibly generate pixels according to color, depth, and texture priors learnt automatically from the training data. We show view interpolation results on imagery from the KITTI dataset [12], from data from [1] as well as on Google Street View images. To our knowledge, our work is the first to apply deep learning to the problem of new view synthesis from sets of real-world, natural imagery.},
  annote        = {From Duplicate 2 (DeepStereo: Learning to Predict New Views from the World's Imagery - Flynn, John; Neulander, Ivan; Philbin, James; Snavely, Noah) _eprint: 1506.06825},
  archiveprefix = {arXiv},
  arxivid       = {1506.06825},
  doi           = {10.1109/CVPR.2016.595},
  eprint        = {1506.06825},
  isbn          = {9781467388504},
  url           = {http://arxiv.org/abs/1506.06825},
}

@Article{Wu2016,
  author        = {Wu, Jiajun and Xue, Tianfan and Lim, Joseph J. and Tian, Yuandong and Tenenbaum, Joshua B. and Torralba, Antonio and Freeman, William T.},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Single image 3D interpreter network}},
  year          = {2016},
  issn          = {16113349},
  pages         = {365--382},
  volume        = {9910 LNCS},
  abstract      = {Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an endto-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technic al innovations.First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as image retrieval.},
  annote        = {From Duplicate 2 (Single Image 3D Interpreter Network - Wu, Jiajun; Xue, Tianfan; Lim, Joseph J; Tian, Yuandong; Tenenbaum, Joshua B; Torralba, Antonio; Freeman, William T) _eprint: 1604.08685},
  archiveprefix = {arXiv},
  arxivid       = {1604.08685},
  doi           = {10.1007/978-3-319-46466-4_22},
  eprint        = {1604.08685},
  isbn          = {9783319464657},
  keywords      = {3D structure,Keypoint estimation,Neural network,Single image 3D reconstruction,Synthetic data},
  url           = {http://arxiv.org/abs/1604.08685},
}

@Misc{Zhang2018,
  author        = {shi Zhang, Quan and chun Zhu, Song},
  title         = {{Visual interpretability for deep learning: a survey}},
  year          = {2018},
  abstract      = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  annote        = {_eprint: 1802.00614},
  archiveprefix = {arXiv},
  arxivid       = {1802.00614},
  booktitle     = {Frontiers of Information Technology and Electronic Engineering},
  doi           = {10.1631/FITEE.1700808},
  eprint        = {1802.00614},
  issn          = {20959230},
  keywords      = {Artificial intelligence,Deep learning,Interpretable model},
  number        = {1},
  pages         = {27--39},
  volume        = {19},
}

@Article{Yong2012,
  author   = {Yong, Ching Yee and Chew, Kim Mey and Mahmood, Nasrul Humaimi and Ariffin, Ismail},
  journal  = {Procedia - Social and Behavioral Sciences},
  title    = {{A Survey of Visualization Tools in Medical Imaging}},
  year     = {2012},
  issn     = {18770428},
  pages    = {265--271},
  volume   = {56},
  abstract = {More than 30 students from university campus participated in the Development of Biomedical Image Processing Software Package for New Learners Survey investigating the use of software package for processing and editing image. The survey was available online for six months. Facts and opinions were sought to learn the general information, interactive image processing tool, non-interactive (automatic) tool, current status and future of image processing package tool. Composed of 19 questions, the survey built a comprehensive picture of the software package, programming language, workflow of the tool and captured the attitudes of the respondents. Result shows that MATLAB was difficult to use but it was viewed in high regard however. The result of this study is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed software package.},
  doi      = {10.1016/j.sbspro.2012.09.654},
  keywords = {Image editting,Image processing,Medical imaging,Software package,Visualisation tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S187704281204116X},
}

@Article{Mueller2019,
  author        = {M{\"{u}}ller, Dominik and Kramer, Frank},
  title         = {{MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}},
  year          = {2019},
  annote        = {_eprint: 1910.09308},
  archiveprefix = {arXiv},
  arxivid       = {1910.09308},
  eprint        = {1910.09308},
}

@Article{art/HenschelL_2020,
  author        = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
  journal       = {NeuroImage},
  title         = {{FastSurfer - A fast and accurate deep learning based neuroimaging pipeline}},
  year          = {2020},
  issn          = {10959572},
  volume        = {219},
  abstract      = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, replicating FreeSurfer's anatomical segmentation including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole-brain segmentation into 95 classes. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and subcortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (in under 1 ​min) and surface-based thickness analysis (within only around 1 ​h runtime). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and high sensitivity to group differences in dementia.},
  annote        = {_eprint: 1910.03866},
  archiveprefix = {arXiv},
  arxivid       = {1910.03866},
  doi           = {10.1016/j.neuroimage.2020.117012},
  eprint        = {1910.03866},
  keywords      = {Artificial intelligence,Computational neuroimaging,Deep learning,Freesurfer,Structural MRI},
  pmid          = {32526386},
  url           = {http://arxiv.org/abs/1910.03866},
}

@Article{Chetlur2014,
  author        = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal       = {arXiv: Neural and Evolutionary Computing},
  title         = {{cuDNN: Efficient Primitives for Deep Learning}},
  year          = {2014},
  abstract      = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption.},
  archiveprefix = {arXiv},
  arxivid       = {1410.0759},
  eprint        = {1410.0759},
  url           = {http://arxiv.org/abs/1410.0759},
}

@Article{Maloney2010,
  author   = {Maloney, John and Resnick, Mitchel and Rusk, Natalie and Silverman, Brian and Eastmond, Evelyn},
  journal  = {ACM Transactions on Computing Education},
  title    = {{The scratch programming language and environment}},
  year     = {2010},
  issn     = {1946-6226},
  number   = {4},
  pages    = {16},
  volume   = {10},
  abstract = {Scratch is a visual programming environment that allows users (primarily ages 8 to 16) to learn computer programming while working on personally meaningful projects such as animated stories and games. A key design goal of Scratch is to support self-directed learning through tinkering and collaboration with peers. This article explores how the Scratch programming language and environment support this goal. {\textcopyright} 2010 ACM.},
  doi      = {10.1145/1868358.1868363},
  keywords = {Programming environment,Programming language,Scratch,Visual programming language},
}

@Misc{Fischl2012,
  author    = {Fischl, Bruce},
  title     = {{FreeSurfer}},
  year      = {2012},
  abstract  = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. {\textcopyright} 2012 Elsevier Inc.},
  booktitle = {NeuroImage},
  doi       = {10.1016/j.neuroimage.2012.01.021},
  issn      = {10538119},
  keywords  = {MRI,Morphometry,Registration,Segmentation},
  number    = {2},
  pages     = {774--781},
  pmid      = {22248573},
  url       = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
  volume    = {62},
}

@Article{Beers2021,
  author        = {Beers, Andrew and Brown, James and Chang, Ken and Hoebel, Katharina and Patel, Jay and Ly, K. Ina and Tolaney, Sara M. and Brastianos, Priscilla and Rosen, Bruce and Gerstner, Elizabeth R. and Kalpathy-Cramer, Jayashree},
  journal       = {Neuroinformatics},
  title         = {{DeepNeuro: an open-source deep learning toolbox for neuroimaging}},
  year          = {2021},
  issn          = {15590089},
  number        = {1},
  pages         = {127--140},
  volume        = {19},
  abstract      = {Translating deep learning research from theory into clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a Python-based deep learning framework that puts deep neural networks for neuroimaging into practical usage with a minimum of friction during implementation. We show how this framework can be used to design deep learning pipelines that can load and preprocess data, design and train various neural network architectures, and evaluate and visualize the results of trained networks on evaluation data. We present a way of reproducibly packaging data pre- and postprocessing functions common in the neuroimaging community, which facilitates consistent performance of networks across variable users, institutions, and scanners. We show how deep learning pipelines created with DeepNeuro can be concisely packaged into shareable Docker and Singularity containers with user-friendly command-line interfaces.},
  annote        = {_eprint: 1808.04589},
  archiveprefix = {arXiv},
  arxivid       = {1808.04589},
  doi           = {10.1007/s12021-020-09477-5},
  eprint        = {1808.04589},
  keywords      = {Augmentation,Deep learning,Docker,Neuroimaging,Preprocessing},
  pmid          = {32578020},
  url           = {http://arxiv.org/abs/1808.04589},
}

@Article{VanDerWalt2011,
  author        = {{Van Der Walt}, St{\'{e}}fan and Colbert, S. Chris and Varoquaux, Ga{\"{e}}l},
  journal       = {Computing in Science and Engineering},
  title         = {{The NumPy array: A structure for efficient numerical computation}},
  year          = {2011},
  issn          = {15219615},
  month         = {mar},
  number        = {2},
  pages         = {22--30},
  volume        = {13},
  abstract      = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. {\textcopyright} 2011 IEEE.},
  archiveprefix = {arXiv},
  arxivid       = {1102.1523},
  doi           = {10.1109/MCSE.2011.37},
  eprint        = {1102.1523},
  keywords      = {NumPy,Python,numerical computations,programming libraries,scientific programming},
}

@Article{Oliphant2007,
  author  = {Oliphant, Travis E.},
  journal = {Computing in Science and Engineering},
  title   = {{Python for scientific computing}},
  year    = {2007},
  issn    = {15219615},
  number  = {3},
  pages   = {10--20},
  volume  = {9},
  annote  = {_eprint: https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.58},
  doi     = {10.1109/MCSE.2007.58},
  url     = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.58},
}

@Article{Avants2011,
  author   = {Avants, Brian B. and Tustison, Nicholas J. and Song, Gang and Cook, Philip A. and Klein, Arno and Gee, James C.},
  journal  = {NeuroImage},
  title    = {{A reproducible evaluation of ANTs similarity metric performance in brain image registration}},
  year     = {2011},
  issn     = {10538119},
  number   = {3},
  pages    = {2033--2044},
  volume   = {54},
  abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard = 0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard = 0.669. ±. 0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling. {\textcopyright} 2010 Elsevier Inc.},
  doi      = {10.1016/j.neuroimage.2010.09.025},
  pmid     = {20851191},
  url      = {http://www.sciencedirect.com/science/article/pii/S1053811910012061},
}

@Article{Jenkinson2012,
  author   = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
  journal  = {NeuroImage},
  title    = {{FSL - Review}},
  year     = {2012},
  issn     = {1095-9572},
  number   = {2},
  pages    = {782--90},
  volume   = {62},
  abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on "20 years of fMRI" we have been asked to write about the history, developments and current status of FSL. We also include some descriptions of parts of FSL that are not well covered in the existing literature. We hope that some of this content might be of interest to users of FSL, and also maybe to new research groups considering creating, releasing and supporting new software packages for brain image analysis.},
  doi      = {10.1016/j.neuroimage.2011.09.015},
  keywords = {20th Century,21st Century,Brain,Brain Mapping,Brain Mapping: history,Brain Mapping: methods,Brain: anatomy & histology,Brain: physiology,Computer-Assisted,Computer-Assisted: history,Computer-Assisted: methods,Diffusion Magnetic Resonance Imaging,Diffusion Magnetic Resonance Imaging: history,Diffusion Magnetic Resonance Imaging: methods,History,Humans,Image Processing,Software,Software: history},
  pmid     = {21979382},
  url      = {http://www.ncbi.nlm.nih.gov/pubmed/21979382},
}

@Article{Paszke2019,
  author        = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  journal       = {Advances in Neural Information Processing Systems},
  title         = {{PyTorch: An imperative style, high-performance deep learning library}},
  year          = {2019},
  issn          = {10495258},
  volume        = {32},
  abstract      = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  arxivid       = {1912.01703},
  eprint        = {1912.01703},
  url           = {http://arxiv.org/abs/1912.01703},
}

@InProceedings{Paszke2017,
  author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle = {NIPS 2017 Autodiff Workshop: The Future of Gradient-based Machine Learning Software and Techniques},
  title     = {{Automatic differentiation in \uppercase{P}y\uppercase{T}orch}},
  year      = {2017},
  pages     = {8024--8035},
  abstract  = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Article{Lowekamp2013,
  author   = {Lowekamp, Bradley C. and Chen, David T. and Ib{\'{a}}{\~{n}}ez, Luis and Blezek, Daniel},
  journal  = {Frontiers in Neuroinformatics},
  title    = {{The design of simpleITK}},
  year     = {2013},
  issn     = {16625196},
  number   = {DEC},
  pages    = {45},
  volume   = {7},
  abstract = {SimpleITK is a new interface to the Insight Segmentation and Registration Toolkit (ITK) designed to facilitate rapid prototyping, education and scientific activities via high level programming languages. ITK is a templated C++ library of image processing algorithms and frameworks for biomedical and other applications, and it was designed to be generic, flexible and extensible. Initially, ITK provided a direct wrapping interface to languages such as Python and Tcl through the WrapITK system. Unlike WrapITK, which exposed ITK's complex templated interface, SimpleITK was designed to provide an easy to use and simplified interface to ITK's algorithms. It includes procedural methods, hides ITK's demand driven pipeline, and provides a template-less layer. Also SimpleITK provides practical conveniences such as binary distribution packages and overloaded operators. Our user-friendly design goals dictated a departure from the direct interface wrapping approach of WrapITK, toward a new facade class structure that only exposes the required functionality, hiding ITK's extensive template use. Internally SimpleITK utilizes a manual description of each filter with code-generation and advanced C++ meta-programming to provide the higher-level interface, bringing the capabilities of ITK to a wider audience. SimpleITK is licensed as open source software library under the Apache License Version 2.0 and more information about downloading it can be found at http://www.simpleitk.org. {\textcopyright} 2013 Lowek amp, Chen, Ib{\'{a}}{\~{n}}ez and Blezek.},
  doi      = {10.3389/fninf.2013.00045},
  keywords = {Image processing and analysis,Image processing software,Insight toolkit,Segmentation,Software design,Software development},
}

@Article{Gibson2018,
  author        = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I. and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C. and Ourselin, S{\'{e}}bastien and Cardoso, M. Jorge and Vercauteren, Tom},
  journal       = {Computer Methods and Programs in Biomedicine},
  title         = {{NiftyNet: a deep-learning platform for medical imaging}},
  year          = {2018},
  issn          = {18727565},
  pages         = {113--122},
  volume        = {158},
  abstract      = {Background and objectives: Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. Methods: The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default. Results: We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses. Conclusions: The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
  archiveprefix = {arXiv},
  arxivid       = {1709.03485},
  doi           = {10.1016/j.cmpb.2018.01.025},
  eprint        = {1709.03485},
  keywords      = {Convolutional neural network,Deep learning,Generative adversarial network,Image regression,Medical image analysis,Segmentation},
  pmid          = {29544777},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
}

@Article{Satyanarayanan2013,
  author   = {Satyanarayanan, Mahadev and Goode, Adam and Gilbert, Benjamin and Harkes, Jan and Jukic, Drazen},
  journal  = {Journal of Pathology Informatics},
  title    = {{OpenSlide: A vendor-neutral software foundation for digital pathology}},
  year     = {2013},
  issn     = {2153-3539},
  number   = {1},
  pages    = {27},
  volume   = {4},
  abstract = {Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission, processing and interoperability. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools, viewers and software libraries. This creates issues not only for pathologists, but also for interoperability. In this paper, we present the design and implementation of OpenSlide<i>,</i> a vendor-neutral C library for reading and manipulating digital slides of diverse vendor formats. The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface can transparently handle multiple vendor formats. OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health.},
  doi      = {10.4103/2153-3539.119005},
}

@InProceedings{Abadi2016,
  author        = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  booktitle     = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
  title         = {{TensorFlow: A system for large-scale machine learning}},
  year          = {2016},
  pages         = {265--283},
  volume        = {abs/1605.0},
  abstract      = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
  annote        = {_eprint: 1605.08695},
  archiveprefix = {arXiv},
  arxivid       = {1605.08695},
  eprint        = {1605.08695},
  isbn          = {9781931971331},
  url           = {http://arxiv.org/abs/1605.08695},
}

@InProceedings{Jia2014,
  author        = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  booktitle     = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
  title         = {{Caffe: Convolutional architecture for fast feature embedding}},
  year          = {2014},
  pages         = {675--678},
  abstract      = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
  archiveprefix = {arXiv},
  arxivid       = {1408.5093},
  doi           = {10.1145/2647868.2654889},
  eprint        = {1408.5093},
  isbn          = {9781450330633},
  issn          = {1450330630},
  keywords      = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
}

@Article{He2021a,
  author        = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  journal       = {Knowledge-Based Systems},
  title         = {{AutoML: A survey of the state-of-the-art}},
  year          = {2021},
  issn          = {09507051},
  volume        = {212},
  abstract      = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods – covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms' performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
  annote        = {_eprint: 1908.00709},
  archiveprefix = {arXiv},
  arxivid       = {1908.00709},
  doi           = {10.1016/j.knosys.2020.106622},
  eprint        = {1908.00709},
  keywords      = {Automated machine learning (autoML),Deep learning,Hyperparameter optimization (HPO),Neural architecture search (NAS)},
  url           = {http://arxiv.org/abs/1908.00709},
}

@Article{Reinhard2001,
  author  = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
  journal = {IEEE Computer Graphics and Applications},
  title   = {{Color transfer between images}},
  year    = {2001},
  issn    = {02721716},
  number  = {5},
  pages   = {34--41},
  volume  = {21},
  doi     = {10.1109/38.946629},
}

@Article{Senthilraja2014,
  author   = {Senthilraja, S and Suresh, P and Suganthi, M},
  journal  = {International Journal of Scientific and Engineering Research},
  title    = {{Noise Reduction in Computed Tomography Image Using WB–Filter}},
  year     = {2014},
  issn     = {2229-5518},
  month    = {mar},
  number   = {3},
  pages    = {243},
  volume   = {5},
  abstract = {Image processing concept play a important role in the field of Medical to diagnosis of diseases. Noise is introduced in the medical images due to various reasons. In Medical Imaging, Noise degrades the quality of images. This degradation includes suppression of edges, blurring boundaries etc. Edge and preservation details are very important to discover a disease. Noise removal is a very challenging issue in the Medical Image Processing. Denoising can help the physicians to diagnose the diseases. Medical Images include CT, MRI scan, X-ray and ultrasound images etc. This paper we implemented a new filter called WB-Filter for Medical Image denoising. WB-Filter mainly focuses on speckle noise & Gaussian Noise removal especially in the CT scan images. Experimental results are compared with other three filtering concepts. The result images quality is measured by the PSNR, RMSE and MSE. The results demonstrate that the proposed WB-Filter concept obtaining the optimum result quality of the Medical Image.},
}

@Article{Ruifrok2001,
  author   = {Ruifrok, A. C. and Johnston, D. A.},
  journal  = {Analytical and Quantitative Cytology and Histology},
  title    = {{Quantification of histochemical staining by color deconvolution}},
  year     = {2001},
  issn     = {08846812},
  number   = {4},
  pages    = {291--299},
  volume   = {23},
  abstract = {OBJECTIVE: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. STUDY DESIGN: An algorithm was developed to deconvolve the color information acquired with red-green-blue (RGB) cameras and to calculate the contribution of each of the applied stains based on stain-specific RGB absorption. The algorithm was tested using different combinations of diaminobenzidine, hematoxylin and eosin at different staining levels. RESULTS: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility and saturation of staining was prevented. CONCLUSION: This image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains using a laboratory microscope, standard RGB camera setup and the public domain program NIH Image.},
  keywords = {Color deconvolution,Color separation,Computer-assisted,Image processing,Immunohistochemistry},
  pmid     = {11531144},
}

@Article{Rameshkumar2016,
  author   = {Rameshkumar, S and Thilak, J Anish Jafrin and Suresh, P and Sathishkumar, S and Subramani, N},
  journal  = {International Journal of Innovative Research in Science Engineering and Technology},
  title    = {{Speckle Noise Removal in MRI Scan Image Using WB – Filter}},
  year     = {2016},
  issn     = {2319-8753},
  month    = {dec},
  number   = {12},
  pages    = {21079--21083},
  volume   = {5},
  doi      = {10.15680/IJIRSET.2016.0512161},
  keywords = {filter,image denoising,mri,mse,psnr,rmse,wb},
}

@Article{Tustison2010,
  author   = {Tustison, Nicholas J. and Avants, Brian B. and Cook, Philip A. and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A. and Gee, James C.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{N4ITK: Improved N3 bias correction}},
  year     = {2010},
  issn     = {02780062},
  month    = {jun},
  number   = {6},
  pages    = {1310--1320},
  volume   = {29},
  abstract = {A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as N4ITK, available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized 3He lung image data, and 9.4T postmortem hippocampus data. {\textcopyright} 2006 IEEE.},
  doi      = {10.1109/TMI.2010.2046908},
  keywords = {B-spline approximation,Bias field,Inhomogeneity,N3},
  pmid     = {20378467},
}

@Article{Ogiela2008,
  author    = {Ogiela, Marek R. and Tadeusiewicz, Ryszard},
  journal   = {Studies in Computational Intelligence},
  title     = {{Preprocessing medical images and their overall enhancement}},
  year      = {2008},
  issn      = {1860949X},
  pages     = {65--97},
  volume    = {84},
  abstract  = {This chapter briefly discusses the main stages of image preprocessing. The introduction to this book mentioned that the preprocessing of medical image is subject to certain restrictions and is generally more complex than the processing of other image types [26, 52]. This is why, of the many different techniques and methods for image filtering, we have decided to discuss here only selected ones, most frequently applied to medical images and which have been proven to be suitable for that purpose in numerous practical cases. Their operation will be illustrated with examples of simple procedures aimed at improving the quality of imaging and allowing significant information to be generated for its use at the stages of image interpretation. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
  address   = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-540-75402-2_4},
  isbn      = {9783540753995},
  publisher = {Springer Berlin Heidelberg},
  url       = {https://doi.org/10.1007/978-3-540-75402-2_4},
}

@Article{Thenua2010,
  author   = {Thenua, Raj and Agarwal, SK},
  journal  = {International Journal of Engineering Science and Technology},
  title    = {{Simulation and performance analysis of adaptive filter in noise cancellation}},
  year     = {2010},
  number   = {9},
  pages    = {4373--4378},
  volume   = {2},
  abstract = {Noise problems in the environment have gained attention due to the tremendous growth of technology that has led to noisy engines, heavy machinery, high speed wind buffeting and other noise sources. The problem of controlling the noise level has become the focus of a tremendous amount of research over the years. In last few years various adaptive algorithms are developed for noise cancellation. In this paper we present an implementation of LMS (Least Mean Square), NLMS (Normalized Least Mean Square) and RLS (Recursive Least Square) algorithms on MATLAB platform with the intention to compare their performance in noise cancellation. We simulate the adaptive filter in MATLAB with a noisy tone signal and white noise signal and analyze the performance of algorithms in terms of MSE (Mean Squared Error), percentage noise removal, computational complexity and stability. The obtained results shows that RLS has the best performance but at the cost of large computational complexity and memory requirement.},
  annote   = {_eprint: 1104.1962},
  keywords = {Adaptive Filter,Computer Science - Other Computer Science,Convergence,DOAJ:Computer Science,DOAJ:Technology and Engineering,Electronic computers. Computer science,FTRLS,GAL,IJCSI,Instruments and machines,Mathematics,Mean Square Error,Noise,Q,QA1-939,QA71-90,QA75.5-76.95,RLS,Science},
}

@Article{BeaulahJeyavathana2016,
  author   = {{Beaulah Jeyavathana}, R and Balasubramanian, R and Pandian, A Anbarasa},
  journal  = {International Journal of Research and Scientific Innovation},
  title    = {{A Survey : Analysis on Pre - processing and Segmentation Techniques for Medical Images}},
  year     = {2016},
  number   = {June},
  pages    = {2321--2705},
  volume   = {III},
  abstract = {See, stats, and : https : / / www . researchgate . net / publication / 305502844 A : Analysis -processing Segmentation Article CITATIONS 0 READS 171 3 , including : Balasubramanian Manonmaniam 17 SEE Anbarasa Manonmaniam 12 SEE All . The . Abstract : Pre - Processing and Segmentation Techniques are used in the application of medical images . Image segmentation is a tediousprocess due to restrictions on Image acquisitions . The most important goal of medical image segmentation is to perform operations on images to detect patterns and to retrieve information from it . In this paper , first medical image processing is discussed . Then we have been proposed approaches to segment CT and CXR images . The comparative study of various image processing techniques has been given in tabular form . This survey provides details of automated segmentation methods , specifically discussed in the context of CT images . The motive is to discuss the problems encountered in the segmentation of CT images , and the relative merits and limitations of methods currently available for segmentation of medical images .},
  keywords = {CT,CXR,Pre - processing,Segmentation},
}

@Article{Yao2017,
  author   = {Yao, Gui Lin},
  journal  = {Journal of Computer Science and Technology},
  title    = {{A Survey on Pre-Processing in Image Matting}},
  year     = {2017},
  issn     = {10009000},
  number   = {1},
  pages    = {122--138},
  volume   = {32},
  abstract = {Pre-processing is an important step in digital image matting, which aims to classify more accurate foreground and background pixels from the unknown region of the input three-region mask (Trimap). This step has no relation with the well-known matting equation and only compares color differences between the current unknown pixel and those known pixels. These newly classified pure pixels are then fed to the matting process as samples to improve the quality of the final matte. However, in the research field of image matting, the importance of pre-processing step is still blurry. Moreover, there are no corresponding review articles for this step, and the quantitative comparison of Trimap and alpha mattes after this step still remains unsolved. In this paper, the necessity and the importance of pre-processing step in image matting are firstly discussed in details. Next, current pre-processing methods are introduced by using the following two categories: static thresholding methods and dynamic thresholding methods. Analyses and experimental results show that static thresholding methods, especially the most popular iterative method, can make accurate pixel classifications in those general Trimaps with relatively fewer unknown pixels. However, in a much larger Trimap, there methods are limited by the conservative color and spatial thresholds. In contrast, dynamic thresholding methods can make much aggressive classifications on much difficult cases, but still strongly suffer from noises and false classifications. In addition, the sharp boundary detector is further discussed as a prior of pure pixels. Finally, summaries and a more effective approach are presented for pre-processing compared with the existing methods.},
  doi      = {10.1007/s11390-017-1709-z},
  keywords = {Trimap expansion,image matting,pixel classification,pre-processing},
  url      = {https://doi.org/10.1007/s11390-017-1709-z},
}

@Article{Chen2018,
  author        = {Chen, Shuai and Bruijne, Marleen De},
  journal       = {arXiv},
  title         = {{An End-to-end Approach to Semantic Segmentation with 3D CNN and Posterior-CRF in Medical Images}},
  year          = {2018},
  abstract      = {Fully-connected Conditional Random Field (CRF) is often used as post-processing to refine voxel classification results by encouraging spatial coherence. In this paper, we propose a new end-to-end training method called Posterior-CRF. In contrast with previous approaches which use the original image intensity in the CRF, our approach applies 3D, fully connected CRF to the posterior probabilities from a CNN and optimizes both CNN and CRF together. The experiments on white matter hyperintensities segmentation demonstrate that our method outperforms CNN, post-processing CRF and different end-to-end training CRF approaches.},
  annote        = {_eprint: 1811.03549},
  archiveprefix = {arXiv},
  arxivid       = {1811.03549},
  eprint        = {1811.03549},
  url           = {http://arxiv.org/abs/1811.03549},
}

@Article{Marlow2010,
  author   = {Marlow, Simon},
  journal  = {Language},
  title    = {{Haskell 2010 Language Report}},
  year     = {2010},
  pages    = {329},
  abstract = {Haskell is a general purpose, purely functional programming language incorporating many recent innovations in programming language design. Haskell provides higher-order functions, non-strict semantics, static poly- morphic typing, user-defined algebraic datatypes, pattern-matching, list comprehensions, a module system, a monadic I/O system, and a rich set of primitive datatypes, including lists, arrays, arbitrary and fixed precision integers, and floating-point numbers. Haskell is both the culmination and solidification of many years of research on non-strict functional languages.},
  url      = {http://haskell.org/definition/haskell2010.pdf},
}

@Article{Radul2001,
  author   = {Radul, Taras},
  journal  = {Applied Categorical Structures},
  title    = {{Functional representations of Lawson monads}},
  year     = {2001},
  issn     = {09272852},
  number   = {5},
  pages    = {457--463},
  volume   = {9},
  abstract = {We introduce a class of Lawson monads and show that these monads have a functional representation. A characterisation of such a representation for the inclusion hyperspace monad is given.},
  doi      = {10.1023/A:1012052928198},
  keywords = {Functional representations,Lawson monad},
}

@InProceedings{Lee2015,
  author    = {Lee, Lay Khoon and Liew, Siau Chuin},
  booktitle = {2015 4th International Conference on Software Engineering and Computer Systems, ICSECS 2015: Virtuous Software Solutions for Big Data},
  title     = {{A survey of medical image processing tools}},
  year      = {2015},
  pages     = {171--176},
  abstract  = {A precise analysis of medical image is an important stage in the contouring phase throughout radiotherapy preparation. Medical images are mostly used as radiographic techniques in diagnosis, clinical studies and treatment planning Medical image processing tool are also similarly as important. With a medical image processing tool, it is possible to speed up and enhance the operation of the analysis of the medical image. This paper describes medical image processing software tool which attempts to secure the same kind of programmability advantage for exploring applications of the pipelined processors. These tools simulate complete systems consisting of several of the proposed processing components, in a configuration described by a graphical schematic diagram. In this paper, fifteen different medical image processing tools will be compared in several aspects. The main objective of the comparison is to gather and analysis on the tool in order to recommend users of different operating systems on what type of medical image tools to be used when analysing different types of imaging. A result table was attached and discussed in the paper.},
  doi       = {10.1109/ICSECS.2015.7333105},
  isbn      = {9781467367226},
  issn      = {2289-8522},
  keywords  = {computer vision,image processing,tools component},
}

@Article{Crankshaw2018,
  author        = {Crankshaw, Daniel and Sela, Gur Eyal and Mo, Simon and Zumar, Corey and Gonzalez, Joseph E. and Stoica, Ion and Tumanov, Alexey},
  journal       = {arXiv},
  title         = {{InferLine: ML prediction pipeline provisioning and management for tight latency objectives}},
  year          = {2018},
  issn          = {23318422},
  abstract      = {Serving ML prediction pipelines spanning multiple models and hardware accelerators is a key challenge in production machine learning. Optimally configuring these pipelines to meet tight end-to-end latency goals is complicated by the interaction between model batch size, the choice of hardware accelerator, and variation in the query arrival process. In this paper we introduce InferLine, a system which provisions and manages the individual stages of prediction pipelines to meet end-to-end tail latency constraints while minimizing cost. InferLine consists of a low-frequency combinatorial planner and a high-frequency auto-scaling tuner. The low-frequency planner leverages stage-wise profiling, discrete event simulation, and constrained combinatorial search to automatically select hardware type, replication, and batching parameters for each stage in the pipeline. The high-frequency tuner uses network calculus to auto-scale each stage to meet tail latency goals in response to changes in the query arrival process. We demonstrate that InferLine outperforms existing approaches by up to 7.6x in cost while achieving up to 34.5x lower latency SLO miss rate on realistic workloads and generalizes across state-of-the-art model serving frameworks.},
  annote        = {_eprint: 1812.01776},
  archiveprefix = {arXiv},
  arxivid       = {1812.01776},
  eprint        = {1812.01776},
  url           = {http://arxiv.org/abs/1812.01776},
}

@Article{Zhang2017,
  author        = {Zhang, Jeffrey and Gajjala, Sravani and Agrawal, Pulkit and Tison, Geoffrey H. and Hallock, Laura A. and Beussink-Nelson, Lauren and Lassen, Mats H. and Fan, Eugene and Aras, Mandar A. and Jordan, Cha Randle and Fleischmann, Kirsten E. and Melisko, Michelle and Qasim, Atif and Efros, Alexei and Shah, Sanjiv J. and Bajcsy, Ruzena and Deo, Rahul C.},
  journal       = {arXiv},
  title         = {{A computer vision pipeline for automated determination of cardiac structure and function and detection of disease by two-dimensional echocardiography}},
  year          = {2017},
  issn          = {23318422},
  abstract      = {Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function by non-experts in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing of complete echo studies; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented individual cardiac chambers. The resulting cardiac structure measurements agreed with study report values [e.g. median absolute deviations (MAD) of 11.8 g/kg/m2 for left ventricular mass index and 7.7 mL/kg/m2 for left ventricular diastolic volume index, derived from 1319 and 2918 studies, respectively]. In terms of cardiac function, we computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics (e.g. the correlation of left ventricular diastolic volumes with left atrial volumes) with an average increase in the absolute Spearman correlation coefficient of 0.05 (p=0.02). Finally, we used CNNs to develop disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.},
  annote        = {_eprint: 1706.07342},
  archiveprefix = {arXiv},
  arxivid       = {1706.07342},
  eprint        = {1706.07342},
  url           = {http://arxiv.org/abs/1706.07342},
}

@Article{Rajchl2018,
  author        = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M. and Glocker, Ben},
  journal       = {arXiv},
  title         = {{NeuroNet: Fast and robust reproduction of multiple brain image segmentation pipelines}},
  year          = {2018},
  abstract      = {NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful "all-in-one", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.},
  annote        = {_eprint: 1806.04224},
  archiveprefix = {arXiv},
  arxivid       = {1806.04224},
  eprint        = {1806.04224},
  url           = {http://arxiv.org/abs/1806.04224},
}

@Article{Rajan2019,
  author        = {Rajan, Deepta and Beymer, David and Abedin, Shafiqul and Dehghan, Ehsan},
  journal       = {arXiv},
  title         = {{Pi-PE: A pipeline for pulmonary embolism detection using sparsely annotated 3D CT images}},
  year          = {2019},
  abstract      = {Pulmonary embolisms (PE) are known to be one of the leading causes for cardiac-related mortality. Due to inherent variabilities in how PE manifests and the cumbersome nature of manual diagnosis, there is growing interest in leveraging AI tools for detecting PE. In this paper, we build a two-stage detection pipeline that is accurate, computationally efficient, robust to variations in PE types and kernels used for CT reconstruction, and most importantly, does not require dense annotations. Given the challenges in acquiring expert annotations in large-scale datasets, our approach produces state-of-the-art results with very sparse emboli contours (at 10mm slice spacing), while using models with significantly lower number of parameters. We achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of highly severe PEs. Using a large, real-world dataset characterized by complex PE types and patients from multiple hospitals, we present an elaborate empirical study and provide guidelines for designing highly generalizable pipelines.},
  annote        = {_eprint: 1910.02175},
  archiveprefix = {arXiv},
  arxivid       = {1910.02175},
  eprint        = {1910.02175},
  url           = {http://arxiv.org/abs/1910.02175},
}

@Article{Zhang2019,
  author        = {Zhang, Kai and Snavely, Noah and Sun, Jin},
  title         = {{Leveraging Vision Reconstruction Pipelines for Satellite Imagery}},
  year          = {2019},
  abstract      = {Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.},
  annote        = {_eprint: 1910.02989},
  archiveprefix = {arXiv},
  arxivid       = {1910.02989},
  eprint        = {1910.02989},
  url           = {http://arxiv.org/abs/1910.02989},
}

@Article{Skibbe2019,
  author        = {Skibbe, Henrik and Watakabe, Akiya and Nakae, Ken and Gutierrez, Carlos Enrique and Tsukada, Hiromichi and Hata, Junichi and Kawase, Takashi and Gong, Rui and Woodward, Alexander and Doya, Kenji and Okano, Hideyuki and Yamamori, Tetsuo and Ishii, Shin},
  journal       = {arXiv},
  title         = {{MarmoNet: A pipeline for automated projection mapping of the common marmoset brain from whole-brain serial two-photon tomography}},
  year          = {2019},
  issn          = {23318422},
  abstract      = {Understanding the connectivity in the brain is an important prerequisite for understanding how the brain processes information. In the Brain/MINDS project, a connectivity study on marmoset brains uses two-photon microscopy fluorescence images of axonal projections to collect the neuron connectivity from defined brain regions at the mesoscopic scale. The processing of the images requires the detection and segmentation of the axonal tracer signal. The objective is to detect as much tracer signal as possible while not misclassifying other background structures as the signal. This can be challenging because of imaging noise, a cluttered image background, distortions or varying image contrast cause problems. We are developing MarmoNet, a pipeline that processes and analyzes tracer image data of the common marmoset brain. The pipeline incorporates state-of-the-art machine learning techniques based on artificial convolutional neural networks (CNN) and image registration techniques to extract and map all relevant information in a robust manner. The pipeline processes new images in a fully automated way. This report introduces the current state of the tracer signal analysis part of the pipeline.},
  annote        = {_eprint: 1908.00876},
  archiveprefix = {arXiv},
  arxivid       = {1908.00876},
  eprint        = {1908.00876},
  url           = {http://arxiv.org/abs/1908.00876},
}

@Article{Yang2019,
  author        = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  journal       = {arXiv},
  title         = {{XLNet: Generalized autoregressive pretraining for language understanding}},
  year          = {2019},
  issn          = {23318422},
  abstract      = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
  annote        = {_eprint: 1906.08237},
  archiveprefix = {arXiv},
  arxivid       = {1906.08237},
  eprint        = {1906.08237},
  url           = {http://arxiv.org/abs/1906.08237},
}

@Article{Devlin2019,
  author        = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
  journal       = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
  title         = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
  year          = {2019},
  pages         = {4171--4186},
  volume        = {1},
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  annote        = {_eprint: 1810.04805},
  archiveprefix = {arXiv},
  arxivid       = {1810.04805},
  eprint        = {1810.04805},
  isbn          = {9781950737130},
  url           = {http://arxiv.org/abs/1810.04805},
}

@InProceedings{Chen2017,
  author        = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{Dual path networks}},
  year          = {2017},
  pages         = {4468--4476},
  volume        = {2017-Decem},
  abstract      = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
  archiveprefix = {arXiv},
  arxivid       = {1707.01629},
  eprint        = {1707.01629},
  issn          = {10495258},
}

@Article{He2020b,
  author        = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title         = {{Mask R-CNN}},
  year          = {2020},
  issn          = {19393539},
  number        = {2},
  pages         = {386--397},
  volume        = {42},
  abstract      = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  arxivid       = {1703.06870},
  doi           = {10.1109/TPAMI.2018.2844175},
  eprint        = {1703.06870},
  keywords      = {Instance segmentation,convolutional neural network,object detection,pose estimation},
  pmid          = {29994331},
}

@InProceedings{Simonyan2015,
  author        = {Simonyan, Karen and Zisserman, Andrew},
  booktitle     = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
  title         = {{Very deep convolutional networks for large-scale image recognition}},
  year          = {2015},
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  annote        = {_eprint: 1409.1556},
  archiveprefix = {arXiv},
  arxivid       = {1409.1556},
  eprint        = {1409.1556},
}

@InProceedings{art/TrulloR_2017,
  author    = {Trullo, R. and Petitjean, C. and Ruan, S. and Dubray, B. and Nie, D. and Shen, D.},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  title     = {{Segmentation of Organs at Risk in thoracic CT images using a SharpMask architecture and Conditional Random Fields}},
  year      = {2017},
  pages     = {1003--1006},
  volume    = {2017},
  abstract  = {Cancer is one of the leading causes of death worldwide. Radiotherapy is a standard treatment for this condition and the first step of the radiotherapy process is to identify the target volumes to be targeted and the healthy organs at risk (OAR) to be protected. Unlike previous methods for automatic segmentation of OAR that typically use local information and individually segment each OAR, in this paper, we propose a deep learning framework for the joint segmentation of OAR in CT images of the thorax, specifically the heart, esophagus, trachea and the aorta. Making use of Fully Convolutional Networks (FCN), we present several extensions that improve the performance, including a new architecture that allows to use low level features with high level information, effectively combining local and global information for improving the localization accuracy. Finally, by using Conditional Random Fields (specifically the CRF as Recurrent Neural Network model), we are able to account for relationships between the organs to further improve the segmentation results. Experiments demonstrate competitive performance on a dataset of 30 CT scans.},
  doi       = {10.1109/ISBI.2017.7950685},
  isbn      = {9781509011711},
  issn      = {19458452},
  keywords  = {CRF,CRFasRNN,CT Segmentation,Fully Convolutional Networks (FCN)},
}

@InProceedings{Berman2018,
  author        = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B.},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}},
  year          = {2018},
  pages         = {4413--4421},
  abstract      = {The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov{\~{A}}¡sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.},
  annote        = {_eprint: 1705.08790},
  archiveprefix = {arXiv},
  arxivid       = {1705.08790},
  doi           = {10.1109/CVPR.2018.00464},
  eprint        = {1705.08790},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@InProceedings{He2015a,
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
  year          = {2015},
  pages         = {1026--1034},
  volume        = {2015 Inter},
  abstract      = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.},
  annote        = {_eprint: 1502.01852},
  archiveprefix = {arXiv},
  arxivid       = {1502.01852},
  doi           = {10.1109/ICCV.2015.123},
  eprint        = {1502.01852},
  isbn          = {9781467383912},
  issn          = {15505499},
}

@InProceedings{Milletari2016,
  author        = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed Ahmad},
  booktitle     = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
  title         = {{V-Net: Fully convolutional neural networks for volumetric medical image segmentation}},
  year          = {2016},
  month         = {oct},
  pages         = {565--571},
  publisher     = {IEEE},
  abstract      = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
  annote        = {_eprint: 1606.04797},
  archiveprefix = {arXiv},
  arxivid       = {1606.04797},
  doi           = {10.1109/3DV.2016.79},
  eprint        = {1606.04797},
  isbn          = {9781509054077},
  keywords      = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
  url           = {http://ieeexplore.ieee.org/document/7785132/},
}

@InProceedings{Zhu2018,
  author        = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
  booktitle     = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
  title         = {{DeepLung: Deep 3D dual path nets for automated pulmonary nodule detection and classification}},
  year          = {2018},
  pages         = {673--681},
  volume        = {2018-Janua},
  abstract      = {In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and classification (classifying candidate nodules into benign or malignant). Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classification respectively. Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nodule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network features is proposed. The nodule classification subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved better performance than state-of-the-art approaches and surpassed the performance of experienced doctors based on image modality. Within the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate that DeepLung has performance comparable to experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
  annote        = {_eprint: 1801.09555},
  archiveprefix = {arXiv},
  arxivid       = {1801.09555},
  doi           = {10.1109/WACV.2018.00079},
  eprint        = {1801.09555},
  isbn          = {9781538648865},
}

@Article{Mingolla1989,
  author    = {Mingolla, Ennio and Bullock, Daniel},
  journal   = {Neural Networks},
  title     = {{Neurocomputing: Foundations of Research}},
  year      = {1989},
  issn      = {08936080},
  number    = {5},
  pages     = {405--409},
  volume    = {2},
  abstract  = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  address   = {Cambridge, MA, USA},
  annote    = {Section: Learning Representations by Back-propagating Errors},
  doi       = {10.1016/0893-6080(89)90025-7},
  editor    = {Anderson, James A and Rosenfeld, Edward},
  isbn      = {0-262-01097-6},
  publisher = {MIT Press},
  url       = {http://dl.acm.org/citation.cfm?id=65669.104451},
}

@Article{Thesis2018,
  author = {Thesis, Science and Appl, Ph D and Harvard, Math},
  title  = {{Beyond Regression : New Tools for Prediction and Analysis in the Behavioral}},
  year   = {2018},
  number = {January 1974},
}

@Article{Ruder2016,
  author        = {Ruder, Sebastian},
  journal       = {CoRR},
  title         = {{An overview of gradient descent optimization algorithms}},
  year          = {2016},
  volume        = {abs/1609.0},
  abstract      = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  annote        = {_eprint: 1609.04747},
  archiveprefix = {arXiv},
  arxivid       = {1609.04747},
  eprint        = {1609.04747},
  url           = {http://arxiv.org/abs/1609.04747},
}

@Article{Srivastava2014,
  author   = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal  = {Journal of Machine Learning Research},
  title    = {{Dropout: A simple way to prevent neural networks from overfitting}},
  year     = {2014},
  issn     = {15337928},
  pages    = {1929--1958},
  volume   = {15},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
  keywords = {Deep learning,Model combination,Neural networks,Regularization},
  url      = {http://jmlr.org/papers/v15/srivastava14a.html},
}

@InCollection{Theodoridis2015,
  author    = {Theodoridis, Sergios},
  booktitle = {Machine Learning},
  title     = {{Stochastic Gradient Descent}},
  year      = {2015},
  month     = {aug},
  pages     = {161--231},
  abstract  = {http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/},
  doi       = {10.1016/b978-0-12-801522-3.00005-7},
  url       = {https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants},
}

@InCollection{Bottou2012,
  author    = {Bottou, L{\'{e}}on},
  booktitle = {Neural Networks: Tricks of the Trade - Second Edition},
  title     = {{Stochastic Gradient Descent Tricks}},
  year      = {2012},
  pages     = {421--436},
  abstract  = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
  doi       = {10.1007/978-3-642-35289-8_25},
  url       = {https://doi.org/10.1007/978-3-642-35289-8_25},
}

@Article{Zhang2020b,
  author   = {Zhang, Liang and Zhang, Jiaming and Shen, Peiyi and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Zhang, Huan and Shah, Syed Afaq and Bennamoun, Mohammed},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Block Level Skip Connections across Cascaded V-Net for Multi-Organ Segmentation}},
  year     = {2020},
  issn     = {1558254X},
  number   = {9},
  pages    = {2782--2793},
  volume   = {39},
  abstract = {Multi-organ segmentation is a challenging task due to the label imbalance and structural differences between different organs. In this work, we propose an efficient cascaded V-Net model to improve the performance of multi-organ segmentation by establishing dense Block Level Skip Connections (BLSC) across cascaded V-Net. Our model can take full advantage of features from the first stage network and make the cascaded structure more efficient. We also combine stacked small and large kernels with an inception-like structure to help our model to learn more patterns, which produces superior results for multi-organ segmentation. In addition, some small organs are commonly occluded by large organs and have unclear boundaries with other surrounding tissues, which makes them hard to be segmented. We therefore first locate the small organs through a multi-class network and crop them randomly with the surrounding region, then segment them with a single-class network. We evaluated our model on SegTHOR 2019 challenge unseen testing set and Multi-Atlas Labeling Beyond the Cranial Vault challenge validation set. Our model has achieved an average dice score gain of 1.62 percents and 3.90 percents compared to traditional cascaded networks on these two datasets, respectively. For hard-to-segment small organs, such as the esophagus in SegTHOR 2019 challenge, our technique has achieved a gain of 5.63 percents on dice score, and four organs in Multi-Atlas Labeling Beyond the Cranial Vault challenge have achieved a gain of 5.27 percents on average dice score.},
  doi      = {10.1109/TMI.2020.2975347},
  keywords = {Multi-organ segmentation,cascaded network,hard-to-segment,inception-like structure,skip connections},
  pmid     = {32091995},
  url      = {https://ieeexplore.ieee.org/document/9006924},
}

@Article{Zhang2019e,
  author = {Zhang, Liang and Fan, Zhonghao and Li, Yuehan and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Afaq, Syed},
  title  = {{U-net based analysis of MRI for Alzheimer ' s disease diagnosis}},
  year   = {2019},
}

@Article{Zhao2018,
  author   = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and Li, Zhenye and Zhang, Yazhuo and Fan, Yong},
  journal  = {Medical Image Analysis},
  title    = {{A deep learning model integrating FCNNs and CRFs for brain tumor segmentation}},
  year     = {2018},
  issn     = {13618423},
  pages    = {98--111},
  volume   = {43},
  abstract = {Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.},
  doi      = {10.1016/j.media.2017.10.002},
  keywords = {Brain tumor segmentation,Conditional random fields,Deep learning,Fully convolutional neural networks},
  pmid     = {29040911},
  url      = {http://www.sciencedirect.com/science/article/pii/S136184151730141X},
}

@InProceedings{Khvostikov2017,
  author    = {Khvostikov, A. and Benois-Pineau, J. and Krylov, A. and Catheline, G.},
  booktitle = {GraphiCon 2017 - 27th International Conference on Computer Graphics and Vision},
  title     = {{Classification methods on different brain imaging modalities for Alzheimer disease studies}},
  year      = {2017},
  pages     = {237--242},
  abstract  = {Computer-aided early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. This paper reviews the major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Different fusion methodologies to combine heterogeneous image modalities to improve classification scores are also considered.},
  isbn      = {9785794429633},
  keywords  = {Alzheimer's Disease,Convolutional neural networks,Deep learning,Fusion,Machine Learning,Medical imaging,Mild cognitive impairment,Review},
}

@Article{Khvostikov2018,
  author        = {Khvostikov, Alexander and Aderghal, Karim and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
  journal       = {arXiv},
  title         = {{3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies}},
  year          = {2018},
  issn          = {23318422},
  month         = {jan},
  abstract      = {Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research. In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own algorithm for Alzheimer's Disease diagnostics based on a convolutional neural network and sMRI and DTI modalities fusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). Comparison with a single modality approach shows promising results. We also propose our own method of data augmentation for balancing classes of different size and analyze the impact of the ROI size on the classification results as well.
MSC Codes 68U10},
  annote        = {_eprint: 1801.05968},
  archiveprefix = {arXiv},
  arxivid       = {1801.05968},
  eprint        = {1801.05968},
  keywords      = {Alzheimers Disease,Convolutional Neural Networks,Deep learning,Image Fusion,Machine Learning,Medical Imaging,Mild Cognitive Impairment},
  url           = {http://arxiv.org/abs/1801.05968},
}

@InProceedings{Khagi2019,
  author    = {Khagi, Bijen and Lee, Chung Ghiu and Kwon, Goo Rak},
  booktitle = {BMEiCON 2018 - 11th Biomedical Engineering International Conference},
  title     = {{Alzheimer's disease Classification from Brain MRI based on transfer learning from CNN}},
  year      = {2019},
  abstract  = {Various Convolutional Neural Network (CNN) architecture has been proposed for image classification and Object recognition. For the image based classification, it is a complex task for CNN to deal with hundreds of MRI Image slices, each of almost identical nature in a single patient. So, classifying a number of patients as an AD, MCI or NC based on 3D MRI becomes vague technique using 2D CNN architecture. Hence, to address this issue, we have simplified the idea of classifying patients on basis of 3D MRI but acknowledging the 2D features generated from the CNN framework. We present our idea regarding how to obtain 2D features from MRI and transform it to be applicable to classify using machine learning algorithm. Our experiment shows the result of classifying 3 class subjects patients. We employed scratched trained CNN or pretrained Alexnet CNN as generic feature extractor of 2D image which dimensions were reduced using PCA+TSNE, and finally classifying using simple Machine learning algorithm like KNN, Navies Bayes Classifier. Although the result is not so impressive but it definitely shows that this can be better than scratch trained CNN softmax classification based on probability score. The generated feature can be well manipulated and refined for better accuracy, sensitivity, and specificity.},
  doi       = {10.1109/BMEiCON.2018.8609974},
  isbn      = {9781538657249},
  keywords  = {CNN,Classifier,Generic feature,MRI,PCA,TSNE},
}

@InProceedings{Cai2017a,
  author    = {Cai, Jinzheng and Lu, Le and Xie, Yuanpu and Xing, Fuyong and Yang, Lin},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Pancreas segmentation in MRI using graph-based decision fusion on convolutional neural networks}},
  year      = {2017},
  address   = {Cham},
  editor    = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
  pages     = {674--682},
  publisher = {Springer International Publishing},
  volume    = {10435 LNCS},
  abstract  = {Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.},
  doi       = {10.1007/978-3-319-66179-7_77},
  isbn      = {9783319661780},
  issn      = {16113349},
}

@InProceedings{Zhou2017,
  author        = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K. and Yuille, Alan L.},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{A fixed-point model for pancreas segmentation in abdominal CT scans}},
  year          = {2017},
  address       = {Cham},
  editor        = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
  pages         = {693--701},
  publisher     = {Springer International Publishing},
  volume        = {10433 LNCS},
  abstract      = {Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than 4%, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report 62.43% DSC in the worst case, which guarantees the reliability of our approach in clinical applications.},
  archiveprefix = {arXiv},
  arxivid       = {1612.08230},
  doi           = {10.1007/978-3-319-66182-7_79},
  eprint        = {1612.08230},
  isbn          = {9783319661810},
  issn          = {16113349},
}

@Article{Lu2017,
  author        = {Lu, Fang and Wu, Fa and Hu, Peijun and Peng, Zhiyi and Kong, Dexing},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title         = {{Automatic 3D liver location and segmentation via convolutional neural network and graph cut}},
  year          = {2017},
  issn          = {18616429},
  number        = {2},
  pages         = {171--182},
  volume        = {12},
  abstract      = {Purpose: Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans. Methods: The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural network; (ii) accuracy refinement of the initial segmentation with graph cut and the previously learned probability map. Results: The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb1. For the MICCAI-Sliver07 test dataset, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root-mean-square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9, 2.7 %, 0.91, 1.88 and 18.94 mm, respectively. For the 3Dircadb1 dataset, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36, 0.97 %, 1.89, 4.15 and 33.14 mm, respectively. Conclusions: The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and nonreproducible manual segmentation method.},
  archiveprefix = {arXiv},
  arxivid       = {1605.03012},
  doi           = {10.1007/s11548-016-1467-3},
  eprint        = {1605.03012},
  keywords      = {3D convolution neural network,CT images,Graph cut,Liver segmentation},
  pmid          = {27604760},
  url           = {https://doi.org/10.1007/s11548-016-1467-3},
}

@InProceedings{Dou2016,
  author        = {Dou, Qi and Chen, Hao and Jin, Yueming and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{3D deeply supervised network for automatic liver segmentation from CT volumes}},
  year          = {2016},
  address       = {Cham},
  editor        = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
  pages         = {149--157},
  publisher     = {Springer International Publishing},
  volume        = {9901 LNCS},
  abstract      = {Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper,we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly,we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties,and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN,a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.},
  archiveprefix = {arXiv},
  arxivid       = {1607.00582},
  doi           = {10.1007/978-3-319-46723-8_18},
  eprint        = {1607.00582},
  isbn          = {9783319467221},
  issn          = {16113349},
}

@Article{Naylor2019,
  author   = {Naylor, Peter and La{\'{e}}, Marick and Reyal, Fabien and Walter, Thomas},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map}},
  year     = {2019},
  issn     = {1558254X},
  number   = {2},
  pages    = {448--459},
  volume   = {38},
  abstract = {The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (HE) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.},
  doi      = {10.1109/TMI.2018.2865709},
  keywords = {Cancer research,deep learning,digital pathology,histopathology,nuclei segmentation},
  pmid     = {30716022},
}

@Article{Tofighi2019,
  author        = {Tofighi, Mohammad and Guo, Tiantong and Vanamala, Jairam K.P. and Monga, Vishal},
  journal       = {IEEE transactions on medical imaging},
  title         = {{Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection}},
  year          = {2019},
  issn          = {1558254X},
  month         = {sep},
  number        = {9},
  pages         = {2047--2058},
  volume        = {38},
  abstract      = {Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e., varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call shape priors (SPs) with CNNs (SPs-CNN). We further extend the network to introduce an SP layer and then allowing it to become trainable (i.e., optimizable). We call this network as tunable SP-CNN (TSP-CNN). In summary, we present new network structures that can incorporate "expected behavior" of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes and 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform the state-of-the-art alternatives.},
  archiveprefix = {arXiv},
  arxivid       = {1901.07061},
  doi           = {10.1109/TMI.2019.2895318},
  eprint        = {1901.07061},
  keywords      = {Biomedical imaging,Computer architecture,Deep learning,Image edge detection,Image segmentation,Microprocessors,Nucleus detection,Shape,TSP-CNN,biology computing,canonical cell nuclei shapes,cell nuclei detection,cell nucleus boundary,cell nucleus detection,cellular biophysics,cellular image quality,convolutional neural nets,convolutional neural networks,deep learning,deep learning methods,domain expert,fixed processing part,input images,labeled nuclei locations,learnable layers,learnable shapes,learning (artificial intelligence),medical image processing,morphological processing,multiple cell nuclei,network structures,nuclear morphology,nucleus shapes,regularization terms,shape priors,spatial processing,training set,tunable SP-CNN},
  pmid          = {30703016},
}

@Article{Song2019,
  author   = {Song, Jie and Xiao, Liang and Molaei, Mohsen and Lian, Zhichao},
  journal  = {Knowledge-Based Systems},
  title    = {{Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images}},
  year     = {2019},
  issn     = {09507051},
  pages    = {40--53},
  volume   = {176},
  abstract = {It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC)model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs)are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy.},
  doi      = {10.1016/j.knosys.2019.03.031},
  keywords = {Cascade classification,Multi-layer boosting sparse convolutional model,Nucleus segmentation,Probabilistic binary decision tree,Representation learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S095070511930156X},
}

@Article{JimenezCarretero2019,
  author   = {Jimenez-Carretero, Daniel and Bermejo-Pel{\'{a}}ez, David and Nardelli, Pietro and Fraga, Patricia and Fraile, Eduardo and {San Jos{\'{e}} Est{\'{e}}par}, Ra{\'{u}}l and Ledesma-Carbayo, Maria J.},
  journal  = {Medical Image Analysis},
  title    = {{A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images}},
  year     = {2019},
  issn     = {13618423},
  pages    = {144--159},
  volume   = {52},
  abstract = {Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (∼ 20%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.},
  doi      = {10.1016/j.media.2018.11.011},
  keywords = {Arteries,Artery-vein segmentation,Graph-cuts,Lung,Noncontrast CT,Phantoms,Random forest,Veins},
  pmid     = {30579223},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361841518308740},
}

@Article{Liu2019,
  author   = {Liu, Tianjiao and Guo, Qianqian and Lian, Chunfeng and Ren, Xuhua and Liang, Shujun and Yu, Jing and Niu, Lijuan and Sun, Weidong and Shen, Dinggang},
  journal  = {Medical Image Analysis},
  title    = {{Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks}},
  year     = {2019},
  issn     = {13618423},
  pages    = {101555},
  volume   = {58},
  abstract = {Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5% and 97.1%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.},
  doi      = {10.1016/j.media.2019.101555},
  keywords = {Clinical knowledge,Convolutional neural networks,Thyroid nodule,Ultrasound image},
  pmid     = {31520984},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361841519300970},
}

@Article{Jamaludin2017,
  author   = {Jamaludin, Amir and Kadir, Timor and Zisserman, Andrew},
  journal  = {Medical Image Analysis},
  title    = {{SpineNet: Automated classification and evidence visualization in spinal MRIs}},
  year     = {2017},
  issn     = {13618423},
  pages    = {63--73},
  volume   = {41},
  abstract = {The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.},
  doi      = {10.1016/j.media.2017.07.002},
  keywords = {MRI analysis,Radiological classification,Spinal MRI},
  pmid     = {28756059},
  url      = {http://www.sciencedirect.com/science/article/pii/S136184151730110X},
}

@Article{Wang2020,
  author   = {Wang, Zhiwei and Lin, Yi and Cheng, Kwang Ting (Tim) and Yang, Xin},
  journal  = {Medical Image Analysis},
  title    = {{Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization}},
  year     = {2020},
  issn     = {13618423},
  pages    = {101565},
  volume   = {59},
  abstract = {The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics.},
  doi      = {10.1016/j.media.2019.101565},
  keywords = {Deep learning,GAN,Generative models,Multimodal image synthesis},
  pmid     = {31630010},
  url      = {http://www.sciencedirect.com/science/article/pii/S1361841519301057},
}

@Article{Yi2019,
  author        = {Yi, Xin and Walia, Ekta and Babyn, Paul},
  journal       = {Medical Image Analysis},
  title         = {{Generative adversarial network in medical imaging: A review}},
  year          = {2019},
  issn          = {13618423},
  pages         = {101552},
  volume        = {58},
  abstract      = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
  archiveprefix = {arXiv},
  arxivid       = {1809.07294},
  doi           = {10.1016/j.media.2019.101552},
  eprint        = {1809.07294},
  keywords      = {Deep learning,Generative adversarial network,Generative model,Medical imaging,Review},
  pmid          = {31521965},
  url           = {http://www.sciencedirect.com/science/article/pii/S1361841518308430},
}

@Misc{Litjens2017,
  author        = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
  title         = {{A survey on deep learning in medical image analysis}},
  year          = {2017},
  abstract      = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
  archiveprefix = {arXiv},
  arxivid       = {1702.05747},
  booktitle     = {Medical Image Analysis},
  doi           = {10.1016/j.media.2017.07.005},
  eprint        = {1702.05747},
  issn          = {13618423},
  keywords      = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
  pages         = {60--88},
  pmid          = {28778026},
  url           = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
  volume        = {42},
}

@Article{Pereira2016,
  author   = {Pereira, Sergio and Pinto, Adriano and Alves, Victor and Silva, Carlos A.},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images}},
  year     = {2016},
  issn     = {1558254X},
  number   = {5},
  pages    = {1240--1251},
  volume   = {35},
  abstract = {Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.},
  doi      = {10.1109/TMI.2016.2538465},
  keywords = {Brain tumor,brain tumor segmentation,convolutional neural networks,deep learning,glioma,magnetic resonance imaging},
  pmid     = {26960222},
}

@Misc{Minati2009,
  author    = {Minati, Ludovico and Edginton, Trudi and {Grazia Bruzzone}, Maria and Giaccone, Giorgio},
  title     = {{Reviews: Current concepts in alzheimer's disease: A multidisciplinary review}},
  year      = {2009},
  abstract  = {This comprehensive, pedagogically-oriented review is aimed at a heterogeneous audience representative of the allied disciplines involved in research and patient care. After a foreword on epidemiology, genetics, and risk factors, the amyloid cascade model is introduced and the main neuropathological hallmarks are discussed. The progression of memory, language, visual processing, executive, attentional, and praxis deficits, and of behavioral symptoms is presented. After a summary on neuropsychological assessment, emerging biomarkers from cerebrospinal fluid assays, magnetic resonance imaging, nuclear medicine, and electrophysiology are discussed. Existing treatments are briefly reviewed, followed by an introduction to emerging disease-modifying therapies such as secretase modulators, inhibitors of Abeta aggregation, immunotherapy, inhibitors of tau protein phosphorylation, and delivery of nerve growth factor. {\textcopyright} 2009 Sage Publications.},
  booktitle = {American Journal of Alzheimer's Disease and other Dementias},
  doi       = {10.1177/1533317508328602},
  issn      = {15333175},
  keywords  = {Alzheimer's disease,Neuroimaging,Neuropathology,Neuropsychological testing,Pharmacotherapy},
  number    = {2},
  pages     = {95--121},
  pmid      = {19116299},
  volume    = {24},
}

@Article{Ju2015,
  author   = {Ju, Wei and Xiang, Deihui and Zhang, Bin and Wang, Lirong and Kopriva, Ivica and Chen, Xinjian},
  journal  = {IEEE Transactions on Image Processing},
  title    = {{Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images}},
  year     = {2015},
  issn     = {10577149},
  number   = {12},
  pages    = {5854--5867},
  volume   = {24},
  abstract = {Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
  doi      = {10.1109/TIP.2015.2488902},
  keywords = {Computed Tomography (CT),Positron Emission Tomography (PET),graph cut,image segmentation,interactive segmentation,lung tumor,prior information,random walk},
  pmid     = {26462198},
}

@InProceedings{Tran2015,
  author        = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Learning spatiotemporal features with 3D convolutional networks}},
  year          = {2015},
  pages         = {4489--4497},
  volume        = {2015 Inter},
  abstract      = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
  annote        = {_eprint: 1412.0767},
  archiveprefix = {arXiv},
  arxivid       = {1412.0767},
  doi           = {10.1109/ICCV.2015.510},
  eprint        = {1412.0767},
  isbn          = {9781467383912},
  issn          = {15505499},
  keywords      = {Computer Science - Computer Vision and Pattern Rec},
}

@InProceedings{Han2011,
  author    = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Globally optimal tumor segmentation in PET-CT images: A graph-based co-segmentation method}},
  year      = {2011},
  pages     = {245--256},
  volume    = {6801 LNCS},
  abstract  = {Tumor segmentation in PET and CT images is notoriously challenging due to the low spatial resolution in PET and low contrast in CT images. In this paper, we have proposed a general framework to use both PET and CT images simultaneously for tumor segmentation. Our method utilizes the strength of each imaging modality: the superior contrast of PET and the superior spatial resolution of CT. We formulate this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized term that penalizes the segmentation difference between PET and CT. Our method simulates the clinical practice of delineating tumor simultaneously using both PET and CT, and is able to concurrently segment tumor from both modalities, achieving globally optimal solutions in low-order polynomial time by a single maximum flow computation. The method was evaluated on clinically relevant tumor segmentation problems. The results showed that our method can effectively make use of both PET and CT image information, yielding segmentation accuracy of 0.85 in Dice similarity coefficient and the average median hausdorff distance (HD) of 6.4 mm, which is 10 % (resp., 16 %) improvement compared to the graph cuts method solely using the PET (resp., CT) images. {\textcopyright} 2011 Springer-Verlag.},
  doi       = {10.1007/978-3-642-22092-0_21},
  isbn      = {9783642220913},
  issn      = {03029743},
  pmid      = {21761661},
}

@InProceedings{Zhong2018,
  author    = {Zhong, Zisha and Kim, Yusung and Zhou, Leixin and Plichta, Kristin and Allen, Bryan and Buatti, John and Wu, Xiaodong},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  title     = {{3D fully convolutional networks for co-segmentation of tumors on PET-CT images}},
  year      = {2018},
  month     = {apr},
  pages     = {228--231},
  volume    = {2018-April},
  abstract  = {Positron emission tomography and computed tomography (PET-CT) dual-modality imaging provides critical diagnostic information in modern cancer diagnosis and therapy. Automated accurate tumor delineation is essentially important in computer-assisted tumor reading and interpretation based on PET-CT. In this paper, we propose a novel approach for the segmentation of lung tumors that combines the powerful fully convolutional networks (FCN) based semantic segmentation framework (3D-UNet) and the graph cut based co-segmentation model. First, two separate deep UNets are trained on PET and CT, separately, to learn high level discriminative features to generate tumor/non-tumor masks and probability maps for PET and CT images. Then, the two probability maps on PET and CT are further simultaneously employed in a graph cut based co-segmentation model to produce the final tumor segmentation results. Comparative experiments on 32 PET-CT scans of lung cancer patients demonstrate the effectiveness of our method.},
  annote    = {ISSN: 1945-8452},
  doi       = {10.1109/ISBI.2018.8363561},
  isbn      = {9781538636367},
  issn      = {19458452},
  keywords  = {Co-segmentation,Deep learning,Fully convolutional networks,Image segmentation,Lung tumor segmentation},
}

@InProceedings{Zhong2017b,
  author    = {Zhong, Zisha and Kim, Yusung and Buatti, John and Wu, Xiaodong},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{3D alpha matting based co-segmentation of tumors on PET-CT images}},
  year      = {2017},
  pages     = {31--42},
  volume    = {10555 LNCS},
  abstract  = {Positron emission tomography – computed tomography (PET-CT) has been widely used in modern cancer imaging. Accurate tumor delineation from PET and CT plays an important role in radiation therapy. The PET-CT co-segmentation technique, which makes use of advantages of both modalities, has achieved impressive performance for tumor delineation. In this work, we propose a novel 3D image matting based semi-automated co-segmentation method for tumor delineation on dual PET-CT scans. The “matte” values generated by 3D image matting are employed to compute the region costs for the graph based co-segmentation. Compared to previous PET-CT co-segmentation methods, our method is completely data-driven in the design of cost functions, thus using much less hyper-parameters in our segmentation model. Comparative experiments on 54 PET-CT scans of lung cancer patients demonstrated the effectiveness of our method.},
  doi       = {10.1007/978-3-319-67564-0_4},
  isbn      = {9783319675633},
  issn      = {16113349},
  keywords  = {Co-segmentation,Image matting,Image segmentation,Interactive segmentation,Lung tumor segmentation},
}

@Article{Song2013,
  author   = {Song, Qi and Bai, Junjie and Han, Dongfeng and Bhatia, Sudershan and Sun, Wenqing and Rockey, William and Bayouth, John E. and Buatti, John M. and Wu, Xiaodong},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{Optimal Co-segmentation of tumor in PET-CT images with context information}},
  year     = {2013},
  issn     = {02780062},
  month    = {sep},
  number   = {9},
  pages    = {1685--1697},
  volume   = {32},
  abstract = {Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT. {\textcopyright} 2012 IEEE.},
  doi      = {10.1109/TMI.2013.2263388},
  keywords = {Context information,Positron emission tomography-computed tomography (,global optimization,graph cut,image segmentation,lung tumor},
  pmid     = {23693127},
}

@Article{Mahmood2018,
  author        = {Mahmood, Faisal and Yang, Ziyun and Ashley, Thomas and Durr, Nicholas J.},
  journal       = {arXiv},
  title         = {{Multimodal densenet}},
  year          = {2018},
  issn          = {23318422},
  abstract      = {Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multimodal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.},
  annote        = {_eprint: 1811.07407},
  archiveprefix = {arXiv},
  arxivid       = {1811.07407},
  eprint        = {1811.07407},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning},
  url           = {http://arxiv.org/abs/1811.07407},
}

@Article{Dolz2019,
  author        = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and {Ben Ayed}, Ismail},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation}},
  year          = {2019},
  issn          = {1558254X},
  number        = {5},
  pages         = {1116--1126},
  volume        = {38},
  abstract      = {Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.},
  annote        = {_eprint: 1804.02967},
  archiveprefix = {arXiv},
  arxivid       = {1804.02967},
  doi           = {10.1109/TMI.2018.2878669},
  eprint        = {1804.02967},
  keywords      = {3-D CNN,Deep learning,brain MRI,multi-modal imaging,segmentation},
  pmid          = {30387726},
  url           = {http://arxiv.org/abs/1804.02967},
}

@Article{Kumar2020b,
  author        = {Kumar, Ashnil and Fulham, Michael and Feng, Dagan and Kim, Jinman},
  journal       = {IEEE Transactions on Medical Imaging},
  title         = {{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}},
  year          = {2020},
  issn          = {1558254X},
  number        = {1},
  pages         = {204--217},
  volume        = {39},
  abstract      = {The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29%, p < {0.05) than the fusion baselines (FS: 99.00%, MB: 99.08%, and TC: 98.92%) and a significantly higher Dice score (63.85%) than the recent PET-CT tumor segmentation methods.}},
  annote        = {_eprint: 1810.02492},
  archiveprefix = {arXiv},
  arxivid       = {1810.02492},
  doi           = {10.1109/TMI.2019.2923601},
  eprint        = {1810.02492},
  keywords      = {Multi-modality imaging,PET-CT,deep learning,fusion learning},
  pmid          = {31217099},
}

@Article{John2001,
  author   = {lafferty John and Andrew, McCallum and Fernando, C.N. Pereira},
  journal  = {ICML '01: Proceedings of the Eighteenth International Conference on Machine Learning},
  title    = {{Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data}},
  year     = {2001},
  issn     = {1410-3680},
  number   = {June},
  pages    = {282--289},
  volume   = {2001},
  abstract = {We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.},
  doi      = {10.29122/mipi.v11i1.2792},
  url      = {https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers},
}

@Article{Cortes1995,
  author   = {Cortes, Corinna and Vapnik, Vladimir},
  journal  = {Machine Learning},
  title    = {{Support-vector networks}},
  year     = {1995},
  issn     = {0885-6125},
  number   = {3},
  pages    = {273--297},
  volume   = {20},
  abstract = {In this paper, the optimal margin algorithm is generalized\nto non-separable problems by the introduction of slack\nvariables in the statement of the optimization problem.},
  doi      = {10.1007/bf00994018},
}

@InProceedings{Boser1992,
  author    = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  booktitle = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
  title     = {{Training algorithm for optimal margin classifiers}},
  year      = {1992},
  pages     = {144--152},
  publisher = {ACM Press},
  abstract  = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  doi       = {10.1145/130385.130401},
  isbn      = {089791497X},
}

@Article{McCulloch1943,
  author   = {McCulloch, Warren S. and Pitts, Walter},
  journal  = {The Bulletin of Mathematical Biophysics},
  title    = {{A logical calculus of the ideas immanent in nervous activity}},
  year     = {1943},
  issn     = {00074985},
  number   = {4},
  pages    = {115--133},
  volume   = {5},
  abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
  doi      = {10.1007/BF02478259},
  url      = {https://doi.org/10.1007/BF02478259},
}

@Book{Haykin2008,
  author     = {Haykin, Simon},
  publisher  = {China Machine Press and Pearson Education},
  title      = {{Neural Networks and Learning Machines}},
  year       = {2008},
  isbn       = {9780131471399},
  volume     = {3},
  abstract   = {Fluid and authoritative, this well-organized book represents the first comprehensive treatment of neural networks from an engineering perspective, providing extensive, state-of-the-art coverage that will expose readers to the myriad facets of neural networks and help them appreciate the technology's origin, capabilities, and potential applications.Examines all the important aspects of this emerging technolgy, covering the learning process, back propogation, radial basis functions, recurrent networks, self-organizing systems, modular networks, temporal processing, neurodynamics, and VLSI implementation. Integrates computer experiments throughout to demonstrate how neural networks are designed and perform in practice. Chapter objectives, problems, worked examples, a bibliography, photographs, illustrations, and a thorough glossary all reinforce concepts throughout. New chapters delve into such areas as support vector machines, and reinforcement learning/neurodynamic programming, plus readers will find an entire chapter of case studies to illustrate the real-life, practical applications of neural networks. A highly detailed bibliography is included for easy reference.For professional engineers and research scientists.},
  booktitle  = {Pearson Prentice Hall New Jersey USA 936 pLinks},
  issn       = {14337851},
  keywords   = {Neural Network, Machines Learning},
  pages      = {906},
  translator = {Shen, Furao and Xu, Ye and Zheng, Jun and Chao, Jin},
  url        = {https://www.pearson.com/store/p/neural-networks-and-learning-machines/P100001415658},
}

@Article{Fowler2000,
  author        = {Fowler, Bridget},
  journal       = {Theory, Culture and Society},
  title         = {{A sociological analysis of the satanic verses affair}},
  year          = {2000},
  issn          = {02632764},
  month         = {may},
  number        = {1},
  pages         = {39--61},
  volume        = {17},
  abstract      = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
  archiveprefix = {arXiv},
  arxivid       = {0-387-31073-8},
  doi           = {10.1177/02632760022050997},
  eprint        = {0-387-31073-8},
  isbn          = {013805326X},
  pmid          = {18190633},
  school        = {Department of Computer Science National Taiwan University, Taipei 106, Taiwan},
  url           = {http://www.csie.ntu.edu.tw/%7B$\sim$%7Dcjlin/papers/guide/guide.pdf},
}

@Book{Flach2012,
  author     = {Flach, Peter},
  publisher  = {Posts Telecom Press and Cambridge University Press},
  title      = {{Machine Learning: The Art and Science of Algorithms That Make Sense of Data}},
  year       = {2012},
  isbn       = {1107422221, 9781107422223},
  abstract   = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
  translator = {Duan, Fei},
}

@Article{art/WangY_2019,
  author        = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
  journal       = {ACM Transactions on Graphics},
  title         = {{Dynamic graph Cnn for learning on point clouds}},
  year          = {2019},
  issn          = {15577368},
  number        = {5},
  volume        = {38},
  abstract      = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
  annote        = {_eprint: 1801.07829},
  archiveprefix = {arXiv},
  arxivid       = {1801.07829},
  doi           = {10.1145/3326362},
  eprint        = {1801.07829},
  keywords      = {Classification,Point cloud,Segmentation},
  url           = {http://arxiv.org/abs/1801.07829},
}

@InProceedings{Simonovsky2017,
  author        = {Simonovsky, Martin and Komodakis, Nikos},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{Dynamic edge-conditioned filters in convolutional neural networks on graphs}},
  year          = {2017},
  pages         = {29--38},
  volume        = {2017-Janua},
  abstract      = {A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.},
  annote        = {_eprint: 1704.02901},
  archiveprefix = {arXiv},
  arxivid       = {1704.02901},
  doi           = {10.1109/CVPR.2017.11},
  eprint        = {1704.02901},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1704.02901},
}

@InProceedings{Kipf2017,
  author        = {Kipf, Thomas N. and Welling, Max},
  booktitle     = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  title         = {{Semi-supervised classification with graph convolutional networks}},
  year          = {2017},
  volume        = {abs/1609.0},
  abstract      = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  annote        = {_eprint: 1609.02907},
  archiveprefix = {arXiv},
  arxivid       = {1609.02907},
  eprint        = {1609.02907},
  url           = {http://arxiv.org/abs/1609.02907},
}

@InCollection{Ronneberger2015,
  author        = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{U-net: Convolutional networks for biomedical image segmentation}},
  year          = {2015},
  isbn          = {9783319245737},
  pages         = {234--241},
  volume        = {9351},
  abstract      = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  annote        = {_eprint: 1505.04597},
  archiveprefix = {arXiv},
  arxivid       = {1505.04597},
  doi           = {10.1007/978-3-319-24574-4_28},
  eprint        = {1505.04597},
  issn          = {16113349},
  url           = {http://link.springer.com/10.1007/978-3-319-24574-4_28},
}

@InProceedings{Szegedy2017,
  author        = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  booktitle     = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
  title         = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
  year          = {2017},
  pages         = {4278--4284},
  abstract      = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  annote        = {_eprint: 1602.07261},
  archiveprefix = {arXiv},
  arxivid       = {1602.07261},
  eprint        = {1602.07261},
}

@Misc{Andrew1999,
  author    = {Andrew, Alex M.},
  title     = {{The Handbook of Brain Theory and Neural Networks}},
  year      = {1999},
  address   = {Cambridge, MA, USA},
  annote    = {Section: Convolutional Networks for Images, Speech, and Time Series},
  booktitle = {Kybernetes},
  doi       = {10.1108/k.1999.28.9.1084.1},
  editor    = {Arbib, Michael A},
  isbn      = {0-262-51102-9},
  issn      = {0368492X},
  keywords  = {Artificial intelligence,Brain,Cybernetics,Neural networks,Publication},
  number    = {9},
  pages     = {1084--1094},
  publisher = {MIT Press},
  url       = {http://dl.acm.org/citation.cfm?id=303568.303704},
  volume    = {28},
}

@Misc{Bronstein2017,
  author        = {Bronstein, Michael M. and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  title         = {{Geometric Deep Learning: Going beyond Euclidean data}},
  year          = {2017},
  abstract      = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
  annote        = {_eprint: 1611.08097},
  archiveprefix = {arXiv},
  arxivid       = {1611.08097},
  booktitle     = {IEEE Signal Processing Magazine},
  doi           = {10.1109/MSP.2017.2693418},
  eprint        = {1611.08097},
  issn          = {10535888},
  number        = {4},
  pages         = {18--42},
  url           = {http://arxiv.org/abs/1611.08097},
  volume        = {34},
}

@InProceedings{Huang2017,
  author        = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{Densely connected convolutional networks}},
  year          = {2017},
  pages         = {2261--2269},
  volume        = {2017-Janua},
  abstract      = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  annote        = {_eprint: 1608.06993},
  archiveprefix = {arXiv},
  arxivid       = {1608.06993},
  doi           = {10.1109/CVPR.2017.243},
  eprint        = {1608.06993},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1608.06993},
}

@Article{Kuo2016,
  author        = {Kuo, C. C.Jay},
  journal       = {Journal of Visual Communication and Image Representation},
  title         = {{Understanding convolutional neural networks with a mathematical model}},
  year          = {2016},
  issn          = {10959076},
  pages         = {406--413},
  volume        = {41},
  abstract      = {This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): (1) why a nonlinear activation function is essential at the filter output of all intermediate layers? (2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the “REctified-COrrelations on a Sphere” (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multilayer system with the AlexNet as an example.},
  annote        = {_eprint: 1609.04112},
  archiveprefix = {arXiv},
  arxivid       = {1609.04112},
  doi           = {10.1016/j.jvcir.2016.11.003},
  eprint        = {1609.04112},
  keywords      = {Convolutional neural network (CNN),MNIST dataset,Nonlinear activation,RECOS model,Rectified linear unit (ReLU)},
  url           = {http://arxiv.org/abs/1609.04112},
}

@Article{Singanamalli2016,
  author   = {Singanamalli, Asha and Rusu, Mirabela and Sparks, Rachel E. and Shih, Natalie N.C. and Ziober, Amy and Wang, Li Ping and Tomaszewski, John and Rosen, Mark and Feldman, Michael and Madabhushi, Anant},
  journal  = {Journal of Magnetic Resonance Imaging},
  title    = {{Identifying in vivo DCE MRI markers associated with microvessel architecture and gleason grades of prostate cancer}},
  year     = {2016},
  issn     = {15222586},
  number   = {1},
  pages    = {149--158},
  volume   = {43},
  abstract = {Background To identify computer extracted in vivo dynamic contrast enhanced (DCE) MRI markers associated with quantitative histomorphometric (QH) characteristics of microvessels and Gleason scores (GS) in prostate cancer. Methods This study considered retrospective data from 23 biopsy confirmed prostate cancer patients who underwent 3 Tesla multiparametric MRI before radical prostatectomy (RP). Representative slices from RP specimens were stained with vascular marker CD31. Tumor extent was mapped from RP sections onto DCE MRI using nonlinear registration methods. Seventy-seven microvessel QH features and 18 DCE MRI kinetic features were extracted and evaluated for their ability to distinguish low from intermediate and high GS. The effect of temporal sampling on kinetic features was assessed and correlations between those robust to temporal resolution and microvessel features discriminative of GS were examined. Results A total of 12 microvessel architectural features were discriminative of low and intermediate/high grade tumors with area under the receiver operating characteristic curve (AUC) > 0.7. These features were most highly correlated with mean washout gradient (WG) (max rho = -0.62). Independent analysis revealed WG to be moderately robust to temporal resolution (intraclass correlation coefficient [ICC] = 0.63) and WG variance, which was poorly correlated with microvessel features, to be predictive of low grade tumors (AUC = 0.77). Enhancement ratio was the most robust (ICC = 0.96) and discriminative (AUC = 0.78) kinetic feature but was moderately correlated with microvessel features (max rho = -0.52). Conclusion Computer extracted features of prostate DCE MRI appear to be correlated with microvessel architecture and may be discriminative of low versus intermediate and high GS.},
  annote   = {_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.24975},
  doi      = {10.1002/jmri.24975},
  keywords = {DCE MRI,Gleason grades,imaging biomarkers,microvessel architecture,prostate cancer,quantitative histomorphometry},
  pmid     = {26110513},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.24975},
}

@Article{Rusu2017,
  author   = {Rusu, Mirabela and Rajiah, Prabhakar and Gilkeson, Robert and Yang, Michael and Donatelli, Christopher and Thawani, Rajat and Jacono, Frank J. and Linden, Philip and Madabhushi, Anant},
  journal  = {European Radiology},
  title    = {{Co-registration of pre-operative CT with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study}},
  year     = {2017},
  issn     = {14321084},
  number   = {10},
  pages    = {4209--4217},
  volume   = {27},
  abstract = {Objective: To develop an approach for radiology-pathology fusion of ex vivo histology of surgically excised pulmonary nodules with pre-operative CT, to radiologically map spatial extent of the invasive adenocarcinomatous component of the nodule. Methods: Six subjects (age: 75 ± 11 years) with pre-operative CT and surgically excised ground-glass nodules (size: 22.5 ± 5.1 mm) with a significant invasive adenocarcinomatous component (>5 mm) were included. The pathologist outlined disease extent on digitized histology specimens; two radiologists and a pulmonary critical care physician delineated the entire nodule on CT (in-plane resolution: <0.8 mm, inter-slice distance: 1–5 mm). We introduced a novel reconstruction approach to localize histology slices in 3D relative to each other while using CT scan as spatial constraint. This enabled the spatial mapping of the extent of tumour invasion from histology onto CT. Results: Good overlap of the 3D reconstructed histology and the nodule outlined on CT was observed (65.9 ± 5.2%). Reduction in 3D misalignment of corresponding anatomical landmarks on histology and CT was observed (1.97 ± 0.42 mm). Moreover, the CT attenuation (HU) distributions were different when comparing invasive and in situ regions. Conclusion: This proof-of-concept study suggests that our fusion method can enable the spatial mapping of the invasive adenocarcinomatous component from 2D histology slices onto in vivo CT. Key Points: • 3D reconstructions are generated from 2D histology specimens of ground glass nodules. • The reconstruction methodology used pre-operative in vivo CT as 3D spatial constraint. • The methodology maps adenocarcinoma extent from digitized histology onto in vivo CT. • The methodology potentially facilitates the discovery of CT signature of invasive adenocarcinoma.},
  doi      = {10.1007/s00330-017-4813-0},
  keywords = {Computed tomography,Computer-assisted image processing,Lung adenocarcinoma,Multimodal imaging,Pathology},
  pmid     = {28386717},
  url      = {https://doi.org/10.1007/s00330-017-4813-0},
}

@Article{Bilic1a2019,
  author        = {Bilic1a, Patrick and Christa, Patrick Ferdinand and Vorontsov, Eugene and Chlebusr, Grzegorz and Chenm, Hao and Doum, Qi and Fum, Chi Wing and Hanp, Xiao and Hengm, Pheng Ann and Hesserq, Jrgen and Kadourye, Samuel and Kopczyskiv, Tomasz and Leo, Miao and Lio, Chunming and Lim, Xiaomeng and Lipkova, Jana and Lowengrubn, John and Meiner, Hans and Moltzr, Jan Hendrik and Pale, Chris and Pirauda, Marie and Qim, Xiaojuan and Qil, Jin and Rempera, Markus and Rothq, Karsten and Schenkr, Andrea and Sekuboyinaa, Anjany and Zhouk, Ping and Hulsemeyera, Christian and Beetza, Marcel and Ettlingera, Florian and Gruena, Felix and Kaissisb, Georgios and Lohferb, Fabian and Brarenb, Rickmer and Holchc, Julian and Hofmannc, Felix and Sommerc, Wieland and Heinemannc, Volker and Jacobsd, Colin and Mamanid, Gabriel Efrain Humpire and Ginnekend, Bram Van and Chartrande, Gabriel and Tange, An and Drozdzale, Michal and Kadourye, Samuel and Ben-Cohenf, Avi and Klangf, Eyal and Amitaif, Marianne M. and Konenf, Eli and Greenspanf, Hayit and Moreaug, Johan and Hostettlerg, Alexandre and Solerg, Luc and Vivantih, Refael and Szeskinh, Adi and Lev-Cohainh, Naama and Sosnah, Jacob and Joskowiczh, Leo and Kumarw, Ashnil and Korex, Avinash and Wangy, Chunliang and Fengz, Dagan and Liaa, Fan and Krishnamurthix, Ganapathy and Heab, Jian and Wuaa, Jianrong and Kimx, Jinman and Zhouac, Jinyi and Maad, Jun and Liaa, Junbo and Maninisae, Kevis Kokitsi and Kaluvax, Krishna Chaitanya and Bix, Lei and Khenedx, Mahendra and Beliverae, Miriam and Linaa, Qizhong and Yangad, Xiaoping and Yuanaf, Yading and Chenaa, Yinan and Liad, Yuanqiang and Qius, Yudong and Wuad, Yuli and Menzea, Bjoern},
  journal       = {arXiv},
  title         = {{The liver tumor segmentation benchmark (LiTS)}},
  year          = {2019},
  issn          = {23318422},
  volume        = {abs/1901.0},
  abstract      = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LiTS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 and International Conference On Medical Image Computing & Computer Assisted Intervention (MICCAI) 2017. Twenty-four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually blind reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LiTS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
  annote        = {_eprint: 1901.04056},
  archiveprefix = {arXiv},
  arxivid       = {1901.04056},
  eprint        = {1901.04056},
  keywords      = {Benchmark.,CT,Liver,Medical imaging,Segmentation,Tumor},
  url           = {http://arxiv.org/abs/1901.04056},
}

@Article{Heller2019,
  author        = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
  journal       = {arXiv},
  title         = {{The KiTS19 challenge data: 300 Kidney tumor cases with clinical context, Ct semantic segmentations, and surgical outcomes}},
  year          = {2019},
  abstract      = {Characterization of the relationship between a kidney tumor's appearance on cross-sectional imaging and it's treatment outcomes is a promising direction for informing treatement decisions and improving patient outcomes. Unfortunately, the rigorous study of tumor morphology is limited by the laborious and noisy process of making manual radiographic measurements. Semantic segmentation of the tumor and surrounding organ offers a precise quantitative description of that morphology, but it too requires significant manual effort. A large publicly available dataset of high-fidelity semantic segmentations along with clinical context and treatment outcomes could accelerate not only the study of how morphology relates to outcomes, but also the development of automatic semantic segmentation systems which could enable such studies on unprecedented scales. We present the KiTS19 challenge dataset: a collection of segmented CT imaging and treatment outcomes for 300 patients treated with partial or radical nephrectomy between 2010 and 2018. 210 of these cases have been released publicly and the remaining 90 remain private for the objective evaluation of prediction systems developed using the public cases.},
  annote        = {_eprint: 1904.00445},
  archiveprefix = {arXiv},
  arxivid       = {1904.00445},
  eprint        = {1904.00445},
  keywords      = {Kidney Tumors,Nephrometry,Semantic Segmentation},
  url           = {http://arxiv.org/abs/1904.00445},
}

@Article{Rister2018,
  author        = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L.},
  journal       = {arXiv},
  title         = {{CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss}},
  year          = {2018},
  issn          = {23318422},
  volume        = {abs/1811.1},
  abstract      = {Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm3 CT volumes, our GPU implementation is 2.6-8×faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92% for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.},
  annote        = {_eprint: 1811.11226},
  archiveprefix = {arXiv},
  arxivid       = {1811.11226},
  eprint        = {1811.11226},
  keywords      = {Computed tomography (CT),Computer vision,Data augmentation,Deep learning,Image segmentation},
  url           = {http://arxiv.org/abs/1811.11226},
}

@Article{Mueller2005,
  author   = {Mueller, Susanne G. and Weiner, Michael W. and Thal, Leon J. and Petersen, Ronald C. and Jack, Clifford and Jagust, William and Trojanowski, John Q. and Toga, Arthur W. and Beckett, Laurel},
  journal  = {Neuroimaging Clinics of North America},
  title    = {{The Alzheimer's disease neuroimaging initiative}},
  year     = {2005},
  issn     = {10525149},
  number   = {4},
  pages    = {869--877},
  volume   = {15},
  abstract = {With increasing life expectancy in developed countries, the incidence of Alzheimer's disease (AD) and its socioeconomic impact are growing. Increasing knowledge of the mechanisms of AD facilitates the development of treatment strategies aimed at slowing down or preventing neuronal death. AD treatment trials using clinical outcome measures require long observation times and large patient samples. There is increasing evidence that neuroimaging and cerebrospinal fluid and blood biomarkers may provide information that may reduce sample sizes and observation periods. The Alzheimer's Disease Neuroimaging Initiative will help identify clinical, neuroimaging, and biomarker outcome measures that provide the highest power for measurement of longitudinal changes and for prediction of transitions. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
  doi      = {10.1016/j.nic.2005.09.008},
  pmid     = {16443497},
}

@InProceedings{Vahadane2016a,
  author    = {Vahadane, Abhishek and Kumar, Neeraj and Sethi, Amit},
  booktitle = {Proceedings - International Symposium on Biomedical Imaging},
  title     = {{Learning based super-resolution of histological images}},
  year      = {2016},
  month     = {apr},
  pages     = {816--819},
  volume    = {2016-June},
  abstract  = {For better perception and analysis of images, good quality and high resolution (HR) are always preferred over degraded and low resolution (LR) images. Getting HR images can be cost and time prohibitive. Super resolution (SR) techniques can be an affordable alternative for small zoom factors. In medical imaging, specifically in the case of histological images, estimating an HR image from an LR one requires preservation of complex textures and edges defining various biological features (nuclei, cytoplasm etc.). This challenge is further aggravated by the scale variance of histological images that are taken of a flat biopsy slide instead of a 3D world. We propose an algorithm for SR of histological images that learns a mapping from zero-phase component analysis (ZCA)-whitened LR patches to ZCA-whitened HR patches at the desired scale. ZCA-whitening exploits the redundancy in data and enhances the texture and edges energies to better learn the desired LR to HR mapping, which we learn using a neural network. The qualitative and quantitative validation shows that improvements in HR estimation by proposed algorithm are statistically significant over benchmark learning-based SR algorithms.},
  doi       = {10.1109/ISBI.2016.7493391},
  isbn      = {9781479923502},
  issn      = {19458452},
  keywords  = {Image super-resolution,histological image,neural network},
}

@Article{Kumar2017,
  author   = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
  journal  = {IEEE Transactions on Medical Imaging},
  title    = {{A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology}},
  year     = {2017},
  issn     = {1558254X},
  month    = {jul},
  number   = {7},
  pages    = {1550--1560},
  volume   = {36},
  abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (HE)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other HE-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
  doi      = {10.1109/TMI.2017.2677499},
  keywords = {Annotation,boundaries,dataset,deep learning,nuclear segmentation,nuclei},
  pmid     = {28287963},
}

@Article{Armeni2017,
  author        = {Armeni, Iro and Sax, Sasha and Zamir, Amir Roshan and Savarese, Silvio},
  journal       = {CoRR},
  title         = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
  year          = {2017},
  volume        = {abs/1702.0},
  abstract      = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\deg} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
  annote        = {_eprint: 1702.01105},
  archiveprefix = {arXiv},
  arxivid       = {1702.01105},
  eprint        = {1702.01105},
  keywords      = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Robotics},
  url           = {http://arxiv.org/abs/1702.01105},
}

@InProceedings{Hackel2017,
  author        = {Hackel, T. and Savinov, N. and Ladicky, L. and Wegner, J. D. and Schindler, K. and Pollefeys, M.},
  booktitle     = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  title         = {{Semantic3D.Net: a New Large-Scale Point Cloud Classification Benchmark}},
  year          = {2017},
  number        = {1W1},
  pages         = {91--98},
  volume        = {4},
  abstract      = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
  archiveprefix = {arXiv},
  arxivid       = {1704.03847},
  doi           = {10.5194/isprs-annals-IV-1-W1-91-2017},
  eprint        = {1704.03847},
  issn          = {21949050},
}

@Article{Chang2015,
  author        = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
  journal       = {CoRR},
  title         = {{ShapeNet: An Information-Rich 3D Model Repository}},
  year          = {2015},
  volume        = {abs/1512.0},
  abstract      = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
  annote        = {_eprint: 1512.03012},
  archiveprefix = {arXiv},
  arxivid       = {1512.03012},
  eprint        = {1512.03012},
  url           = {http://arxiv.org/abs/1512.03012},
}

@Article{Geiger2013,
  author   = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
  journal  = {International Journal of Robotics Research},
  title    = {{Vision meets robotics: The KITTI dataset}},
  year     = {2013},
  issn     = {02783649},
  number   = {11},
  pages    = {1231--1237},
  volume   = {32},
  abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
  doi      = {10.1177/0278364913491297},
  keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
}

@Article{Goodfellow2015,
  author        = {Goodfellow, Ian J. and Erhan, Dumitru and {Luc Carrier}, Pierre and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  journal       = {Neural Networks},
  title         = {{Challenges in representation learning: A report on three machine learning contests}},
  year          = {2015},
  issn          = {18792782},
  pages         = {59--63},
  volume        = {64},
  abstract      = {The ICML 2013 Workshop on Challenges in Representation Learning. 11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
  archiveprefix = {arXiv},
  arxivid       = {1307.0414},
  doi           = {10.1016/j.neunet.2014.09.005},
  eprint        = {1307.0414},
  keywords      = {Competition,Dataset,Representation learning},
  pmid          = {25613956},
  url           = {http://www.sciencedirect.com/science/article/pii/S0893608014002159},
}

@Article{Vallet2015,
  author   = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
  journal  = {Computers and Graphics (Pergamon)},
  title    = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
  year     = {2015},
  issn     = {00978493},
  pages    = {126--133},
  volume   = {49},
  abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
  doi      = {10.1016/j.cag.2015.03.004},
  keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene},
}

@InProceedings{Moradi2018,
  author    = {Moradi, Mehdi and Madani, Ali and Karargyris, Alexandros and Syeda-Mahmood, Tanveer F.},
  booktitle = {Medical Imaging 2018: Image Processing},
  title     = {{Chest x-ray generation and data augmentation for cardiovascular abnormality classification}},
  year      = {2018},
  editor    = {Angelini, Elsa D and Landman, Bennett A},
  pages     = {57},
  publisher = {SPIE},
  volume    = {10574},
  abstract  = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Medical imaging datasets are limited in size due to privacy issues and the high cost of obtaining annotations. Augmentation is a widely used practice in deep learning to enrich the data in data-limited scenarios and to avoid overfitting. However, standard augmentation methods that produce new examples of data by varying lighting, field of view, and spatial rigid transformations do not capture the biological variance of medical imaging data and could result in unrealistic images. Generative adversarial networks (GANs) provide an avenue to understand the underlying structure of image data which can then be utilized to generate new realistic samples. In this work, we investigate the use of GANs for producing chest X-ray images to augment a dataset. This dataset is then used to train a convolutional neural network to classify images for cardiovascular abnormalities. We compare our augmentation strategy with traditional data augmentation and show higher accuracy for normal vs abnormal classification in chest X-rays.},
  annote    = {Backup Publisher: International Society for Optics and Photonics},
  doi       = {10.1117/12.2293971},
  isbn      = {9781510616370},
  issn      = {16057422},
  keywords  = {Convolutional networks,Generative adversarial networks,data augmentation},
  url       = {https://doi.org/10.1117/12.2293971},
}

@Article{Takahashi2020a,
  author        = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
  journal       = {IEEE Transactions on Circuits and Systems for Video Technology},
  title         = {{Data Augmentation Using Random Image Cropping and Patching for Deep CNNs}},
  year          = {2020},
  issn          = {15582205},
  number        = {9},
  pages         = {2917--2931},
  volume        = {30},
  abstract      = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage of the soft labels. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of 2.19% on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet, an image-caption retrieval task using Microsoft COCO, and other computer vision tasks.},
  annote        = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
  archiveprefix = {arXiv},
  arxivid       = {1811.09030},
  doi           = {10.1109/TCSVT.2019.2935128},
  eprint        = {1811.09030},
  keywords      = {Data augmentation,convolutional neural network,image classification,image-caption retrieval},
  url           = {http://dx.doi.org/10.1109/TCSVT.2019.2935128},
}

@Article{Zhong2017,
  author        = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  journal       = {arXiv},
  title         = {{Random erasing data augmentation}},
  year          = {2017},
  issn          = {23318422},
  abstract      = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  annote        = {_eprint: 1708.04896},
  archiveprefix = {arXiv},
  arxivid       = {1708.04896},
  doi           = {10.1609/aaai.v34i07.7000},
  eprint        = {1708.04896},
  url           = {http://arxiv.org/abs/1708.04896},
}

@Book{Summers2018,
  author = {Summers, Cecilia and Dinneen, Michael J},
  title  = {{Improved Mixed-Example Data Augmentation}},
  year   = {2018},
  annote = {_eprint: 1805.11272},
}

@InProceedings{Jurio2010,
  author    = {Jurio, Aranzazu and Pagola, Miguel and Galar, Mikel and Lopez-Molina, Carlos and Paternain, Daniel},
  booktitle = {Communications in Computer and Information Science},
  title     = {{A comparison study of different color spaces in clustering based image segmentation}},
  year      = {2010},
  address   = {Berlin, Heidelberg},
  editor    = {H{\"{u}}llermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
  pages     = {532--541},
  publisher = {Springer Berlin Heidelberg},
  volume    = {81 PART 2},
  abstract  = {In this work we carry out a comparison study between different color spaces in clustering-based image segmentation. We use two similar clustering algorithms, one based on the entropy and the other on the ignorance. The study involves four color spaces and, in all cases, each pixel is represented by the values of the color channels in that space. Our purpose is to identify the best color representation, if there is any, when using this kind of clustering algorithms. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
  doi       = {10.1007/978-3-642-14058-7_55},
  isbn      = {9783642140570},
  issn      = {18650929},
  keywords  = {CMY,Clustering,Color space,HSV,Image segmentation,RGB,YUV},
}

@InProceedings{Chatfield2014,
  author        = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle     = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
  title         = {{Return of the devil in the details: Delving deep into convolutional nets}},
  year          = {2014},
  abstract      = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
  annote        = {_eprint: 1405.3531},
  archiveprefix = {arXiv},
  arxivid       = {1405.3531},
  doi           = {10.5244/c.28.6},
  eprint        = {1405.3531},
}

@Article{Shorten2019,
  author   = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  journal  = {Journal of Big Data},
  title    = {{A survey on Image Data Augmentation for Deep Learning}},
  year     = {2019},
  issn     = {21961115},
  number   = {1},
  pages    = {60},
  volume   = {6},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  doi      = {10.1186/s40537-019-0197-0},
  keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
  url      = {https://doi.org/10.1186/s40537-019-0197-0},
}

@InProceedings{Engelcke2017,
  author        = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
  booktitle     = {Proceedings - IEEE International Conference on Robotics and Automation},
  title         = {{Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks}},
  year          = {2017},
  pages         = {1355--1361},
  abstract      = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that VoteSDeep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40% while remaining highly competitive in terms of processing time.},
  archiveprefix = {arXiv},
  arxivid       = {1609.06666},
  doi           = {10.1109/ICRA.2017.7989161},
  eprint        = {1609.06666},
  isbn          = {9781509046331},
  issn          = {10504729},
  url           = {https://doi.org/10.1109/ICRA.2017.7989161},
}

@InProceedings{Rippel2015,
  author        = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P.},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{Spectral representations for convolutional neural networks}},
  year          = {2015},
  pages         = {2449--2457},
  volume        = {2015-Janua},
  abstract      = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
  annote        = {_eprint: 1506.03767},
  archiveprefix = {arXiv},
  arxivid       = {1506.03767},
  eprint        = {1506.03767},
  issn          = {10495258},
  keywords      = {Computer Science - Learning,Statistics - Machine Learning},
}

@InProceedings{Yi2017,
  author        = {Yi, Li and Su, Hao and Guo, Xingwen and Guibas, Leonidas},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation}},
  year          = {2017},
  pages         = {6584--6592},
  volume        = {2017-Janua},
  abstract      = {In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.},
  annote        = {_eprint: 1612.00606},
  archiveprefix = {arXiv},
  arxivid       = {1612.00606},
  doi           = {10.1109/CVPR.2017.697},
  eprint        = {1612.00606},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1612.00606},
}

@InProceedings{Li2018,
  author        = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{PointCNN: Convolution on X-transformed points}},
  year          = {2018},
  pages         = {820--830},
  volume        = {2018-Decem},
  abstract      = {We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
  annote        = {_eprint: 1801.07791},
  archiveprefix = {arXiv},
  arxivid       = {1801.07791},
  eprint        = {1801.07791},
  issn          = {10495258},
  url           = {http://arxiv.org/abs/1801.07791},
}

@InProceedings{Qi2017,
  author        = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{PointNet++: Deep hierarchical feature learning on point sets in a metric space}},
  year          = {2017},
  pages         = {5100--5109},
  volume        = {2017-Decem},
  abstract      = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
  annote        = {_eprint: 1706.02413},
  archiveprefix = {arXiv},
  arxivid       = {1706.02413},
  eprint        = {1706.02413},
  issn          = {10495258},
  url           = {http://arxiv.org/abs/1706.02413},
}

@InProceedings{Qi2017a,
  author        = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
  year          = {2017},
  pages         = {77--85},
  volume        = {2017-Janua},
  abstract      = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
  annote        = {_eprint: 1612.00593},
  archiveprefix = {arXiv},
  arxivid       = {1612.00593},
  doi           = {10.1109/CVPR.2017.16},
  eprint        = {1612.00593},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1612.00593},
}

@InCollection{Cai2019,
  author        = {Cai, Jinzheng and Lu, Le and Xing, Fuyong and Yang, Lin},
  booktitle     = {Advances in Computer Vision and Pattern Recognition},
  title         = {{Pancreas Segmentation in CT and MRI via Task-Specific Network Design and Recurrent Neural Contextual Learning}},
  year          = {2019},
  pages         = {3--21},
  abstract      = {Automatic pancreas segmentation in radiology images, e.g., computed tomography (CT), and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet, pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNN) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. In this chapter, we first discuss the CNN configurations and training objectives that lead to the state-of-the-art performance on pancreas segmentation. We then present a recurrent neural network (RNN) to address the problem of segmentation spatial inconsistency across adjacent image slices. The RNN takes outputs of the CNN and refines the segmentation by improving the shape smoothness.},
  annote        = {_eprint: 1803.11303},
  archiveprefix = {arXiv},
  arxivid       = {1803.11303},
  doi           = {10.1007/978-3-030-13969-8_1},
  eprint        = {1803.11303},
  issn          = {21916594},
  keywords      = {Computer Science - Computer Vision and Pattern Rec},
}

@Article{Shao2018,
  author        = {Shao, Tianjia and Yang, Yin and Weng, Yanlin and Hou, Qiming and Zhou, Kun},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title         = {{H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis}},
  year          = {2018},
  issn          = {19410506},
  abstract      = {We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method builds hierarchical hash tables for an input model under different resolutions that leverage the sparse occupancy of 3D shape boundary. Based on this data structure, we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The perfect spatial hashing is employed as our spatial hashing scheme, which is not only free of hash collision but also nearly minimal so that our data structure is almost of the same size as the raw input. Compared with existing 3D CNN methods, our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed, CNN operations also run faster with our data structure. The experiment shows that, under the same network structure, our method yields comparable or better benchmark results compared with the state-of-the-art while it has only one-third memory consumption when under high resolutions (i.e. 256 3).},
  annote        = {_eprint: 1803.11385},
  archiveprefix = {arXiv},
  arxivid       = {1803.11385},
  doi           = {10.1109/TVCG.2018.2887262},
  eprint        = {1803.11385},
  keywords      = {Computational modeling,Convolution,Data structures,Shape,Solid modeling,Three-dimensional displays,Two dimensional displays,convolutional neural network,perfect hashing,shape classification,shape retrieval,shape segmentation},
}

@InProceedings{Su2018,
  author        = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming Hsuan and Kautz, Jan},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}},
  year          = {2018},
  pages         = {2530--2539},
  volume        = {abs/1802.0},
  abstract      = {We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Na{\~{A}}ely applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.},
  annote        = {_eprint: 1802.08275},
  archiveprefix = {arXiv},
  arxivid       = {1802.08275},
  doi           = {10.1109/CVPR.2018.00268},
  eprint        = {1802.08275},
  isbn          = {9781538664209},
  issn          = {10636919},
  url           = {http://arxiv.org/abs/1802.08275},
}

@InProceedings{Tchapmi2018,
  author        = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, Junyoung and Savarese, Silvio},
  booktitle     = {Proceedings - 2017 International Conference on 3D Vision, 3DV 2017},
  title         = {{SEGCloud: Semantic segmentation of 3D point clouds}},
  year          = {2018},
  pages         = {537--547},
  volume        = {abs/1710.0},
  abstract      = {3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-To-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-The-Art on all datasets.},
  annote        = {_eprint: 1710.07563},
  archiveprefix = {arXiv},
  arxivid       = {1710.07563},
  doi           = {10.1109/3DV.2017.00067},
  eprint        = {1710.07563},
  isbn          = {9781538626108},
  issn          = {2475-7888},
  keywords      = {3D-Conditional-Random-Fields,3D-Convolutional-Neural-Networks,3D-Point-Clouds,3D-Semantic-Segmentation,RGB-D},
  url           = {http://arxiv.org/abs/1710.07563},
}

@InProceedings{Wang2018,
  author        = {Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico},
  booktitle     = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
  title         = {{Adversarial semantic scene completion from a single depth image}},
  year          = {2018},
  pages         = {426--434},
  abstract      = {We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features. This is done by correlating the latent features of the encoder working on partial 2.5D data with the latent features extracted from a variational 3D auto-encoder trained to reconstruct the complete semantic scene. In addition, differently from other approaches that operate entirely through 3D convolutions, at test time we retain the original 2.5D structure of the input during downsampling to improve the effectiveness of the internal representation of our model. We test our approach on the main benchmark datasets for semantic scene completion to qualitatively and quantitatively assess the effectiveness of our proposal.},
  archiveprefix = {arXiv},
  arxivid       = {1810.10901},
  doi           = {10.1109/3DV.2018.00056},
  eprint        = {1810.10901},
  isbn          = {9781538684252},
  keywords      = {Adversarial training,Depth image,Latent space,Scene completion},
}

@InProceedings{Esteves2018,
  author        = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Learning SO(3) Equivariant Representations with Spherical CNNs}},
  year          = {2018},
  number        = {November},
  pages         = {54--70},
  volume        = {11217 LNCS},
  abstract      = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
  annote        = {_eprint: 1711.06721},
  archiveprefix = {arXiv},
  arxivid       = {1711.06721},
  doi           = {10.1007/978-3-030-01261-8_4},
  eprint        = {1711.06721},
  isbn          = {9783030012601},
  issn          = {16113349},
  url           = {http://arxiv.org/abs/1711.06721},
}

@InProceedings{Sedaghat2017,
  author        = {Sedaghat, Nima and Zolfaghari, Mohammadreza and Amiri, Ehsan and Brox, Thomas},
  booktitle     = {British Machine Vision Conference 2017, BMVC 2017},
  title         = {{Orientation-boosted Voxel nets for 3D object recognition}},
  year          = {2017},
  volume        = {abs/1604.0},
  abstract      = {Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.},
  annote        = {_eprint: 1604.03351},
  archiveprefix = {arXiv},
  arxivid       = {1604.03351},
  doi           = {10.5244/c.31.97},
  eprint        = {1604.03351},
  isbn          = {190172560X},
  url           = {http://arxiv.org/abs/1604.03351},
}

@InProceedings{Sharma2016,
  author        = {Sharma, Abhishek and Grau, Oliver and Fritz, Mario},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{VConv-DAE: Deep volumetric shape learning without object labels}},
  year          = {2016},
  pages         = {236--250},
  volume        = {9915 LNCS},
  abstract      = {With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition.We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.},
  annote        = {_eprint: 1604.03755},
  archiveprefix = {arXiv},
  arxivid       = {1604.03755},
  doi           = {10.1007/978-3-319-49409-8_20},
  eprint        = {1604.03755},
  isbn          = {9783319494081},
  issn          = {16113349},
  keywords      = {3D deep learning,Denoising auto-encoder,Shape blending,Shape completion},
  url           = {http://arxiv.org/abs/1604.03755},
}

@Article{Wilhelms1992,
  author   = {Wilhelms, Jane and {Van Gelder}, Allen},
  journal  = {ACM Transactions on Graphics (TOG)},
  title    = {{Octrees for Faster Isosurface Generation}},
  year     = {1992},
  issn     = {15577368},
  number   = {3},
  pages    = {201--227},
  volume   = {11},
  abstract = {The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees 1992. Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional “marching” methods are presented. {\textcopyright} 1992, ACM. All rights reserved.},
  annote   = {Place: New York, NY, USA Publisher: ACM},
  doi      = {10.1145/130881.130882},
  keywords = {hierarchical spatial enumeration,isosurface extraction,octree,scientific visualization},
  url      = {http://doi.acm.org/10.1145/130881.130882},
}

@Article{Meagher1982,
  author   = {Meagher, Donald},
  journal  = {Computer Graphics and Image Processing},
  title    = {{Geometric modeling using octree encoding}},
  year     = {1982},
  issn     = {0146664X},
  number   = {2},
  pages    = {129--147},
  volume   = {19},
  abstract = {A geometric modeling technique called Octree Encoding is presented. Arbitrary 3-D objects can be represented to any specified resolution in a hierarchical 8-ary tree structure or "octree" Objects may be concave or convex, have holes (including interior holes), consist of disjoint parts, and possess sculptured (i.e., "free-form") surfaces. The memory required for representation and manipulation is on the order of the surface area of the object. A complexity metric is proposed based on the number of nodes in an object's tree representation. Efficient (linear time) algorithms have been developed for the Boolean operations (union, intersection and difference), geometric operations (translation, scaling and rotation), N-dimensional interference detection, and display from any point in space with hidden surfaces removed. The algorithms require neither floating-point operations, integer multiplications, nor integer divisions. In addition, many independent sets of very simple calculations are typically generated, allowing implementation over many inexpensive high-bandwidth processors operating in parallel. Real time analysis and manipulation of highly complex situations thus becomes possible. {\textcopyright} 1982.},
  doi      = {10.1016/0146-664X(82)90104-6},
  url      = {https://www.sciencedirect.com/science/article/pii/0146664X82901046%0Ahttp://fab.cba.mit.edu/classes/S62.12/docs/Meagher_octree.pdf},
}

@InProceedings{Silberman2012,
  author    = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Indoor segmentation and support inference from RGBD images}},
  year      = {2012},
  address   = {Berlin, Heidelberg},
  number    = {PART 5},
  pages     = {746--760},
  publisher = {Springer-Verlag},
  series    = {ECCV'12},
  volume    = {7576 LNCS},
  abstract  = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation. {\textcopyright} 2012 Springer-Verlag.},
  annote    = {event-place: Florence, Italy},
  doi       = {10.1007/978-3-642-33715-4_54},
  isbn      = {9783642337147},
  issn      = {03029743},
  url       = {http://dx.doi.org/10.1007/978-3-642-33715-4_54},
}

@Article{Brock2016,
  author        = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  journal       = {CoRR},
  title         = {{Generative and Discriminative Voxel Modeling with Convolutional Neural Networks}},
  year          = {2016},
  volume        = {abs/1608.0},
  abstract      = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5% relative improvement in the state of the art for object classification.},
  annote        = {_eprint: 1608.04236},
  archiveprefix = {arXiv},
  arxivid       = {1608.04236},
  eprint        = {1608.04236},
  url           = {http://arxiv.org/abs/1608.04236},
}

@InProceedings{Tran2016,
  author        = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle     = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  title         = {{Deep End2End Voxel2Voxel Prediction}},
  year          = {2016},
  address       = {United States},
  pages         = {402--409},
  publisher     = {IEEE Computer Society},
  abstract      = {Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.},
  archiveprefix = {arXiv},
  arxivid       = {1511.06681},
  doi           = {10.1109/CVPRW.2016.57},
  eprint        = {1511.06681},
  isbn          = {9781467388504},
  issn          = {21607516},
}

@InProceedings{Maturana2015,
  author    = {Maturana, Daniel and Scherer, Sebastian},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  title     = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
  year      = {2015},
  pages     = {922--928},
  volume    = {2015-Decem},
  abstract  = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
  doi       = {10.1109/IROS.2015.7353481},
  isbn      = {9781479999941},
  issn      = {21530866},
  url       = {https://doi.org/10.1109/IROS.2015.7353481},
}

@InProceedings{Kim2013,
  author    = {Kim, Byung Soo and Kohli, Pushmeet and Savarese, Silvio},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  title     = {{3D scene understanding by voxel-CRF}},
  year      = {2013},
  pages     = {1425--1432},
  abstract  = {Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images. {\textcopyright} 2013 IEEE.},
  doi       = {10.1109/ICCV.2013.180},
  isbn      = {9781479928392},
  keywords  = {3D reconstruction,RGB-D,Scene understanding},
  url       = {https://doi.org/10.1109/ICCV.2013.180},
}

@Article{Kamnitsas2017,
  author        = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F.J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
  journal       = {Medical Image Analysis},
  title         = {{Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation}},
  year          = {2017},
  issn          = {13618423},
  pages         = {61--78},
  volume        = {36},
  abstract      = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
  annote        = {_eprint: 1603.05959},
  archiveprefix = {arXiv},
  arxivid       = {1603.05959},
  doi           = {10.1016/j.media.2016.10.004},
  eprint        = {1603.05959},
  keywords      = {3D convolutional neural network,Brain lesions,Deep learning,Fully connected CRF,Segmentation},
  pmid          = {27865153},
  url           = {http://arxiv.org/abs/1603.05959},
}

@InProceedings{Klokov2017,
  author        = {Klokov, Roman and Lempitsky, Victor},
  booktitle     = {Proceedings of the IEEE International Conference on Computer Vision},
  title         = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models}},
  year          = {2017},
  pages         = {863--872},
  volume        = {2017-Octob},
  abstract      = {We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.},
  annote        = {_eprint: 1704.01222},
  archiveprefix = {arXiv},
  arxivid       = {1704.01222},
  doi           = {10.1109/ICCV.2017.99},
  eprint        = {1704.01222},
  isbn          = {9781538610329},
  issn          = {15505499},
  url           = {http://arxiv.org/abs/1704.01222},
}

@InProceedings{Boulch2017,
  author    = {Boulch, A. and {Le Saux}, B. and Audebert, N.},
  booktitle = {Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
  title     = {{Unstructured point cloud semantic labeling using deep segmentation networks}},
  year      = {2017},
  editor    = {Pratikakis, Ioannis and Dupont, Florent and Ovsjanikov, Maks},
  pages     = {17--24},
  publisher = {The Eurographics Association},
  volume    = {2017-April},
  abstract  = {In this work, we describe a new, general, and efficient method for unstructured point cloud labeling. As the question of efficiently using deep Convolutional Neural Networks (CNNs) on 3D data is still a pending issue, we propose a framework which applies CNNs on multiple 2D image views (or snapshots) of the point cloud. The approach consists in three core ideas. (i) We pick many suitable snapshots of the point cloud. We generate two types of images: a Red-Green-Blue (RGB) view and a depth composite view containing geometric features. (ii) We then perform a pixel-wise labeling of each pair of 2D snapshots using fully convolutional networks. Different architectures are tested to achieve a profitable fusion of our heterogeneous inputs. (iii) Finally, we perform fast back-projection of the label predictions in the 3D space using efficient buffering to label every 3D point. Experiments show that our method is suitable for various types of point clouds such as Lidar or photogrammetric data.},
  annote    = {ISSN: 1997-0471},
  doi       = {10.2312/3dor.20171047},
  isbn      = {9783038680307},
  issn      = {19970471},
}

@InProceedings{Jing2016,
  author    = {Jing, Huang and You, Suya},
  booktitle = {Proceedings - International Conference on Pattern Recognition},
  title     = {{Point cloud labeling using 3D Convolutional Neural Network}},
  year      = {2016},
  pages     = {2670--2675},
  volume    = {0},
  abstract  = {In this paper, we tackle the labeling problem for 3D point clouds. We introduce a 3D point cloud labeling scheme based on 3D Convolutional Neural Network. Our approach minimizes the prior knowledge of the labeling problem and does not require a segmentation step or hand-crafted features as most previous approaches did. Particularly, we present solutions for large data handling during the training and testing process. Experiments performed on the urban point cloud dataset containing 7 categories of objects show the robustness of our approach.},
  doi       = {10.1109/ICPR.2016.7900038},
  isbn      = {9781509048472},
  issn      = {10514651},
  keywords  = {3D convolutional neural network,3D point cloud labeling scheme,Labeling,Neural networks,Testing,Three-dimensional displays,Training,Training data,Two dimensional displays,computer vision,data handling,neural nets,object recognition,testing process,training process,urban point cloud dataset},
}

@InProceedings{Riegler2017,
  author        = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  title         = {{OctNet: Learning deep 3D representations at high resolutions}},
  year          = {2017},
  pages         = {6620--6629},
  volume        = {2017-Janua},
  abstract      = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.},
  annote        = {_eprint: 1611.05009},
  archiveprefix = {arXiv},
  arxivid       = {1611.05009},
  doi           = {10.1109/CVPR.2017.701},
  eprint        = {1611.05009},
  isbn          = {9781538604571},
  url           = {http://arxiv.org/abs/1611.05009},
}

@InProceedings{Wang2017,
  author        = {Wang, Peng Shuai and Liu, Yang and Guo, Yu Xiao and Sun, Chun Yu and Tong, Xin},
  booktitle     = {ACM Transactions on Graphics},
  title         = {{O-CNN: Octree-based convolutional neural networks for 3D shape analysis}},
  year          = {2017},
  number        = {4},
  pages         = {72:1--72:11},
  volume        = {36},
  abstract      = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {1712.01537},
  doi           = {10.1145/3072959.3073608},
  eprint        = {1712.01537},
  issn          = {15577368},
  keywords      = {Convolutional neural network,Object classification,Octree,Shape retrieval,Shape segmentation},
  url           = {http://doi.acm.org/10.1145/3072959.3073608},
}

@InProceedings{Tatarchenko2016,
  author        = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Multi-view 3D models from single images with a convolutional network}},
  year          = {2016},
  pages         = {322--337},
  volume        = {9911 LNCS},
  abstract      = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
  annote        = {_eprint: 1511.06702},
  archiveprefix = {arXiv},
  arxivid       = {1511.06702},
  doi           = {10.1007/978-3-319-46478-7_20},
  eprint        = {1511.06702},
  isbn          = {9783319464770},
  issn          = {16113349},
  keywords      = {3D from single image,Convolutional networks,Deep learning},
  url           = {http://arxiv.org/abs/1511.06702},
}

@InProceedings{Hutter2011,
  author    = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title     = {{Sequential model-based optimization for general algorithm configuration}},
  year      = {2011},
  pages     = {507--523},
  volume    = {6683 LNCS},
  abstract  = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
  doi       = {10.1007/978-3-642-25566-3_40},
  isbn      = {9783642255656},
  issn      = {03029743},
}

@Article{Bergstra2012,
  author   = {Bergstra, James and Bengio, Yoshua},
  journal  = {Journal of Machine Learning Research},
  title    = {{Random search for hyper-parameter optimization}},
  year     = {2012},
  issn     = {15324435},
  number   = {1},
  pages    = {281--305},
  volume   = {13},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
  keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
}

@InCollection{Mendoza2019,
  author    = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Urban, Matthias and Burkart, Michael and Dippel, Maximilian and Lindauer, Marius and Hutter, Frank},
  booktitle = {AutoML@ICML},
  title     = {{Towards Automatically-Tuned Deep Neural Networks}},
  year      = {2019},
  pages     = {135--149},
  abstract  = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. In this work, we present two versions of Auto-Net, which provide automatically-tuned deep neural networks without any human intervention. The first version, Auto-Net 1.0, builds upon ideas from the competition-winning system Auto-sklearn by using the Bayesian Optimization method SMAC and uses Lasagne as the underlying deep learning (DL) library. The more recent Auto-Net 2.0 builds upon a recent combination of Bayesian Optimization and HyperBand, called BOHB, and uses PyTorch as DL library. To the best of our knowledge, Auto-Net 1.0 was the first automatically-tuned neural network to win competition datasets against human experts (as part of the first AutoML challenge). Further empirical results show that ensembling Auto-Net 1.0 with Auto-sklearn can perform better than either approach alone, and that Auto-Net 2.0 can perform better yet. 7.1},
  doi       = {10.1007/978-3-030-05318-5_7},
}

@Article{Zela2018,
  author        = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
  journal       = {arXiv},
  title         = {{Towards automated deep learning: Efficient joint neural architecture and hyperparameter search}},
  year          = {2018},
  issn          = {23318422},
  volume        = {abs/1807.0},
  abstract      = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
  archiveprefix = {arXiv},
  arxivid       = {1807.06906},
  eprint        = {1807.06906},
  keywords      = {Bayesian Optimization,Hyperparameter Optimization,Neural Architecture Search,Object Recognition},
  url           = {http://arxiv.org/abs/1807.06906},
}

@InProceedings{Klein2017,
  author        = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  booktitle     = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
  title         = {{Fast Bayesian optimization of machine learning hyperparameters on large datasets}},
  year          = {2017},
  volume        = {abs/1605.0},
  abstract      = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
  archiveprefix = {arXiv},
  arxivid       = {1605.07079},
  eprint        = {1605.07079},
}

@Article{Stanley2002,
  author   = {Stanley, Kenneth O. and Miikkulainen, Risto},
  journal  = {Evolutionary Computation},
  title    = {{Evolving neural networks through augmenting topologies}},
  year     = {2002},
  issn     = {10636560},
  number   = {2},
  pages    = {99--127},
  volume   = {10},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  doi      = {10.1162/106365602320169811},
  keywords = {Competing conventions,Genetic algorithms,Network topologies,Neural networks,Neuroevolution,Speciation},
  pmid     = {12180173},
}

@Article{Real2019,
  author        = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
  journal       = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
  title         = {{Regularized evolution for image classifier architecture search}},
  year          = {2019},
  issn          = {2159-5399},
  pages         = {4780--4789},
  volume        = {33},
  abstract      = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier-AmoebaNet-A-that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9% top-1 / 96.6% top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
  archiveprefix = {arXiv},
  arxivid       = {1802.01548},
  doi           = {10.1609/aaai.v33i01.33014780},
  eprint        = {1802.01548},
  isbn          = {9781577358091},
}

@InProceedings{Baker2019,
  author        = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
  booktitle     = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
  title         = {{Designing neural network architectures using reinforcement learning}},
  year          = {2019},
  volume        = {abs/1611.0},
  abstract      = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
  archiveprefix = {arXiv},
  arxivid       = {1611.02167},
  eprint        = {1611.02167},
}

@InProceedings{Chen2016a,
  author        = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  booktitle     = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
  title         = {{Net2Net: Accelerating learning via knowledge transfer}},
  year          = {2016},
  volume        = {abs/1511.0},
  abstract      = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1511.05641},
  eprint        = {1511.05641},
}

@InProceedings{Jin2019,
  author        = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  booktitle     = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  title         = {{Auto-keras: An efficient neural architecture search system}},
  year          = {2019},
  pages         = {1946--1956},
  abstract      = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
  archiveprefix = {arXiv},
  arxivid       = {1806.10282},
  doi           = {10.1145/3292500.3330648},
  eprint        = {1806.10282},
  isbn          = {9781450362016},
  keywords      = {AutoML,Automated Machine Learning,Bayesian Optimization,Network Morphism,Neural Architecture Search},
}

@InProceedings{Liu2017,
  author        = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  booktitle     = {arXiv},
  title         = {{Hierarchical representations for efficient architecture search}},
  year          = {2017},
  volume        = {abs/1711.0},
  abstract      = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
  archiveprefix = {arXiv},
  arxivid       = {1711.00436},
  eprint        = {1711.00436},
  issn          = {23318422},
}

@InProceedings{Zoph2018,
  author        = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Learning Transferable Architectures for Scalable Image Recognition}},
  year          = {2018},
  pages         = {8697--8710},
  abstract      = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1707.07012},
  doi           = {10.1109/CVPR.2018.00907},
  eprint        = {1707.07012},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@InProceedings{Zhong2018a,
  author        = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng Lin},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Practical Block-Wise Neural Network Architecture Generation}},
  year          = {2018},
  pages         = {2423--2432},
  abstract      = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
  archiveprefix = {arXiv},
  arxivid       = {1708.05552},
  doi           = {10.1109/CVPR.2018.00257},
  eprint        = {1708.05552},
  isbn          = {9781538664209},
  issn          = {10636919},
}

@Misc{Liu2018,
  author        = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  title         = {{Darts: Differentiable architecture search}},
  year          = {2018},
  abstract      = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  archiveprefix = {arXiv},
  arxivid       = {1806.09055},
  booktitle     = {7th International Conference on Learning Representations, ICLR 2019arXivarXiv},
  eprint        = {1806.09055},
  volume        = {abs/1806.0},
}

@InProceedings{Liu2018a,
  author        = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title         = {{Progressive Neural Architecture Search}},
  year          = {2018},
  pages         = {19--35},
  volume        = {11205 LNCS},
  abstract      = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
  archiveprefix = {arXiv},
  arxivid       = {1712.00559},
  doi           = {10.1007/978-3-030-01246-5_2},
  eprint        = {1712.00559},
  isbn          = {9783030012458},
  issn          = {16113349},
}

@InProceedings{Gao2019,
  author        = {Gao, Yang and Yang, Hong and Zhang, Peng and Zhou, Chuan and Hu, Yue},
  booktitle     = {arXiv},
  title         = {{GraphNAS: Graph neural architecture search with reinforcement learning}},
  year          = {2019},
  volume        = {abs/1611.0},
  abstract      = {Graph Neural Networks (GNNs) have been popularly used for analyzing non-Euclidean data such as social network data and biological data. Despite their success, the design of graph neural networks requires a lot of manual work and domain knowledge. In this paper, we propose a Graph Neural Architecture Search method (GraphNAS for short) that enables automatic search of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS first uses a recurrent network to generate variable-length strings that describe the architectures of graph neural networks, and then trains the recurrent network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation data set. Extensive experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that GraphNAS can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. On node classification tasks, GraphNAS can design a novel network architecture that rivals the best humaninvented architecture in terms of test set accuracy.},
  archiveprefix = {arXiv},
  arxivid       = {1904.09981},
  eprint        = {1904.09981},
  isbn          = {9780999241165},
  issn          = {23318422},
}

@InProceedings{Brock2017,
  author        = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
  booktitle     = {arXiv},
  title         = {{SMASH: One-shot model architecture search through hypernetworks}},
  year          = {2017},
  volume        = {abs/1708.0},
  abstract      = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized handdesigned networks.},
  archiveprefix = {arXiv},
  arxivid       = {1708.05344},
  eprint        = {1708.05344},
}

@InProceedings{He2016,
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle     = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title         = {{Deep residual learning for image recognition}},
  year          = {2016},
  pages         = {770--778},
  volume        = {2016-Decem},
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {1512.03385},
  doi           = {10.1109/CVPR.2016.90},
  eprint        = {1512.03385},
  isbn          = {9781467388504},
  issn          = {10636919},
  url           = {http://arxiv.org/abs/1512.03385},
}

@Article{Li,
  author = {Li, Johann},
  title  = {{Medical Image Datasets and Challenges for Machine Learning: A Review}},
}

@Article{Kavur_2021,
  author        = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Aslan, Sinem and Conze, Pierre-Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and Özkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and Nürnberger, Andreas and Maier-Hein, Klaus H. and Bozdağı Akar, Gözde and Ünal, Gözde and Dicle, Oğuz and Selver, M. Alper},
  journal       = {Medical Image Analysis},
  title         = {CHAOS Challenge - combined (CT-MR) healthy abdominal organ segmentation},
  year          = {2021},
  issn          = {1361-8415},
  month         = apr,
  pages         = {101950},
  volume        = {69},
  abstract      = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) have introduced new state-of-the-art segmentation systems. In order to expand the knowledge on these topics, the CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge has been organized in conjunction with IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks have been designed to analyze the capabilities of current approaches from multiple perspectives. The results are investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 $\pm$ 0.00 / 0.95 $\pm$ 0.01) but the best MSSD performance remain limited (21.89 $\pm$ 13.94 / 20.85 $\pm$ 10.63 mm). The performances of participating models decrease significantly for cross-modality tasks for the liver (DICE: 0.88 $\pm$ 0.15 MSSD: 36.33 $\pm$ 21.97 mm) and all organs (DICE: 0.85 $\pm$ 0.21 MSSD: 33.17 $\pm$ 38.93 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs seem to perform worse compared to organ-specific ones (performance drop around 5\%). Besides, such directions of further research for cross-modality segmentation would significantly support real-world clinical applications. Moreover, having more than 1500 participants, another important contribution of the paper is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomena.},
  archiveprefix = {arXiv},
  arxivid       = {2001.06535},
  doi           = {10.1016/j.media.2020.101950},
  eprint        = {2001.06535},
  keywords      = {Abdomen,Challenge,Cross-modality,Segmentation},
  pmid          = {33421920},
  publisher     = {Elsevier BV},
  url           = {http://dx.doi.org/10.1016/j.media.2020.101950},
}

@InProceedings{Lake2011,
  author    = {Lake, Brenden M and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B},
  booktitle = {In {Proceedings of the 33rd Annual Conference of the Cognitive Science Society}},
  title     = {{One shot learning of simple visual concepts}},
  year      = {2011},
  abstract  = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing state-of-the-art character model on a challenging one shot learning task, and it provides a good fit to human perceptual data.},
}

@Article{art/WangW_202110,
  author    = {Wang, Wenji and Xia, Qing and Hu, Zhiqiang and Yan, Zhennan and Li, Zhuowei and Wu, Yang and Huang, Ning and Gao, Yue and Metaxas, Dimitris and Zhang, Shaoting},
  journal   = {IEEE Transactions on Medical Imaging},
  title     = {{Few-Shot Learning by a Cascaded Framework with Shape-Constrained Pseudo Label Assessment for Whole Heart Segmentation}},
  year      = {2021},
  issn      = {1558254X},
  month     = {oct},
  number    = {10},
  pages     = {2629--2641},
  volume    = {40},
  abstract  = {Automatic and accurate 3D cardiac image segmentation plays a crucial role in cardiac disease diagnosis and treatment. Even though CNN based techniques have achieved great success in medical image segmentation, the expensive annotation, large memory consumption, and insufficient generalization ability still pose challenges to their application in clinical practice, especially in the case of 3D segmentation from high-resolution and large-dimension volumetric imaging. In this paper, we propose a few-shot learning framework by combining ideas of semi-supervised learning and self-training for whole heart segmentation and achieve promising accuracy with a Dice score of 0.890 and a Hausdorff distance of 18.539 mm with only four labeled data for training. When more labeled data provided, the model can generalize better across institutions. The key to success lies in the selection and evolution of high-quality pseudo labels in cascaded learning. A shape-constrained network is built to assess the quality of pseudo labels, and the self-training stages with alternative global-local perspectives are employed to improve the pseudo labels. We evaluate our method on the CTA dataset of the MM-WHS 2017 Challenge and a larger multi-center dataset. In the experiments, our method outperforms the state-of-the-art methods significantly and has great generalization ability on the unseen data. We also demonstrate, by a study of two 4D (3D+T) CTA data, the potential of our method to be applied in clinical practice.},
  doi       = {10.1109/TMI.2021.3053008},
  keywords  = {Whole heart segmentation,pseudo label,quality assessment,self-training,semi-supervised},
  pmid      = {33471751},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
}

@InCollection{Kotia2021,
  author    = {Kotia, Jai and Kotwal, Adit and Bharti, Rishika and Mangrulkar, Ramchandra},
  booktitle = {Studies in Computational Intelligence},
  publisher = {Springer},
  title     = {{Few shot learning for medical imaging}},
  year      = {2021},
  pages     = {107--132},
  volume    = {907},
  abstract  = {While deep learning systems have provided breakthroughs in several tasks in the medical domain, they are still limited by the problem of dependency on the availability of training data. To counter this limitation, there is active research ongoing in few shot learning. Few shot learning algorithms aim to overcome the data dependency by exploiting the information available from a very small amount of data. In medical imaging, due to the rare occurrence of some diseases, there is often a limitation on the available data, as a result, to which the success of few shot learning algorithms can prove to be a significant advancement. In this chapter, the background and working of few shot learning algorithms are explained. The problem statement for few shot classification and segmentation is described. There is then a detailed study of the problems faced in medical imaging related to the availability of limited data. After establishing context, the recent advances in the application of few shot learning to medical imaging tasks such as classification and segmentation are explored. The results of these applications are examined with a discussion on its future scope.},
  doi       = {10.1007/978-3-030-50641-4_7},
  issn      = {18609503},
  keywords  = {Few Shot Learning,Image Classification,Image Segmentation,Medical Imaging,Meta Learning},
  url       = {http://link.springer.com/10.1007/978-3-030-50641-4_7},
}

@InProceedings{Munkhdalai2018,
  author        = {Munkhdalai, Tsendsuren and Yuan, Xingdi and Mehri, Soroush and Trischler, Adam},
  booktitle     = {35th International Conference on Machine Learning, ICML 2018},
  title         = {{Rapid adaptation with conditionally shifted neurons}},
  year          = {2018},
  pages         = {5898--5909},
  volume        = {8},
  abstract      = {We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.},
  archiveprefix = {arXiv},
  arxivid       = {1712.09926},
  eprint        = {1712.09926},
  isbn          = {9781510867963},
}

@InProceedings{Pham2018,
  author        = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
  booktitle     = {35th International Conference on Machine Learning, ICML 2018},
  title         = {{Efficient Neural Architecture Search via parameter Sharing}},
  year          = {2018},
  pages         = {6522--6531},
  volume        = {9},
  abstract      = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).},
  archiveprefix = {arXiv},
  arxivid       = {1802.03268},
  eprint        = {1802.03268},
  isbn          = {9781510867963},
}

@Article{Munkhdalai2017,
  author        = {Munkhdalai, Tsendsuren and Yu, Hong},
  journal       = {34th International Conference on Machine Learning, ICML 2017},
  title         = {{Meta networks}},
  year          = {2017},
  issn          = {2640-3498},
  month         = {mar},
  pages         = {3933--3943},
  volume        = {5},
  abstract      = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
  archiveprefix = {arXiv},
  arxivid       = {1703.00837},
  eprint        = {1703.00837},
  isbn          = {9781510855144},
  url           = {http://arxiv.org/abs/1703.00837},
}

@InProceedings{Real2017,
  author        = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V. and Kurakin, Alexey},
  booktitle     = {34th International Conference on Machine Learning, ICML 2017},
  title         = {{Large-scale evolution of image classifiers}},
  year          = {2017},
  pages         = {4429--4446},
  volume        = {6},
  abstract      = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
  archiveprefix = {arXiv},
  arxivid       = {1703.01041},
  eprint        = {1703.01041},
  isbn          = {9781510855144},
}

@Article{Wang2020a,
  author        = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
  journal       = {ACM Computing Surveys},
  title         = {{Generalizing from a Few Examples: A Survey on Few-shot Learning}},
  year          = {2020},
  issn          = {15577341},
  month         = {jun},
  number        = {3},
  pages         = {1--34},
  volume        = {53},
  abstract      = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
  archiveprefix = {arXiv},
  arxivid       = {1904.05046},
  doi           = {10.1145/3386252},
  eprint        = {1904.05046},
  file          = {:home/qinka/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-shot Learning.pdf:pdf},
  keywords      = {Few-shot learning,low-shot learning,meta-learning,one-shot learning,prior knowledge,small sample learning},
  publisher     = {Association for Computing Machinery},
  url           = {https://dl.acm.org/doi/10.1145/3386252},
}

@Misc{Yin2019a,
  author        = {Yin, Mingzhang and Tucker, George and Zhou, Mingyuan and Levine, Sergey and Finn, Chelsea},
  month         = {sep},
  title         = {{Meta-learning without memorization}},
  year          = {2019},
  abstract      = {The ability to learn new concepts with small amounts of data is a critical aspect of intelligence that has proven challenging for deep learning methods. Meta-learning has emerged as a promising technique for leveraging data from previous tasks to enable efficient learning of new tasks. However, most meta-learning algorithms implicitly require that the meta-training tasks be mutually-exclusive, such that no single model can solve all of the tasks at once. For example, when creating tasks for few-shot image classification, prior work uses a per-task random assignment of image classes to N-way classification labels. If this is not done, the meta-learner can ignore the task training data and learn a single model that performs all of the meta-training tasks zero-shot, but does not adapt effectively to new image classes. This requirement means that the user must take great care in designing the tasks, for example by shuffling labels or removing task identifying information from the inputs. In some domains, this makes meta-learning entirely inapplicable. In this paper, we address this challenge by designing a meta-regularization objective using information theory that places precedence on data-driven adaptation. This causes the meta-learner to decide what must be learned from the task training data and what should be inferred from the task testing input. By doing so, our algorithm can successfully use data from non-mutually-exclusive tasks to efficiently adapt to novel tasks. We demonstrate its applicability to both contextual and gradient-based meta-learning algorithms, and apply it in practical settings where applying standard meta-learning has been difficult. Our approach substantially outperforms standard meta-learning algorithms in these settings.},
  archiveprefix = {arXiv},
  arxivid       = {1912.03820},
  booktitle     = {arXiv},
  eprint        = {1912.03820},
  issn          = {23318422},
}

@Article{LeCun1998,
  author   = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
  journal  = {Proceedings of the IEEE},
  title    = {{Gradient-based learning applied to document recognition}},
  year     = {1998},
  issn     = {00189219},
  number   = {11},
  pages    = {2278--2323},
  volume   = {86},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
  doi      = {10.1109/5.726791},
  keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
  url      = {http://ieeexplore.ieee.org/document/726791/},
}

@InProceedings{Krizhevsky2017,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Communications of the ACM},
  title     = {{ImageNet classification with deep convolutional neural networks}},
  year      = {2017},
  number    = {6},
  pages     = {84--90},
  volume    = {60},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
  doi       = {10.1145/3065386},
  isbn      = {9781627480031},
  issn      = {15577317},
}

@InProceedings{Snoek2012,
  author        = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  booktitle     = {Advances in Neural Information Processing Systems},
  title         = {{Practical Bayesian optimization of machine learning algorithms}},
  year          = {2012},
  pages         = {2951--2959},
  volume        = {4},
  abstract      = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  archiveprefix = {arXiv},
  arxivid       = {1206.2944},
  eprint        = {1206.2944},
  isbn          = {9781627480031},
  issn          = {10495258},
}

@InProceedings{Schulman2015,
  author        = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
  booktitle     = {32nd International Conference on Machine Learning, ICML 2015},
  title         = {{Trust region policy optimization}},
  year          = {2015},
  pages         = {1889--1897},
  volume        = {3},
  abstract      = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  arxivid       = {1502.05477},
  eprint        = {1502.05477},
  isbn          = {9781510810587},
}

@InProceedings{Ioffe2015,
  author        = {Ioffe, Sergey and Szegedy, Christian},
  booktitle     = {32nd International Conference on Machine Learning, ICML 2015},
  title         = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
  year          = {2015},
  pages         = {448--456},
  volume        = {1},
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  annote        = {_eprint: 1502.03167},
  archiveprefix = {arXiv},
  arxivid       = {1502.03167},
  eprint        = {1502.03167},
  isbn          = {9781510810587},
  url           = {http://arxiv.org/abs/1502.03167},
}

@Article{Chen2016,
  author   = {Chen, Hao and Dou, Qi and Wang, Xi and Qin, Jing and Heng, Pheng Ann},
  journal  = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  title    = {{Mitosis detection in breast cancer histology images via deep cascaded networks}},
  year     = {2016},
  pages    = {1160--1166},
  abstract = {The number of mitoses per tissue area gives an important aggressiveness indication of the invasive breast carcinoma. However, automatic mitosis detection in histology images remains a challenging problem. Traditional methods either employ hand-crafted features to discriminate mitoses from other cells or construct a pixel-wise classifier to label every pixel in a sliding window way. While the former suffers from the large shape variation of mitoses and the existence of many mimics with similar appearance, the slow speed of the later prohibits its use in clinical practice. In order to overcome these shortcomings, we propose a fast and accurate method to detect mitosis by designing a novel deep cascaded convolutional neural network, which is composed of two components. First, by leveraging the fully convolutional neural network, we propose a coarse retrieval model to identify and locate the candidates of mitosis while preserving a high sensitivity. Based on these candidates, a fine discrimination model utilizing knowledge transferred from cross-domain is developed to further single out mitoses from hard mimics. Our approach outperformed other methods by a large margin in 2014 ICPR MITOS-ATYPIA challenge in terms of detection accuracy. When compared with the state-of-the-art methods on the 2012 ICPR MITOSIS data (a smaller and less challenging dataset), our method achieved comparable or better results with a roughly 60 times faster speed.},
  isbn     = {9781577357605},
}

@InProceedings{Mou2016,
  author        = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
  booktitle     = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  title         = {{Convolutional neural networks over tree structures for programming language processing}},
  year          = {2016},
  pages         = {1287--1293},
  volume        = {abs/1409.5},
  abstract      = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
  annote        = {_eprint: 1409.5718},
  archiveprefix = {arXiv},
  arxivid       = {1409.5718},
  eprint        = {1409.5718},
  isbn          = {9781577357605},
  url           = {http://arxiv.org/abs/1409.5718},
}

@Article{Graham2019,
  author        = {Graham, Simon and Vu, Quoc Dang and Raza, Shan E.Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},
  journal       = {Medical Image Analysis},
  title         = {{Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images}},
  year          = {2019},
  issn          = {13618423},
  pages         = {1--11},
  volume        = {58},
  abstract      = {Nuclear segmentation and classification within Haematoxylin & Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin & Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.},
  annote        = {_eprint: 1812.06499},
  archiveprefix = {arXiv},
  arxivid       = {1812.06499},
  doi           = {10.1016/j.media.2019.101563},
  eprint        = {1812.06499},
  keywords      = {Computational pathology,Deep learning,Nuclear classification,Nuclear segmentation},
  pmid          = {31561183},
  url           = {http://arxiv.org/abs/1812.06499},
}

@Article{Oh2016,
  author        = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
  journal       = {33rd International Conference on Machine Learning, ICML 2016},
  title         = {{Control of memory, active perception, and action in minecraft}},
  year          = {2016},
  month         = {may},
  pages         = {4067--4089},
  volume        = {6},
  abstract      = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
  archiveprefix = {arXiv},
  arxivid       = {1605.09128},
  eprint        = {1605.09128},
  isbn          = {9781510829008},
  url           = {http://arxiv.org/abs/1605.09128},
}

@InProceedings{Wei2016,
  author        = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  booktitle     = {33rd International Conference on Machine Learning, ICML 2016},
  title         = {{Network morphism}},
  year          = {2016},
  pages         = {842--850},
  volume        = {2},
  abstract      = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  archiveprefix = {arXiv},
  arxivid       = {1603.01670},
  eprint        = {1603.01670},
  isbn          = {9781510829008},
}

@Misc{Oskar2015,
  author  = {Oskar, Maier and Bj{\"{o}}rn, Menze and Mauricio, Reyes},
  title   = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2015}},
  year    = {2015},
  url     = {http://www.isles-challenge.org/ISLES2015/},
  urldate = {2020-05-20},
}

@Misc{MarcosRomero2019,
  author        = {{Marcos Romero}, Brenden Sewell},
  title         = {{Blueprints Visual Scripting for Unreal Engine}},
  year          = {2019},
  abstract      = {Brenden Sewell is a lead game designer at E-Line Media, and has spent the last 5 years designing and creating games that are both fun to play and have educational or social impact. He has been building games since 2002, when Neverwinter Nights taught him an invaluable lesson about the expressive power of game design. In 2010, he graduated with a degree in cognitive science from Indiana University. Since then, he has focused on enhancing his own craft of game design while harnessing its power to do good in the world, and exposing more people to the joy the profession holds.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  booktitle     = {Packt},
  eprint        = {arXiv:1011.1669v3},
  isbn          = {9788578110796},
  issn          = {1098-6596},
  keywords      = {icle},
  number        = {9},
  pages         = {366},
  pmid          = {25246403},
  url           = {https://docs.unrealengine.com/en-US/Engine/Blueprints/index.html},
  volume        = {53},
}

@Book{Vinet2011,
  author        = {Vinet, Luc and Zhedanov, Alexei},
  publisher     = {The MIT Press},
  title         = {{A 'missing' family of classical orthogonal polynomials}},
  year          = {2011},
  isbn          = {9788578110796},
  number        = {8},
  volume        = {44},
  abstract      = {We study a family of 'classical' orthogonal polynomials which satisfy (apart from a three-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl type. These polynomials can be obtained from the little q-Jacobi polynomials in the limit q = -1. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for q = -1. {\textcopyright} 2011 IOP Publishing Ltd.},
  archiveprefix = {arXiv},
  arxivid       = {1011.1669},
  booktitle     = {Journal of Physics A: Mathematical and Theoretical},
  doi           = {10.1088/1751-8113/44/8/085201},
  eprint        = {1011.1669},
  issn          = {17518113},
  keywords      = {icle},
  pages         = {1689--1699},
  pmid          = {25246403},
  url           = {https://www.xarg.org/ref/a/0262035618/},
}

@Article{Wu2018,
  author        = {Wu, Zhe and Singh, Bharat and Davis, Larry S. and Subrahmanian, V. S.},
  journal       = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
  title         = {{Deception detection in videos}},
  year          = {2018},
  month         = {dec},
  pages         = {1695--1702},
  abstract      = {We present a system for covert automated deception detection using information available in a video. We study the importance of different modalities like vision, audio and text for this task. On the vision side, our system uses classifiers trained on low level video features which predict human micro-expressions. We show that predictions of high-level micro-expressions can be used as features for deception prediction. Surprisingly, IDT (Improved Dense Trajectory) features which have been widely used for action recognition, are also very good at predicting deception in videos. We fuse the score of classifiers trained on IDT features and high-level micro-expressions to improve performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio domain also provide a significant boost in performance, while information from transcripts is not very beneficial for our system. Using various classifiers, our automated system obtains an AUC of 0.877 (10-fold cross-validation) when evaluated on subjects which were not part of the training set. Even though state-of-the-art methods use human annotations of micro-expressions for deception detection, our fully automated approach outperforms them by 5%. When combined with human annotations of micro-expressions, our AUC improves to 0.922. We also present results of a user-study to analyze how well do average humans perform on this task, what modalities they use for deception detection and how they perform if only one modality is accessible.},
  archiveprefix = {arXiv},
  arxivid       = {1712.04415},
  eprint        = {1712.04415},
  isbn          = {9781577358008},
  url           = {http://arxiv.org/abs/1712.04415},
}

@InProceedings{Cai2018,
  author        = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
  booktitle     = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
  title         = {{Efficient architecture search by network transformation}},
  year          = {2018},
  pages         = {2787--2794},
  abstract      = {Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.},
  archiveprefix = {arXiv},
  arxivid       = {1707.04873},
  eprint        = {1707.04873},
  isbn          = {9781577358008},
}

@Article{art/ZhouD_2004,
  author    = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Sch{\"{o}}lkopf, Bernhard},
  journal   = {Advances in Neural Information Processing Systems},
  title     = {{Learning with local and global consistency}},
  year      = {2004},
  issn      = {10495258},
  abstract  = {We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.},
  address   = {Cambridge, Mass. [u.a.]},
  doi       = {10.5555/2981345.2981386},
  editor    = {Sebastian Thrun},
  isbn      = {0262201526},
  keywords  = {class prior knowledge,geodesic distance,label information,semi-supervised learning},
  pagetotal = {1621},
  ppn_gvk   = {394003780},
  publisher = {Neural information processing systems foundation},
  subtitle  = {Annual Conference on Neural Information Processing Systems},
}

@Article{artVinyalsO_2019nov,
  author   = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  journal  = {Nature},
  title    = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
  year     = {2019},
  issn     = {14764687},
  month    = {nov},
  number   = {7782},
  pages    = {350--354},
  volume   = {575},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.},
  doi      = {10.1038/s41586-019-1724-z},
  pmid     = {31666705},
  ranking  = {rank2},
  url      = {http://www.nature.com/articles/s41586-019-1724-z},
}

@Comment{jabref-meta: VersionDBStructure:1;}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Own\;0\;1\;0xe64d4dff\;HOME_HEART\;我自己发表的\;;
}

@Comment{jabref-meta: keypatterndefault:art\\\\:[auth][authForeIni]_[year][month];}
